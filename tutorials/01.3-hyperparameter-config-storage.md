---
date: 2026-02-11
summary: "How generation_config stores LLM hyperparameters with commits for full call provenance, threading through the entire commit/compile/query pipeline with cache-safe copy semantics"
audience: intermediate
---

# Phase 1.3: Hyperparameter Config Storage

**Generation configs give every commit a memory of how it was produced.**

After Phase 1.2, Trace can commit structured content, compile it into LLM-ready messages, and track token counts from both tiktoken and API responses. But there's a provenance gap: when you look at a commit, you know *what* the LLM said and roughly *how many tokens* it cost, but you don't know *what parameters* produced it. Was the temperature 0.3 or 0.9? Which model? What top_p? Without that context, you can't reproduce results, compare branches with different configs, or reason about why one response was better than another.

Phase 1.3 closes that gap by adding an optional `generation_config` dict to every commit. The config is immutable once written, preserved through compilation, excluded from content hashing, and queryable via SQL-side JSON extraction.

## Why Store Generation Configs?

The core insight is that LLM outputs are a function of two things: the prompt (which Trace already versions) and the generation parameters (which, until now, were invisible). Consider a scenario where you're tuning an agent's behavior: you try temperature 0.3 for precise reasoning, then switch to 0.9 for creative brainstorming. Without generation config storage, both branches of exploration look identical in the commit history -- you'd have to remember externally which commits used which settings.

This becomes especially important for the exploration/exploitation pattern that Phase 3's branching will enable. Imagine creating two branches from the same context: one with `{"temperature": 0.2, "model": "gpt-4o"}` for careful analysis, another with `{"temperature": 0.9, "model": "claude-3-opus"}` for creative generation. After each branch produces results, you want to compare them -- and to do that meaningfully, you need the generation parameters stored right alongside the content they produced.

Generation config storage also serves as an audit trail. In production multi-agent systems, being able to query "which commits used temperature > 0.8?" or "show me all commits generated by claude-3-opus" is the kind of observability that turns debugging from guesswork into data analysis.

## Design Decisions

### Immutable at Commit Time

Generation config is set once when you call `tract.commit(..., generation_config={...})` and never modified afterward. This follows the same principle as all commit data in Trace: commits are immutable records. You always know the generation parameters *before* making the API call, so there's no reason to update them after the fact.

We considered extending `record_usage()` to accept generation config (the original plan had this), but rejected it during planning. The reasoning: `record_usage()` exists for information you learn *after* the API call (actual token counts), while generation config is information you have *before* the call. Mixing the two semantics would confuse the API surface -- "set config at commit time" is simpler and more predictable than "set it at commit time OR update it after the call."

### Provider-Agnostic JSON Blob

The `generation_config` field accepts any dict. There's no schema validation, no required keys, no enum of allowed parameters. This is deliberate: different LLM providers use wildly different parameter names and ranges.

```python
# OpenAI
{"model": "gpt-4o", "temperature": 0.7, "frequency_penalty": 0.5}

# Anthropic
{"model": "claude-3-opus", "top_k": 40, "temperature": 0.3}

# Meta/Llama
{"model": "llama-3", "repetition_penalty": 1.2, "min_p": 0.05}
```

All three work without migration. If a new provider introduces a `quantum_coherence` parameter next month, it just goes in the dict. This mirrors the design of `metadata_json` on CommitRow -- both are nullable JSON blobs that accept arbitrary structures.

The alternative would have been a structured Pydantic model with known fields (temperature, top_p, model, etc.), but that would require migrations whenever a provider adds new parameters. At Trace's current scale, the flexibility of a raw dict far outweighs the type safety of a structured model.

### Not in the Commit Hash

This is a critical design decision: `generation_config` is excluded from both `content_hash` and `commit_hash`. Two commits with identical content but different generation configs produce the same `content_hash`:

```python
c1 = tract.commit(content, generation_config={"temperature": 0.1})
c2 = tract.commit(content, generation_config={"temperature": 0.9})
assert c1.content_hash == c2.content_hash  # Same content, same hash
```

The reasoning follows from Trace's content-addressable storage model. The hash answers the question "is this the same content?" -- and the content *is* the same regardless of which parameters produced it. Including generation config in the hash would break deduplication: the same assistant response generated at temperature 0.7 and temperature 0.8 would get different hashes despite being byte-identical.

The `commit_hash` still differs between these two commits (because it includes `parent_hash` and `created_at`), so they remain distinct commits. Generation config is metadata *about* the generation process, not part of the content itself.

### Edit-Inherits-Original

When you EDIT a commit, you're replacing its content. But what happens to the original commit's generation config? There are three possible semantics:

1. **Edit's config always wins** -- even if the edit has no config, the result has no config
2. **Original config always wins** -- the edit's config is ignored
3. **Edit's config wins if present, otherwise inherit original** -- chosen approach

We chose option 3 because it handles the two real-world editing scenarios correctly:

- **Re-generating with different params**: You edit a commit and provide new `generation_config` because you re-ran the API call with different settings. The edit's config should be used.
- **Manual content correction**: You edit a commit to fix a typo or adjust wording, without re-running the API. You don't provide a `generation_config` because the generation parameters haven't changed. The original's config should be preserved.

In the compiler, this looks like a two-step fallback:

```python
edit_commit = edit_map.get(c.commit_hash)
if edit_commit is not None and edit_commit.generation_config_json is not None:
    config = edit_commit.generation_config_json  # Edit specified a config
else:
    config = c.generation_config_json or {}      # Inherit original, or {} if neither has one
```

### No Index on the JSON Column

We deliberately didn't add a database index on `generation_config_json`. SQLite supports generated column indexes that could speed up JSON queries, but at Phase 1 scale (hundreds to low thousands of commits per tract), the full table scan filtered by `tract_id` is fast enough. Adding the index would be premature optimization with ongoing maintenance cost. If query performance becomes a concern in later phases, a generated column index can be added without any application code changes.

### Copy-on-Output and Copy-on-Input Cache Safety

This is the most subtle design decision and the one that produced the phase's only deviation from plan. The `CompileSnapshot` caches `generation_configs` as a `tuple[dict, ...]`. The tuple itself is immutable, but the dicts inside it are mutable Python objects. Without careful handling, a user could corrupt the cache:

```python
result = tract.compile()
result.generation_configs[0]["temperature"] = 999  # Mutate returned dict
# Without copy-on-output, this would corrupt the cached snapshot
```

The solution is a two-layer copy strategy:

- **Copy-on-output** in `_snapshot_to_compiled()`: When returning a `CompiledContext` from cache, each dict is shallow-copied via `[dict(c) for c in snapshot.generation_configs]`. User mutations to the returned dicts don't reach the snapshot.

- **Copy-on-input** in `_build_snapshot_from_compiled()`: When storing a `CompiledContext` into a new snapshot, each dict is shallow-copied via `tuple(dict(c) for c in result.generation_configs)`. This prevents the reverse path: if someone stored a reference to the `CompiledContext` and later mutated it, the snapshot would be safe.

This was discovered as a bug during testing (the plan only specified copy-on-output, but the copy-on-input path was needed too). The shallow copy is sufficient because generation configs are flat key-value pairs -- there are no nested dicts or lists that would require deep copying.

Note the contrast with `Message` objects in the same snapshot: those are frozen dataclasses and can be shared safely without copying. Dicts don't have that guarantee, hence the explicit copy pattern.

## Implementation Walkthrough

### The Storage Layer

The implementation starts at the bottom of the stack. In `src/tract/models/commit.py`, a single field is added to `CommitInfo`:

```python
generation_config: Optional[dict] = None
```

This is the SDK-facing representation -- what users see when they receive a `CommitInfo` from `commit()`, `get_commit()`, or `log()`. It sits alongside the existing `metadata` field, following the same pattern of an optional, untyped dict.

On the ORM side, `src/tract/storage/schema.py` adds a nullable JSON column to `CommitRow`:

```python
generation_config_json: Mapped[Optional[dict]] = mapped_column(JSON, nullable=True)
```

SQLAlchemy's `JSON` type handles serialization automatically -- Python dicts become JSON text in SQLite and are deserialized back to dicts on read. The column sits right after `metadata_json`, mirroring the same storage pattern.

### The Repository Query Interface

The `CommitRepository` ABC in `src/tract/storage/repositories.py` gains a new abstract method:

```python
@abstractmethod
def get_by_config(
    self, tract_id: str, json_path: str, operator: str, value: object
) -> Sequence[CommitRow]:
```

The interface accepts a JSON field name (like `"temperature"`), an operator (like `">"`), and a comparison value. This SQL-centric design pushes filtering to the database rather than loading all commits and filtering in Python.

The SQLite implementation in `src/tract/storage/sqlite.py` uses `func.json_extract()` to reach into the JSON blob:

```python
extracted = func.json_extract(CommitRow.generation_config_json, f'$.{json_path}')
```

The `$` prefix is JSON path syntax -- `$.temperature` extracts the `temperature` key from the JSON object. SQLAlchemy generates this into a `json_extract(generation_config_json, '$.temperature')` SQL call, which SQLite evaluates natively.

An operator map translates string operators into SQLAlchemy comparison expressions:

```python
ops = {
    "=": lambda e, v: e == v,
    "!=": lambda e, v: e != v,
    ">": lambda e, v: e > v,
    # ... etc
}
```

Commits with `NULL` generation_config_json are automatically excluded by SQL's NULL semantics -- `json_extract(NULL, '$.temperature') > 0.5` evaluates to NULL, which is falsy in a WHERE clause. This is the correct behavior: commits without generation config shouldn't appear in config-based queries.

### The Engine Layer

`CommitEngine.create_commit()` in `src/tract/engine/commit.py` gains a `generation_config` parameter. The threading is straightforward: the value passes through to the `CommitRow` constructor as `generation_config_json=generation_config` and appears in the returned `CommitInfo` as `generation_config=generation_config`.

Crucially, `generation_config` is *not* included in the hash computation. The `_compute_commit_hash()` and `_compute_content_hash()` methods continue to hash only the content, operation, and structural fields (parent_hash, created_at). This is what preserves content-addressable deduplication.

The `_row_to_info()` helper maps in the reverse direction: `row.generation_config_json` becomes `generation_config` on the CommitInfo. This conversion runs whenever commits are loaded from the database -- for `get_commit()`, `log()`, and `query_by_config()`.

### The Compiler

The `DefaultContextCompiler.compile()` method in `src/tract/engine/compiler.py` collects generation configs in a new "Step 4b" that runs after building the effective commit list (the list of commits that survive priority filtering and edit resolution).

The collection iterates over effective commits and applies the edit-inherits-original fallback. The result is a `list[dict]` where each element corresponds to one effective commit. Commits without any generation config (neither their own nor an edit's) get an empty dict `{}`, ensuring the list length always matches the number of effective commits.

This list is passed into `CompiledContext(generation_configs=generation_configs)` alongside the existing `messages`, `token_count`, `commit_count`, and `token_source` fields.

### The Protocol Layer

`src/tract/protocols.py` adds `generation_configs` to both dataclasses:

- `CompiledContext.generation_configs: list[dict]` -- mutable list returned to users, default `[]`
- `CompileSnapshot.generation_configs: tuple[dict, ...]` -- immutable tuple for caching, default `()`

The tuple vs. list distinction mirrors the existing pattern for `messages` (tuple in snapshot, list in compiled context). The tuple enforces that the snapshot can't be accidentally extended or reordered, while the list gives users a familiar mutable interface.

### The Tract Facade

`src/tract/tract.py` is where everything comes together. The changes touch six methods:

**`commit()`** adds the `generation_config` kwarg and passes it through to `self._commit_engine.create_commit()`. After the commit succeeds, the incremental cache update path (`_extend_snapshot_for_append()`) picks up the new config.

**`_extend_snapshot_for_append()`** reads `commit_row.generation_config_json or {}` and appends it to the snapshot's generation_configs tuple. This is the O(1) APPEND fast path -- no full recompile needed, just extend the tuple by one element.

**`_snapshot_to_compiled()`** converts snapshot tuples to compiled context lists with the copy-on-output pattern: `[dict(c) for c in snapshot.generation_configs]`. Every `compile()` call that hits the cache goes through this method.

**`_build_snapshot_from_compiled()`** does the reverse with copy-on-input: `tuple(dict(c) for c in result.generation_configs)`. This runs after a full compile when building a fresh snapshot from the result.

**`record_usage()`** preserves `self._compile_snapshot.generation_configs` when reconstructing the snapshot with updated token counts. It doesn't accept a generation_config parameter and doesn't modify the configs -- it only updates `token_count` and `token_source`.

**`query_by_config()`** is the new public method that delegates to the repository:

```python
def query_by_config(self, field: str, operator: str, value: object) -> list[CommitInfo]:
    rows = self._commit_repo.get_by_config(self._tract_id, field, operator, value)
    return [self._commit_engine._row_to_info(row) for row in rows]
```

It's a thin wrapper: the repository does the SQL-side filtering, the engine converts rows to CommitInfo objects. The result is a list of commits matching the condition, ordered chronologically.

## How It Connects

### Dependencies (What Phase 1.3 Builds On)

Generation config storage builds directly on three pieces of existing infrastructure:

**The JSON column pattern.** `metadata_json` on CommitRow established the pattern of nullable JSON blobs for flexible, unstructured data. `generation_config_json` follows the same column type, nullability, and serialization behavior. If you understand one, you understand both.

**The CompileSnapshot cache.** The incremental compile cache from Phase 1.1 already handles tuple-based field extension for APPEND commits and full invalidation for EDIT/annotate. Generation configs plug into this machinery -- `_extend_snapshot_for_append()` just appends one more config to the tuple, and EDIT invalidation clears everything including configs.

**The edit_map in the compiler.** The edit resolution logic (Phase 1, Plan 02) already builds a map from original commit hashes to their latest edit commits. The edit-inherits-original fallback for generation configs simply reads from this existing map -- no new data structures needed.

### Dependents (What Will Build On Phase 1.3)

**Phase 2 (Linear History & CLI)** will display generation configs in `log` and `status` output. The `log()` method already returns `CommitInfo` objects with `generation_config` populated, so the CLI just needs to format it.

**Phase 2's `diff`** could show config differences between commits -- "this commit used temperature 0.3, that one used 0.9" -- providing a dimension of comparison beyond content changes.

**Phase 3 (Branching)** enables the exploration/exploitation pattern. With branches, you can fork context, run each branch with different generation configs, and then compare results. The `query_by_config()` method makes it easy to analyze which configs produced which outcomes across branches.

### Ripple Points

If you changed the generation_config field name, you'd need to touch: `CommitInfo` (model), `CommitRow` (schema), `CommitEngine` (create_commit and _row_to_info), `DefaultContextCompiler` (Step 4b collection), `CompiledContext` and `CompileSnapshot` (protocols), and all six Tract methods that handle configs. The key link files are listed in the plan's `key_links` section -- follow them if you ever need to trace the data flow.

If you wanted to add validation (e.g., "temperature must be 0-2"), the right place would be `Tract.commit()` before passing to the engine. The engine and storage layers should remain validation-free to keep the layer separation clean.

## Usage Examples

### Basic: Committing with Generation Config

```python
from tract import Tract
from tract.models import DialogueContent

tract = Tract.open("my-agent")

# Commit with the generation params used for this API call
info = tract.commit(
    DialogueContent(role="assistant", text="The answer is 42."),
    generation_config={
        "model": "gpt-4o",
        "temperature": 0.7,
        "top_p": 0.95,
        "max_tokens": 1024,
    },
)

# The config is stored and returned
print(info.generation_config)
# {"model": "gpt-4o", "temperature": 0.7, "top_p": 0.95, "max_tokens": 1024}

# Retrieve it later
fetched = tract.get_commit(info.commit_hash)
print(fetched.generation_config)
# {"model": "gpt-4o", "temperature": 0.7, "top_p": 0.95, "max_tokens": 1024}
```

Commits without generation config still work exactly as before -- the field is `None`:

```python
info = tract.commit(DialogueContent(role="user", text="What is 6 * 7?"))
assert info.generation_config is None
```

### Compilation: Reading Configs from CompiledContext

After committing several messages with different configs, `compile()` returns a `generation_configs` list parallel to the effective commits:

```python
tract.commit(
    InstructionContent(text="You are a helpful assistant."),
    generation_config={"model": "gpt-4o"},
)
tract.commit(
    DialogueContent(role="user", text="Hello"),
    # No generation config -- user messages aren't generated
)
tract.commit(
    DialogueContent(role="assistant", text="Hi there!"),
    generation_config={"model": "gpt-4o", "temperature": 0.7},
)

result = tract.compile()
print(len(result.generation_configs))  # 3 -- one per effective commit
print(result.generation_configs[0])    # {"model": "gpt-4o"}
print(result.generation_configs[1])    # {} -- no config for user message
print(result.generation_configs[2])    # {"model": "gpt-4o", "temperature": 0.7}
```

The incremental cache handles this efficiently. If you append another commit and compile again, only the new config is added -- no full recompile:

```python
tract.commit(
    DialogueContent(role="user", text="Tell me more"),
)
result = tract.compile()  # O(1) -- extends cached snapshot
print(len(result.generation_configs))  # 4
print(result.generation_configs[3])    # {} -- no config
```

### Edit Inheritance

When you edit a commit without providing a new generation config, the original's config is preserved:

```python
original = tract.commit(
    DialogueContent(role="assistant", text="The answer is 41."),
    generation_config={"model": "gpt-4o", "temperature": 0.3},
)

# Fix a typo -- no new API call, so no new generation config
tract.commit(
    DialogueContent(role="assistant", text="The answer is 42."),
    operation=CommitOperation.EDIT,
    edit_target=original.commit_hash,
    # generation_config not specified -- inherits original's
)

result = tract.compile()
# The effective commit's config is the original's
print(result.generation_configs[0])
# {"model": "gpt-4o", "temperature": 0.3}
```

If you *do* provide a config on the edit (because you re-generated with different params), the edit's config takes precedence:

```python
tract.commit(
    DialogueContent(role="assistant", text="The answer is 42, obviously."),
    operation=CommitOperation.EDIT,
    edit_target=original.commit_hash,
    generation_config={"model": "gpt-4o", "temperature": 0.9},  # Re-generated
)

result = tract.compile()
print(result.generation_configs[0])
# {"model": "gpt-4o", "temperature": 0.9} -- edit's config wins
```

### Querying by Config Values

`query_by_config()` uses SQL-side JSON extraction to find commits matching a condition:

```python
# Find all commits generated with temperature > 0.8
hot_commits = tract.query_by_config("temperature", ">", 0.8)
for c in hot_commits:
    print(f"{c.commit_hash[:8]}: temp={c.generation_config['temperature']}")

# Find all commits from a specific model
gpt_commits = tract.query_by_config("model", "=", "gpt-4o")
print(f"{len(gpt_commits)} commits generated by gpt-4o")

# Supported operators: =, !=, >, <, >=, <=
# Unsupported operators raise ValueError
try:
    tract.query_by_config("model", "LIKE", "gpt%")
except ValueError as e:
    print(e)  # "Unsupported operator: LIKE. Use one of: ['=', '!=', '>', '<', '>=', '<=']"
```

Commits without any generation config are automatically excluded from query results -- they have `NULL` in the JSON column, and SQL's NULL comparison semantics filter them out.

### Cache Safety

The copy-on-output pattern means you can safely mutate returned configs without corrupting the cache:

```python
result1 = tract.compile()
result1.generation_configs[0]["temperature"] = 999  # Mutate returned dict

result2 = tract.compile()  # Hits cache, returns fresh copies
assert result2.generation_configs[0]["temperature"] != 999  # Cache is safe
```

This is a property you can rely on -- the cache and the returned CompiledContext share no mutable references.

---

*Phase: 01.3-hyperparameter-config-storage*
*Files: models/commit.py, storage/schema.py, storage/repositories.py, storage/sqlite.py, engine/commit.py, engine/compiler.py, protocols.py, tract.py*
*Tests: 30 new across 4 test files (250 total)*
*Completed: 2026-02-11*
