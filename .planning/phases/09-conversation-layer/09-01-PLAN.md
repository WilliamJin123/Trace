---
phase: 09-conversation-layer
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/tract/protocols.py
  - src/tract/tract.py
  - src/tract/__init__.py
  - tests/test_conversation.py
autonomous: true

must_haves:
  truths:
    - "User can pass api_key, model, base_url to Tract.open() and have LLM ready without configure_llm()"
    - "User can call t.chat('question') and get a ChatResponse with .text containing the LLM reply"
    - "chat() does user commit + compile + LLM call + assistant commit + record_usage in one call"
    - "User can call t.user('question') then t.generate() for explicit two-step control"
    - "ChatResponse exposes .text, .usage, .commit_info, .generation_config"
    - "generation_config is auto-populated from LLM response model and request params"
    - "record_usage() is auto-called with API-reported token counts"
  artifacts:
    - path: "src/tract/protocols.py"
      provides: "ChatResponse frozen dataclass"
      contains: "class ChatResponse"
    - path: "src/tract/tract.py"
      provides: "chat(), generate(), _build_generation_config(), LLM params on open()"
      contains: "def chat("
    - path: "src/tract/__init__.py"
      provides: "ChatResponse export"
      contains: "ChatResponse"
    - path: "tests/test_conversation.py"
      provides: "Tests for chat/generate/ChatResponse/open LLM config"
      min_lines: 150
  key_links:
    - from: "src/tract/tract.py (chat)"
      to: "src/tract/tract.py (user + generate)"
      via: "chat delegates to user() then generate()"
      pattern: "self\\.user\\(.*\\).*self\\.generate\\("
    - from: "src/tract/tract.py (generate)"
      to: "src/tract/llm/client.py"
      via: "self._llm_client.chat(messages)"
      pattern: "_llm_client\\.chat\\("
    - from: "src/tract/tract.py (generate)"
      to: "src/tract/tract.py (assistant)"
      via: "auto-commits assistant response with generation_config"
      pattern: "self\\.assistant\\(.*generation_config="
    - from: "src/tract/tract.py (generate)"
      to: "src/tract/tract.py (record_usage)"
      via: "auto-records usage from LLM response"
      pattern: "self\\.record_usage\\("
    - from: "src/tract/tract.py (open)"
      to: "src/tract/llm/client.py (OpenAIClient)"
      via: "auto-creates client when api_key provided"
      pattern: "OpenAIClient\\("
---

<objective>
Add conversation layer to Tract: ChatResponse dataclass, LLM config on Tract.open(), chat() and generate() convenience methods that compose existing primitives (user/assistant/compile/record_usage) into one-call operations.

Purpose: Reduce the #1 use case (multi-turn LLM chat with version control) from ~7 lines per turn to 1 line per turn.
Output: ChatResponse in protocols.py, chat()/generate()/_build_generation_config() on Tract, LLM params on Tract.open(), updated exports, comprehensive tests.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-conversation-layer/09-RESEARCH.md
@src/tract/protocols.py
@src/tract/tract.py
@src/tract/llm/client.py
@src/tract/llm/errors.py
@src/tract/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: ChatResponse dataclass, LLM config on Tract.open(), close() lifecycle</name>
  <files>
    src/tract/protocols.py
    src/tract/tract.py
    src/tract/__init__.py
  </files>
  <action>
**1. Add ChatResponse to protocols.py** (after the TokenUsage class, before TokenCounter protocol):

```python
@dataclass(frozen=True)
class ChatResponse:
    """Response from Tract.chat() or Tract.generate().

    Attributes:
        text: The assistant's response text.
        usage: Token usage from the API, or None if not reported.
        commit_info: CommitInfo for the assistant's commit.
        generation_config: The generation config captured from the request/response.
    """
    text: str
    usage: TokenUsage | None
    commit_info: CommitInfo
    generation_config: dict
```

This requires importing CommitInfo. Add to the TYPE_CHECKING block:
```python
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from tract.models.commit import CommitInfo
```

Since CommitInfo is used in a frozen dataclass field annotation and the class needs to be available at runtime for dataclass, use a string annotation `"CommitInfo"` or a forward reference. Actually, since `from __future__ import annotations` is already at the top of protocols.py, all annotations are strings -- so the TYPE_CHECKING import is sufficient.

**2. Add LLM params to Tract.open():**

Add three new keyword-only params to the `open()` classmethod signature (after `verify_cache`):
- `api_key: str | None = None`
- `model: str | None = None`
- `base_url: str | None = None`

After the tract is constructed (after `tract._policy_repo = policy_repo` and the auto-load policy config block, just before `return tract`), add:

```python
# Auto-configure LLM if api_key provided
if api_key is not None:
    from tract.llm.client import OpenAIClient
    client = OpenAIClient(
        api_key=api_key,
        base_url=base_url,
        default_model=model or "gpt-4o-mini",
    )
    tract.configure_llm(client)
    tract._owns_llm_client = True
    tract._default_model = model
```

**3. Initialize ownership tracking in __init__:**

In `Tract.__init__()`, add after `self._token_trigger_fired: bool = False`:
```python
self._owns_llm_client: bool = False
self._default_model: str | None = None
```

**4. Update Tract.close() for LLM client lifecycle:**

In the `close()` method, after `self._closed = True` and before `self._session.close()`, add:
```python
# Close internally-created LLM client (not externally-provided ones)
if self._owns_llm_client and hasattr(self, "_llm_client"):
    self._llm_client.close()
```

**5. Update __init__.py exports:**

Add import: `from tract.protocols import ChatResponse` (add to existing protocols import line).
Add `"ChatResponse"` to `__all__` in the Protocols section.

**Do NOT:**
- Add env var auto-detection on Tract.open() (explicit api_key only)
- Add async support
- Create a separate conversation.py module
  </action>
  <verify>
Run: `python -c "from tract import ChatResponse; from tract.protocols import ChatResponse; print('ChatResponse imported OK')"` succeeds.
Run: `python -c "from tract import Tract; import inspect; sig = inspect.signature(Tract.open); assert 'api_key' in sig.parameters; assert 'model' in sig.parameters; assert 'base_url' in sig.parameters; print('Tract.open() params OK')"` succeeds.
Run existing tests: `python -m pytest tests/ -x -q --tb=short` -- all 921 existing tests still pass.
  </verify>
  <done>
ChatResponse is a frozen dataclass in protocols.py with .text, .usage, .commit_info, .generation_config fields.
Tract.open() accepts api_key, model, base_url params and auto-configures LLM when api_key provided.
Tract.close() closes internally-created LLM clients but not externally-provided ones.
ChatResponse is exported from tract.__init__.py.
All existing tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: chat(), generate(), _build_generation_config() methods and tests</name>
  <files>
    src/tract/tract.py
    tests/test_conversation.py
  </files>
  <action>
**1. Add _build_generation_config() private method on Tract:**

Place it after the `assistant()` method (after line ~593), in a new section:

```python
# ------------------------------------------------------------------
# Conversation layer (chat/generate)
# ------------------------------------------------------------------

def _build_generation_config(
    self,
    response: dict,
    *,
    model: str | None = None,
    temperature: float | None = None,
    max_tokens: int | None = None,
) -> dict:
    """Build generation_config from LLM response and request params.

    The response's model field is authoritative (actual model used may
    differ from requested model due to aliases/routing).
    """
    config: dict = {}
    # Model: prefer response (authoritative) over request param
    if "model" in response:
        config["model"] = response["model"]
    elif model is not None:
        config["model"] = model
    elif self._default_model is not None:
        config["model"] = self._default_model
    # Request params
    if temperature is not None:
        config["temperature"] = temperature
    if max_tokens is not None:
        config["max_tokens"] = max_tokens
    return config
```

**2. Add generate() method on Tract:**

```python
def generate(
    self,
    *,
    model: str | None = None,
    temperature: float | None = None,
    max_tokens: int | None = None,
    message: str | None = None,
    metadata: dict | None = None,
) -> ChatResponse:
    """Compile context, call LLM, commit assistant response, record usage.

    Assumes the conversation context (system prompt, user messages) has
    already been committed. Use :meth:`chat` for the all-in-one path.

    Args:
        model: Model override for this call.
        temperature: Temperature override.
        max_tokens: Max tokens override.
        message: Optional commit message for the assistant commit.
        metadata: Optional metadata for the assistant commit.

    Returns:
        :class:`ChatResponse` with text, usage, commit_info, generation_config.

    Raises:
        LLMConfigError: If no LLM client is configured.
        TraceError: If called inside batch().
    """
    if not hasattr(self, "_llm_client"):
        from tract.llm.errors import LLMConfigError
        raise LLMConfigError(
            "No LLM client configured. Pass api_key to Tract.open() "
            "or call configure_llm(client)."
        )

    if self._in_batch:
        raise TraceError("chat()/generate() cannot be used inside batch()")

    from tract.llm.client import OpenAIClient
    from tract.protocols import ChatResponse

    # 1. Compile context
    compiled = self.compile()
    messages = compiled.to_dicts()

    # 2. Call LLM
    llm_kwargs: dict = {}
    if model is not None:
        llm_kwargs["model"] = model
    if temperature is not None:
        llm_kwargs["temperature"] = temperature
    if max_tokens is not None:
        llm_kwargs["max_tokens"] = max_tokens

    response = self._llm_client.chat(messages, **llm_kwargs)

    # 3. Extract content and usage
    text = OpenAIClient.extract_content(response)
    usage_dict = OpenAIClient.extract_usage(response)

    # 4. Build generation_config
    gen_config = self._build_generation_config(
        response, model=model, temperature=temperature, max_tokens=max_tokens
    )

    # 5. Commit assistant response
    commit_info = self.assistant(
        text, message=message, metadata=metadata, generation_config=gen_config
    )

    # 6. Record usage
    usage = None
    if usage_dict:
        usage = self._normalize_usage_dict(usage_dict)
        self.record_usage(usage)

    return ChatResponse(
        text=text,
        usage=usage,
        commit_info=commit_info,
        generation_config=gen_config,
    )
```

**3. Add chat() method on Tract:**

```python
def chat(
    self,
    text: str,
    *,
    model: str | None = None,
    temperature: float | None = None,
    max_tokens: int | None = None,
    message: str | None = None,
    name: str | None = None,
    metadata: dict | None = None,
) -> ChatResponse:
    """Send a user message and get an LLM response in one call.

    Commits the user message, compiles context, calls the LLM,
    commits the assistant response, and records usage. Equivalent to::

        t.user(text, message=message, name=name, metadata=metadata)
        response = t.generate(model=model, temperature=temperature, ...)

    Args:
        text: The user message text.
        model: Model override for this call.
        temperature: Temperature override.
        max_tokens: Max tokens override.
        message: Optional commit message for the user commit.
        name: Optional speaker name for the user commit.
        metadata: Optional metadata for the user commit.

    Returns:
        :class:`ChatResponse` with text, usage, commit_info, generation_config.

    Raises:
        LLMConfigError: If no LLM client is configured.
        DetachedHeadError: If HEAD is detached.
        TraceError: If called inside batch().
    """
    # Commit user message
    self.user(text, message=message, name=name, metadata=metadata)
    # Delegate to generate
    return self.generate(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
    )
```

**4. Create comprehensive test file tests/test_conversation.py:**

Use the established MockLLMClient pattern. Create a mock that returns predictable responses with usage. Test categories:

1. **ChatResponse model tests:**
   - Frozen dataclass: create instance, verify fields, verify immutability (raises FrozenInstanceError on setattr)
   - usage=None is valid

2. **Tract.open() LLM config tests:**
   - `Tract.open(api_key="test-key")` creates an LLM client (verify `hasattr(t, "_llm_client")`)
   - `Tract.open(api_key="test-key", model="gpt-4o")` stores `_default_model`
   - `Tract.open(api_key="test-key", base_url="http://localhost:8080/v1")` forwards base_url
   - `Tract.open()` without api_key does NOT create LLM client
   - For api_key tests: mock OpenAIClient to avoid real HTTP calls. Use `monkeypatch` to patch `tract.llm.client.OpenAIClient.__init__` to accept any key without HTTP validation, or better: patch the import inside Tract.open(). Simplest: use monkeypatch to set env var TRACT_OPENAI_API_KEY and verify the client was created. Actually simplest: patch `tract.tract.OpenAIClient` (but it's imported lazily). Use `monkeypatch.setattr("tract.llm.client.OpenAIClient.__init__", mock_init)` approach, or just test via the mock client pattern.

   Better approach: Since OpenAIClient.__init__ validates the key, mock at the module level. Use:
   ```python
   def test_open_with_api_key(monkeypatch):
       # Patch OpenAIClient to not make HTTP calls
       mock_client = MockLLMClient()
       monkeypatch.setattr("tract.llm.client.OpenAIClient", lambda **kwargs: mock_client)
       t = Tract.open(api_key="test-key")
       assert hasattr(t, "_llm_client")
       assert t._owns_llm_client is True
       t.close()
   ```
   Wait -- OpenAIClient is imported inside Tract.open() with `from tract.llm.client import OpenAIClient`. So monkeypatching `tract.llm.client.OpenAIClient` works.

3. **close() lifecycle tests:**
   - Internally-created client (via open api_key) is closed on Tract.close()
   - Externally-provided client (via configure_llm) is NOT closed on Tract.close()
   - Track with a mock client that has a `closed` flag

4. **generate() happy path:**
   - Setup: Tract.open() + configure_llm(MockLLMClient) + system() + user() + generate()
   - Verify: ChatResponse returned, .text matches mock response, .usage is TokenUsage, .commit_info is CommitInfo, .generation_config has model
   - Verify: assistant commit was created (check log)
   - Verify: usage was recorded (check compile token_source)

5. **chat() happy path:**
   - Setup: Tract.open() + configure_llm(MockLLMClient) + system() + chat("hello")
   - Verify: ChatResponse returned, user commit + assistant commit both exist
   - Verify: compiled context has system + user + assistant messages

6. **Multi-turn chat:**
   - chat("q1") then chat("q2") -- verify 5 commits (system + user1 + asst1 + user2 + asst2)

7. **generate() with explicit params:**
   - generate(model="gpt-4o", temperature=0.7, max_tokens=100)
   - Verify generation_config contains all three params
   - Verify mock client received the kwargs

8. **Auto generation_config tests:**
   - Response contains `"model": "gpt-4o-2024-01-01"` (actual resolved model)
   - Request had `model="gpt-4o"` (alias)
   - Verify generation_config uses response model (authoritative), not request model

9. **Error cases:**
   - generate() without LLM configured raises LLMConfigError
   - chat() without LLM configured raises LLMConfigError
   - generate() inside batch() raises TraceError
   - chat() inside batch() raises TraceError

10. **generate() with message= and metadata=:**
    - Verify the assistant commit uses the custom message
    - Verify metadata is passed through

The MockLLMClient for these tests:
```python
class MockLLMClient:
    def __init__(self, responses=None, model="mock-model"):
        self.responses = responses or ["Mock response"]
        self._call_count = 0
        self.last_messages = None
        self.last_kwargs = {}
        self._model = model
        self.closed = False

    def chat(self, messages, **kwargs):
        self.last_messages = messages
        self.last_kwargs = kwargs
        text = self.responses[min(self._call_count, len(self.responses) - 1)]
        self._call_count += 1
        return {
            "choices": [{"message": {"content": text}}],
            "usage": {"prompt_tokens": 10, "completion_tokens": 5, "total_tokens": 15},
            "model": kwargs.get("model", self._model),
        }

    def close(self):
        self.closed = True
```

Aim for approximately 20-25 test functions covering all categories above.
  </action>
  <verify>
Run: `python -m pytest tests/test_conversation.py -v --tb=short` -- all new tests pass.
Run: `python -m pytest tests/ -x -q --tb=short` -- full suite passes (921 existing + ~20 new).
Run: `python -c "from tract import Tract, ChatResponse; t = Tract.open(); print('No LLM by default:', not hasattr(t, '_llm_client')); t.close()"` prints True.
  </verify>
  <done>
chat() does user commit + compile + LLM call + assistant commit + record_usage in one call.
generate() does compile + LLM call + assistant commit + record_usage for two-step workflow.
_build_generation_config() captures model from response (authoritative), temperature and max_tokens from request.
Error guards: LLMConfigError if no client, TraceError if inside batch().
ChatResponse has .text, .usage, .commit_info, .generation_config.
All tests pass (~940+ total).
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/ -x -q --tb=short` -- full suite passes with no failures
2. `python -c "from tract import Tract, ChatResponse"` -- imports work
3. `python -c "from tract.protocols import ChatResponse; r = ChatResponse(text='hi', usage=None, commit_info=None, generation_config={}); print(r.text)"` -- ChatResponse constructible
4. Verify Tract.open() signature includes api_key, model, base_url params
5. Verify chat() and generate() are callable methods on Tract instances
</verification>

<success_criteria>
1. Tract.open(api_key="...", model="...", base_url="...") auto-configures LLM without separate configure_llm() call
2. t.chat("question") returns ChatResponse with .text containing LLM reply, performing user commit + compile + LLM call + assistant commit + usage recording
3. t.user("question") + t.generate() provides explicit two-step control, returning same ChatResponse
4. ChatResponse exposes .text (str), .usage (TokenUsage|None), .commit_info (CommitInfo), .generation_config (dict)
5. generation_config auto-captured from response model (authoritative) + request params (temperature, max_tokens)
6. record_usage() auto-called with API token counts after every chat()/generate()
7. All 6 requirements covered: LLM-01 (open config), LLM-02 (auto gen_config), LLM-03 (auto usage), CONV-01 (chat), CONV-02 (generate), CONV-03 (ChatResponse)
</success_criteria>

<output>
After completion, create `.planning/phases/09-conversation-layer/09-01-SUMMARY.md`
</output>
