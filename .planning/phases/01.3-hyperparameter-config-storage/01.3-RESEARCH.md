# Phase 1.3: Hyperparameter Config Storage - Research

**Researched:** 2026-02-11
**Domain:** LLM generation config (hyperparameter) storage, JSON querying in SQLite, schema-flexible parameter storage
**Confidence:** HIGH (internal codebase analysis, authoritative SQLite docs, cross-verified patterns)

## Summary

Phase 1.3 extends the commit model to store LLM generation configuration (temperature, top_p, model name, etc.) alongside each commit, giving full provenance for how context was generated. This is a data-model extension that follows the exact same pattern as the existing `metadata_json` column on `CommitRow` -- a nullable JSON blob that flows from the public API through the engine to storage and back.

The domain research reveals three key insights:

1. **LLM generation configs are inherently provider-diverse.** OpenAI, Anthropic, Google, Meta, and open-source providers each use different parameter names and ranges. A fixed schema would require constant migration. The correct approach is a flexible JSON blob (like `metadata_json`) that stores whatever the user passes, with optional Pydantic validation for known fields.

2. **SQLite JSON querying is mature and well-supported.** SQLite's `json_extract()` function (available since 3.9+, which predates all supported Python versions) enables efficient filtering by JSON field values. SQLAlchemy exposes this via `func.json_extract()`. For frequently-queried fields (e.g., temperature, model), virtual generated columns with indexes can be added later without schema migration.

3. **The existing codebase already has the exact pattern needed.** `metadata_json` on `CommitRow` is a nullable `JSON` column that flows through `CommitEngine.create_commit()` -> `CommitRow` -> `CommitInfo.metadata`. `generation_config` follows the identical path. The heaviest lift is wiring it into `CompiledContext` (SC3) and `record_usage()` (SC4), plus adding query methods (SC5).

**Primary recommendation:** Add `generation_config_json` as a nullable `JSON` column on `CommitRow`, a corresponding `generation_config: Optional[dict]` field on `CommitInfo`, and wire it through the commit/compile/record_usage pipeline. Use `func.json_extract()` for querying. No new external libraries needed.

## Standard Stack

### Core

No new external libraries needed. This phase is entirely internal architecture work using existing dependencies.

| Component | Version | Purpose | Why Standard |
|-----------|---------|---------|--------------|
| SQLAlchemy `JSON` column | 2.0.x (existing) | Store generation_config as JSON blob | Already used for `metadata_json` on `CommitRow` |
| SQLAlchemy `func.json_extract()` | 2.0.x (existing) | Query JSON fields in WHERE clauses | Standard SQLAlchemy approach for SQLite JSON |
| Pydantic `BaseModel` | 2.10.x (existing) | Optional validation model for known params | Already used throughout models/ |
| SQLite `json_extract()` | 3.9+ (guaranteed) | Backend for JSON field queries | Bundled with all Python 3.10+ |

### Supporting

| Component | Version | Purpose | When to Use |
|-----------|---------|---------|-------------|
| SQLAlchemy `Index` | 2.0.x (existing) | Index on generated columns | If query performance needs optimization |
| `dataclasses` | stdlib | Extend `CompiledContext` or `CompileSnapshot` | If generation_configs list is needed in output |

### Alternatives Considered

| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| JSON blob column | Separate normalized table (key-value per param) | Normalized is queryable but adds complexity; JSON blob is simpler, matches metadata pattern, and SQLite json_extract covers querying |
| Flexible dict | Pydantic model with strict schema | Strict schema would reject unknown provider params; dict is more resilient |
| `func.json_extract()` for queries | Virtual generated columns + indexes | Generated columns are faster for hot queries but add schema complexity; json_extract is sufficient for Phase 1 volumes |
| Storing config on CommitRow | Storing config in a separate linked table | Separate table adds joins and complexity; inline JSON is simpler and generation_config is always 1:1 with commit |

## Architecture Patterns

### Recommended Approach: Follow the metadata_json Pattern

The existing `metadata_json` column on `CommitRow` provides a proven, tested pattern for nullable JSON blobs that flow through the full stack. `generation_config` should follow the identical pattern at every layer:

```
src/tract/
├── models/
│   └── commit.py           # Add generation_config: Optional[dict] to CommitInfo
├── storage/
│   └── schema.py           # Add generation_config_json: JSON column to CommitRow
├── engine/
│   └── commit.py           # Accept generation_config in create_commit(), pass through
├── protocols.py             # Add generation_configs to CompiledContext
└── tract.py                 # Accept generation_config in commit() and record_usage()
```

### Pattern 1: JSON Blob Column (Same as metadata_json)

**What:** Store generation_config as a nullable JSON column on CommitRow, exactly like metadata_json.
**When to use:** Always -- this is the pattern for flexible, provider-agnostic dict storage.
**Example:**

```python
# schema.py -- CommitRow addition
generation_config_json: Mapped[Optional[dict]] = mapped_column(JSON, nullable=True)

# commit.py -- CommitInfo addition
generation_config: Optional[dict] = None

# engine/commit.py -- create_commit() addition
def create_commit(
    self,
    content: BaseModel,
    operation: CommitOperation = CommitOperation.APPEND,
    message: str | None = None,
    edit_target: str | None = None,
    metadata: dict | None = None,
    generation_config: dict | None = None,  # NEW
) -> CommitInfo:
```

Source: Existing codebase pattern (`metadata_json` on `CommitRow`)

### Pattern 2: JSON Field Querying via func.json_extract()

**What:** Use SQLAlchemy's `func.json_extract()` to filter commits by generation_config field values.
**When to use:** For SC5 -- querying commits by config values (e.g., "all commits with temperature > 0.8").
**Example:**

```python
# In a new query method on CommitRepository / SqliteCommitRepository
from sqlalchemy import func

stmt = (
    select(CommitRow)
    .where(
        CommitRow.tract_id == tract_id,
        func.json_extract(CommitRow.generation_config_json, '$.temperature') > 0.8,
    )
    .order_by(CommitRow.created_at)
)
```

Source: [SQLite JSON Functions And Operators](https://www.sqlite.org/json1.html), verified with SQLAlchemy 2.0 `func` interface

### Pattern 3: Generation Configs in CompiledContext

**What:** Expose generation configs associated with compiled commits in the CompiledContext output.
**When to use:** SC3 -- users need to see which generation params produced which parts of the compiled context.
**Example:**

```python
# protocols.py -- CompiledContext extension
@dataclass(frozen=True)
class CompiledContext:
    messages: list[Message] = field(default_factory=list)
    token_count: int = 0
    commit_count: int = 0
    token_source: str = ""
    generation_configs: list[dict] = field(default_factory=list)  # NEW: per-effective-commit
```

The `generation_configs` list is parallel to the effective commits (not aggregated messages). Each entry is the generation_config dict for the corresponding effective commit, or an empty dict if none was set.

### Anti-Patterns to Avoid

- **Strict Pydantic schema for generation_config**: Different providers use different parameter names (OpenAI: `frequency_penalty`, Anthropic: doesn't use it; Google: `top_k`, OpenAI: doesn't expose it by default). A strict schema would reject valid configs from providers we haven't anticipated. Use a plain dict.
- **Storing generation_config in the blob/content**: Generation config is metadata about how the content was produced, not part of the content itself. It should not affect content hashing or deduplication.
- **Including generation_config in commit hash computation**: The commit hash identifies content provenance. Generation config is orthogonal -- the same prompt could be sent with different temperatures. Including it in the hash would break dedup for identical content. Keep it out of the hash, same as metadata.
- **Creating a separate table for generation_config**: A 1:1 relationship with commits means a separate table just adds JOINs. Inline JSON column is simpler and follows the metadata_json precedent.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| JSON field querying in SQLite | Custom Python-side filtering (load all rows, filter in Python) | `func.json_extract()` in SQL WHERE clause | SQL-side filtering is orders of magnitude faster and avoids loading all rows |
| Provider-specific config normalization | Custom normalizer for each provider's param names | Just store the dict as-is | Provider param names are already meaningful to users; normalizing adds mapping maintenance and loses fidelity |
| JSON validation for generation_config | Custom validation logic | Optional Pydantic model for common fields (temperature, top_p, model) with `extra="allow"` | Validates known fields while allowing unknown ones |

**Key insight:** The existing metadata_json pattern already solves 90% of this problem. The new work is (a) wiring a second JSON column through the same path, (b) surfacing configs in CompiledContext, and (c) adding query methods.

## Common Pitfalls

### Pitfall 1: Breaking the Commit Hash by Including generation_config

**What goes wrong:** If generation_config is included in `commit_hash()` computation, two commits with identical content but different temperatures would get different hashes. This breaks content deduplication and makes the DAG larger than necessary.
**Why it happens:** Natural instinct to include "everything about a commit" in the hash.
**How to avoid:** Explicitly exclude generation_config from `commit_hash()` in `engine/hashing.py`. Same decision was already made for `metadata` -- it is not part of the hash.
**Warning signs:** Different hashes for commits with identical content, parent, type, operation, and timestamp.

### Pitfall 2: Mutating generation_config After Commit

**What goes wrong:** If the user passes a dict and later mutates it, the stored value could change unexpectedly.
**Why it happens:** Python dicts are mutable references.
**How to avoid:** Deep-copy or `json.dumps()`/`json.loads()` round-trip the dict at storage time. The existing `CommitRow.metadata_json` with `JSON` column type already handles this correctly because SQLAlchemy serializes to JSON on write and deserializes on read. The same applies to `generation_config_json`.
**Warning signs:** Stored config differs from what was passed.

### Pitfall 3: Bloating CompiledContext with Redundant Configs

**What goes wrong:** If every message in CompiledContext carries a full generation_config copy, the output grows significantly, especially with verbose configs.
**Why it happens:** Naive implementation copies config per-message.
**How to avoid:** Attach configs at the effective-commit level (parallel list), not per-aggregated-message. Users who need per-message mapping can correlate by index. Consider making generation_configs optional (default empty list) so existing code is unaffected.
**Warning signs:** CompiledContext serialization size balloons.

### Pitfall 4: Breaking Compile Cache Invariants

**What goes wrong:** The incremental compile cache (`CompileSnapshot`) stores pre-computed compilation state. If generation_configs are added to `CompiledContext` but the snapshot doesn't track them, cache hits return stale/empty configs.
**Why it happens:** The snapshot-to-compiled conversion needs to carry generation_configs through.
**How to avoid:** Add `generation_configs` to `CompileSnapshot` and update `_build_snapshot_from_compiled()`, `_snapshot_to_compiled()`, and `_extend_snapshot_for_append()` in `tract.py`.
**Warning signs:** `compile()` returns empty generation_configs after cache hit but non-empty after cache miss.

### Pitfall 5: SQLite JSON Type Coercion in Queries

**What goes wrong:** `json_extract()` returns TEXT for string values, but comparisons may fail if types don't match. For example, `json_extract(..., '$.temperature') > 0.8` works because SQLite returns REAL for numeric JSON values, but `json_extract(..., '$.model') = 'gpt-4'` works because it returns TEXT.
**Why it happens:** SQLite's type affinity system.
**How to avoid:** Use `->>` operator (SQLite 3.38.0+) which always returns native SQL types, or use `json_extract()` which also returns native types for scalar values. Both work correctly. Test with actual SQLite queries in integration tests.
**Warning signs:** Query returns no results when matching data exists, or unexpected type comparison failures.

## Code Examples

### Example 1: Commit with generation_config (Public API)

```python
from tract import Tract, DialogueContent

with Tract.open() as t:
    t.commit(
        DialogueContent(role="user", text="Tell me a joke"),
        generation_config={
            "model": "gpt-4o",
            "temperature": 0.9,
            "top_p": 0.95,
            "max_tokens": 500,
        },
    )
    # Retrieve it back
    info = t.log(limit=1)[0]
    assert info.generation_config["temperature"] == 0.9
```

### Example 2: generation_config in CompiledContext

```python
result = t.compile()
# generation_configs is parallel to effective commits
for i, config in enumerate(result.generation_configs):
    if config.get("temperature", 0) > 0.8:
        print(f"Commit {i} used high temperature: {config['temperature']}")
```

### Example 3: Query commits by config value

```python
# Find all commits with temperature > 0.8
hot_commits = t.query_by_config("temperature", ">", 0.8)

# Find all commits using a specific model
gpt4_commits = t.query_by_config("model", "=", "gpt-4o")
```

### Example 4: record_usage() with generation_config

```python
result = t.record_usage(
    {"prompt_tokens": 100, "completion_tokens": 50, "total_tokens": 150},
    generation_config={"model": "gpt-4o", "temperature": 0.7},
)
```

### Example 5: SQLAlchemy JSON query (internal)

```python
from sqlalchemy import func, select
from tract.storage.schema import CommitRow

# In SqliteCommitRepository
def get_by_config(
    self, tract_id: str, json_path: str, operator: str, value
) -> Sequence[CommitRow]:
    extracted = func.json_extract(CommitRow.generation_config_json, f'$.{json_path}')
    if operator == ">":
        condition = extracted > value
    elif operator == "=":
        condition = extracted == value
    # ... etc
    stmt = (
        select(CommitRow)
        .where(CommitRow.tract_id == tract_id, condition)
        .order_by(CommitRow.created_at)
    )
    return list(self._session.execute(stmt).scalars().all())
```

## LLM Generation Config Parameter Reference

A comprehensive reference of generation parameters across major providers. This justifies the flexible-dict approach -- no fixed schema can cover all of these without constant updates.

### Common Parameters (most providers)

| Parameter | Type | Range | Providers |
|-----------|------|-------|-----------|
| `model` | str | - | All |
| `temperature` | float | 0.0 - 2.0 | OpenAI, Anthropic, Google, Meta, Mistral |
| `top_p` | float | 0.0 - 1.0 | OpenAI, Anthropic, Google, Meta, Mistral |
| `max_tokens` | int | 1+ | OpenAI, Anthropic, Google |

### Provider-Specific Parameters

| Parameter | Type | Range | Providers |
|-----------|------|-------|-----------|
| `top_k` | int | 1 - 100+ | Anthropic, Google, Meta (NOT OpenAI) |
| `frequency_penalty` | float | -2.0 - 2.0 | OpenAI (NOT Anthropic) |
| `presence_penalty` | float | -2.0 - 2.0 | OpenAI (NOT Anthropic) |
| `repetition_penalty` | float | 0.0 - 2.0 | Meta, vLLM (NOT OpenAI/Anthropic) |
| `min_p` | float | 0.0 - 1.0 | vLLM (NOT others) |
| `stop` | list[str] | - | OpenAI, Anthropic |
| `seed` | int | - | OpenAI |
| `response_format` | dict | - | OpenAI |
| `logprobs` | bool | - | OpenAI |
| `n` | int | 1+ | OpenAI (number of completions) |

Source: [Vendor-recommended LLM parameter quick reference](https://muxup.com/2025q2/recommended-llm-parameter-quick-reference)

This diversity confirms that a flexible dict is the correct storage approach.

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Fixed param enums | Flexible dicts | Industry standard | No migration needed when new providers add params |
| JSONB for fast queries | SQLite json_extract() | SQLite 3.9+ (2015) | Full JSON querying in SQLite, no extensions needed |
| Text JSON storage | JSONB binary format | SQLite 3.45.0 (2024-01) | Faster parsing for JSONB, but standard JSON is fine for our scale |

**Deprecated/outdated:**
- None relevant. JSON column support in SQLAlchemy + SQLite has been stable for years.

## Open Questions

1. **Should generation_config be part of `_row_to_info()` mapping?**
   - What we know: `metadata` is mapped from `metadata_json` in `_row_to_info()`. Same pattern applies.
   - What's unclear: Nothing -- this is straightforward.
   - Recommendation: Yes, add it to `_row_to_info()`.

2. **Should `record_usage()` update the generation_config on the HEAD commit, or just associate it with the usage record?**
   - What we know: SC4 says "record_usage() can optionally accept generation_config, linking post-call actuals with the params that produced them."
   - What's unclear: "Linking" could mean updating the commit's generation_config or storing it alongside the usage record.
   - Recommendation: If the HEAD commit already has a generation_config (set at commit time), `record_usage()` with generation_config should update/overwrite it on that commit. If no generation_config was set at commit time, it sets one. This gives the user two entry points for the same data: commit-time (pre-call) and record_usage-time (post-call). The record_usage path is useful when the user doesn't know generation_config until after the API call returns.

3. **What is the right query API surface?**
   - What we know: SC5 requires filtering commits by config values (e.g., "all commits with temperature > 0.8").
   - What's unclear: Whether this should be a generic query method (`query_by_config(field, op, value)`) or specific methods (`get_commits_by_model()`, `get_commits_by_temperature_range()`).
   - Recommendation: Generic `query_by_config(field, operator, value)` method. Specific convenience methods can be added later. The generic approach covers all providers without knowing their parameter names upfront.

4. **Should `generation_configs` in CompiledContext be parallel to effective commits or to aggregated messages?**
   - What we know: Messages get aggregated (consecutive same-role concatenated). Effective commits are pre-aggregation.
   - What's unclear: Which level users will want configs at.
   - Recommendation: Parallel to effective commits (pre-aggregation). This preserves the 1:1 commit-to-config mapping. Users who need per-message mapping can correlate via the pre-aggregation commit list. This also avoids the question of how to "merge" configs when messages are aggregated.

## Scope and Effort Estimate

### Touched Files

| File | Change Type | Effort |
|------|-------------|--------|
| `src/tract/models/commit.py` | Add field | Trivial |
| `src/tract/storage/schema.py` | Add column | Trivial |
| `src/tract/storage/repositories.py` | Add query method | Small |
| `src/tract/storage/sqlite.py` | Implement query method | Small |
| `src/tract/engine/commit.py` | Thread param through | Small |
| `src/tract/engine/compiler.py` | Collect configs during compile | Medium |
| `src/tract/protocols.py` | Add field to CompiledContext + CompileSnapshot | Small |
| `src/tract/tract.py` | Wire commit/compile/record_usage/query | Medium |
| `src/tract/__init__.py` | Export any new types | Trivial |
| `tests/test_tract.py` | Integration tests for all 5 SCs | Medium |
| `tests/test_engine/test_commit.py` | Unit tests for engine | Small |
| `tests/test_engine/test_compiler.py` | Unit tests for compiler | Small |
| `tests/test_storage/test_repositories.py` | Unit tests for query method | Small |

### Estimated Plan Count: 1

This is a single-concern extension (one new field threaded through existing layers) with no new external dependencies. One plan covering model, storage, engine, compiler, facade, and tests is appropriate, similar in scope to Phase 1.1 plans.

## Sources

### Primary (HIGH confidence)
- **Codebase analysis** -- Direct reading of all source files: `models/commit.py`, `storage/schema.py`, `engine/commit.py`, `engine/compiler.py`, `tract.py`, `protocols.py`. The `metadata_json` pattern is the direct precedent.
- **[SQLite JSON Functions And Operators](https://www.sqlite.org/json1.html)** -- Official SQLite documentation for `json_extract()`, `->`, `->>`, `json_each()`. Confirmed json_extract returns native SQL types for scalar values.
- **SQLAlchemy 2.0 JSON column** -- Already in use in the codebase (`metadata_json` on `CommitRow`). `func.json_extract()` is the standard SQLAlchemy approach.

### Secondary (MEDIUM confidence)
- **[Vendor-recommended LLM parameter quick reference](https://muxup.com/2025q2/recommended-llm-parameter-quick-reference)** -- Cross-provider comparison of generation parameters. Confirms parameter diversity justifying flexible dict.
- **[Prompt Engineering Guide - LLM Settings](https://www.promptingguide.ai/introduction/settings)** -- Standard parameter descriptions and ranges.

### Tertiary (LOW confidence)
- None. All findings verified against authoritative sources.

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- no new libraries, follows existing codebase pattern exactly
- Architecture: HIGH -- direct extension of metadata_json pattern, all touched files identified and analyzed
- Pitfalls: HIGH -- pitfalls derived from actual codebase analysis (commit hash, compile cache, type coercion)
- JSON querying: HIGH -- verified against official SQLite documentation

**Research date:** 2026-02-11
**Valid until:** Indefinite (no external dependencies to age; internal architecture is stable)
