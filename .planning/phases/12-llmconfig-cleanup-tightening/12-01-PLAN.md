---
phase: 12-llmconfig-cleanup-tightening
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/tract/models/config.py
  - src/tract/__init__.py
  - src/tract/tract.py
  - tests/test_operation_config.py
autonomous: true

must_haves:
  truths:
    - "OperationConfigs is a frozen dataclass with chat/merge/compress/orchestrate fields -- typos caught at construction time"
    - "LLMConfig.from_dict() handles cross-framework aliases (stop -> stop_sequences, max_completion_tokens -> max_tokens) and ignores API plumbing keys"
    - "LLMConfig.from_obj() extracts config from arbitrary objects (dataclass, Pydantic model, plain object)"
    - "_default_model eliminated -- replaced by _default_config: LLMConfig | None"
    - "Tract.open(model=...) creates LLMConfig internally; Tract.open(default_config=...) accepted; both raises ValueError"
    - "configure_operations() accepts both OperationConfigs object and **kwargs for backward compat"
    - "operation_configs property returns OperationConfigs instance (not dict)"
  artifacts:
    - path: "src/tract/models/config.py"
      provides: "OperationConfigs frozen dataclass, _ALIASES, _IGNORED, updated from_dict, from_obj"
      contains: "class OperationConfigs"
    - path: "src/tract/__init__.py"
      provides: "OperationConfigs export"
      contains: "OperationConfigs"
    - path: "src/tract/tract.py"
      provides: "_default_config, OperationConfigs usage, updated open/configure_operations/property"
      contains: "_default_config"
  key_links:
    - from: "src/tract/tract.py"
      to: "src/tract/models/config.py"
      via: "import OperationConfigs"
      pattern: "from tract\\.models\\.config import.*OperationConfigs"
    - from: "src/tract/tract.py"
      to: "OperationConfigs"
      via: "getattr access in _resolve_llm_config"
      pattern: "getattr\\(self\\._operation_configs"
---

<objective>
Create the foundation config layer for Phase 12: OperationConfigs typed dataclass, smart from_dict() with aliases, from_obj() classmethod, consolidated _default_config, and updated Tract init/open/configure_operations/property.

Purpose: Replaces untyped dict[str, LLMConfig] with a frozen dataclass that catches typos at construction time and enables IDE autocomplete. Consolidates _default_model into _default_config: LLMConfig for full-field tract defaults. Adds alias/ignore handling to from_dict() for cross-framework compatibility.

Output: Updated models/config.py, __init__.py, tract.py (init/open/configure/property sections), and tests.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-llmconfig-cleanup-tightening/12-RESEARCH.md
@src/tract/models/config.py
@src/tract/__init__.py
@src/tract/tract.py
@tests/test_operation_config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: OperationConfigs dataclass + from_dict aliases + from_obj</name>
  <files>src/tract/models/config.py, src/tract/__init__.py</files>
  <action>
In `src/tract/models/config.py`, make these changes:

1. **Add module-level alias and ignore constants** (before the LLMConfig class):

```python
_ALIASES: dict[str, str] = {
    "stop": "stop_sequences",
    "max_completion_tokens": "max_tokens",
}

_IGNORED: frozenset[str] = frozenset({
    "messages", "tools", "tool_choice", "stream",
    "response_format", "n", "logprobs", "top_logprobs",
    "functions", "function_call",
    "system", "metadata",
})
```

2. **Update LLMConfig.from_dict()** to apply aliases and drop ignored keys BEFORE the existing known/extra routing:
   - Copy input dict to avoid mutation: `d = dict(d)`
   - Apply aliases: for each (alias, canonical) in _ALIASES, if alias is in d and canonical is NOT in d, move alias to canonical. If BOTH exist, drop alias (canonical wins).
   - Drop ignored keys: `for key in _IGNORED: d.pop(key, None)`
   - Then proceed with existing known/extra split logic unchanged.

3. **Add from_obj() classmethod** to LLMConfig:
```python
@classmethod
def from_obj(cls, obj: object) -> LLMConfig | None:
    """Extract LLMConfig from an arbitrary object.

    Handles dataclasses (via fields), Pydantic models (via model_dump),
    and plain objects (via vars). Pipes through from_dict() for alias
    handling and field routing.

    Returns None if obj is None.
    """
    if obj is None:
        return None
    import dataclasses as _dc
    if _dc.is_dataclass(obj) and not isinstance(obj, type):
        d = {f.name: getattr(obj, f.name) for f in _dc.fields(obj)}
    elif hasattr(obj, "model_dump"):
        d = obj.model_dump()
    else:
        d = vars(obj)
    return cls.from_dict(d)
```

4. **Add OperationConfigs frozen dataclass** AFTER the LLMConfig class:
```python
@dataclass(frozen=True)
class OperationConfigs:
    """Per-operation LLM configuration defaults.

    Each field corresponds to an LLM-powered operation on Tract.
    None means 'no operation-level override -- use tract default.'
    Frozen for safety -- use dataclasses.replace() to create modified copies.
    """
    chat: LLMConfig | None = None
    merge: LLMConfig | None = None
    compress: LLMConfig | None = None
    orchestrate: LLMConfig | None = None
```

5. **In `src/tract/__init__.py`:**
   - Add `OperationConfigs` to the import from `tract.models.config` (line 32)
   - Add `"OperationConfigs"` to the `__all__` list in the Config section

  </action>
  <verify>
Run: `python -c "from tract import OperationConfigs, LLMConfig; oc = OperationConfigs(chat=LLMConfig(model='gpt-4o')); print(oc.chat.model)"`
Expected: prints "gpt-4o"

Run: `python -c "from tract import LLMConfig; c = LLMConfig.from_dict({'stop': ['a','b'], 'messages': [1,2], 'model': 'gpt-4o'}); print(c.stop_sequences, c.model, c.extra)"`
Expected: prints "('a', 'b') gpt-4o None" (stop aliased, messages ignored, no extra)

Run: `python -c "from tract import LLMConfig; print(LLMConfig.from_dict({'max_completion_tokens': 100}).max_tokens)"`
Expected: prints "100"
  </verify>
  <done>
OperationConfigs frozen dataclass exists with 4 typed fields. LLMConfig.from_dict() handles _ALIASES and _IGNORED. LLMConfig.from_obj() extracts from dataclasses, Pydantic models, and plain objects. OperationConfigs exported from tract package.
  </done>
</task>

<task type="auto">
  <name>Task 2: Consolidated _default_config and updated Tract init/open/configure/property</name>
  <files>src/tract/tract.py</files>
  <action>
In `src/tract/tract.py`, make these changes:

1. **Update import** on line 26: add `OperationConfigs` to the import from `tract.models.config`:
   `from tract.models.config import LLMConfig, OperationConfigs, TractConfig`

2. **In `__init__()` (line ~163-164):**
   - Replace `self._default_model: str | None = None` with `self._default_config: LLMConfig | None = None`
   - Replace `self._operation_configs: dict[str, LLMConfig] = {}` with `self._operation_configs: OperationConfigs = OperationConfigs()`

3. **In `open()` classmethod (line ~167):**
   - Add parameter `default_config: LLMConfig | None = None` after `base_url`
   - Add parameter `operations: OperationConfigs | None = None` after `default_config`
   - Keep existing `operation_configs: dict[str, LLMConfig] | None = None` for backward compat but mark with comment `# deprecated: use operations=`
   - Add validation after tract creation (~line 309-325):
     ```python
     # Validate: model= and default_config= are mutually exclusive
     if model is not None and default_config is not None:
         raise ValueError(
             "Cannot specify both model= and default_config=. "
             "Use default_config=LLMConfig(model=...) for full control."
         )
     ```
   - Update the LLM auto-config block (~line 310-320):
     Replace `tract._default_model = model` with:
     ```python
     if model is not None:
         tract._default_config = LLMConfig(model=model)
     ```
   - After the api_key block, add default_config handling:
     ```python
     # Apply default_config if provided (without api_key)
     if default_config is not None and tract._default_config is None:
         tract._default_config = default_config
     ```
   - Update operation_configs application (~line 322-324):
     ```python
     # Apply per-operation configs (new typed path)
     if operations is not None:
         tract._operation_configs = operations
     # Apply per-operation configs (legacy dict path)
     elif operation_configs is not None:
         tract.configure_operations(**operation_configs)
     ```

4. **Update `configure_operations()` (line ~1546):**
   Replace the current implementation with dual-path support:
   ```python
   def configure_operations(
       self,
       _configs: OperationConfigs | None = None,
       /,
       **operation_configs: LLMConfig,
   ) -> None:
       """Set per-operation LLM defaults.

       Accepts either an OperationConfigs instance (new style) or keyword
       arguments (backward compatible).

       Args:
           _configs: OperationConfigs instance with typed fields.
           **operation_configs: Operation name -> LLMConfig mappings.
               Valid names: ``"chat"``, ``"merge"``, ``"compress"``,
               ``"orchestrate"``.

       Raises:
           TypeError: If both positional and keyword arguments provided,
               or if a keyword value is not an LLMConfig.

       Example::

           from tract import LLMConfig, OperationConfigs
           # New style:
           t.configure_operations(OperationConfigs(
               chat=LLMConfig(model="gpt-4o"),
               compress=LLMConfig(model="gpt-3.5-turbo"),
           ))
           # Backward compatible:
           t.configure_operations(
               chat=LLMConfig(model="gpt-4o"),
               compress=LLMConfig(model="gpt-3.5-turbo"),
           )
       """
       if _configs is not None and operation_configs:
           raise TypeError(
               "Cannot mix positional OperationConfigs with keyword arguments"
           )
       if _configs is not None:
           if not isinstance(_configs, OperationConfigs):
               raise TypeError(
                   f"Expected OperationConfigs, got {type(_configs).__name__}"
               )
           self._operation_configs = _configs
           return
       # Keyword path: validate and construct OperationConfigs
       _valid_ops = {"chat", "merge", "compress", "orchestrate"}
       for name, config in operation_configs.items():
           if not isinstance(config, LLMConfig):
               raise TypeError(
                   f"Expected LLMConfig for '{name}', "
                   f"got {type(config).__name__}"
               )
           if name not in _valid_ops:
               raise ValueError(
                   f"Unknown operation '{name}'. "
                   f"Valid operations: {', '.join(sorted(_valid_ops))}"
               )
       # Merge with existing: only replace fields that are provided
       from dataclasses import replace as _replace
       updates = {}
       for name, config in operation_configs.items():
           updates[name] = config
       self._operation_configs = _replace(self._operation_configs, **updates)
   ```

5. **Update `operation_configs` property (line ~1580):**
   ```python
   @property
   def operation_configs(self) -> OperationConfigs:
       """Current per-operation LLM configurations (read-only, frozen)."""
       return self._operation_configs
   ```

6. **Update `_resolve_llm_config()` (line ~657):**
   Change `self._operation_configs.get(operation)` to `getattr(self._operation_configs, operation, None)`.
   Change `self._default_model` references to use `self._default_config`:
   - Replace line 666-667:
     ```python
     elif self._default_config is not None and self._default_config.model is not None:
         resolved["model"] = self._default_config.model
     ```
   NOTE: Only change the `_default_model` -> `_default_config` and dict.get -> getattr in this task. The full 4-level rewrite of _resolve_llm_config happens in Plan 02.

7. **Update `_build_generation_config()` (line ~717-718):**
   Change `self._default_model` to `self._default_config.model if self._default_config else None`:
   ```python
   elif self._default_config is not None and self._default_config.model is not None:
       config["model"] = self._default_config.model
   ```
   NOTE: Full rewrite of _build_generation_config happens in Plan 02.

  </action>
  <verify>
Run: `python -c "from tract import Tract, LLMConfig, OperationConfigs; t = Tract.open(); t.configure_operations(OperationConfigs(chat=LLMConfig(model='test'))); print(t.operation_configs.chat.model)"`
Expected: prints "test"

Run: `python -c "from tract import Tract, LLMConfig; t = Tract.open(); t.configure_operations(chat=LLMConfig(model='test')); print(t.operation_configs.chat.model)"`
Expected: prints "test" (backward compat)

Run: `python -c "from tract import Tract, LLMConfig; t = Tract.open(model='x', default_config=LLMConfig(model='y'))"`
Expected: raises ValueError about mutually exclusive

Run: `python -m pytest tests/test_operation_config.py -x -q --tb=short 2>&1 | tail -10`
Expected: some existing tests may fail (dict-indexing tests) -- that is expected and fixed in Task 3.
  </verify>
  <done>
_default_model eliminated, replaced by _default_config: LLMConfig | None. _operation_configs uses OperationConfigs dataclass. open() accepts default_config= and operations=. configure_operations() accepts both OperationConfigs and **kwargs. operation_configs property returns OperationConfigs.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update tests for new config layer</name>
  <files>tests/test_operation_config.py</files>
  <action>
Update `tests/test_operation_config.py` to work with the new OperationConfigs dataclass and add tests for all new functionality. Changes:

1. **Update imports** at top: add `OperationConfigs` to the tract import line.

2. **Update TestConfigureOperations:**
   - `test_configure_single_operation`: Change `configs["chat"]` to `configs.chat` (attribute access instead of dict indexing).
   - `test_configure_multiple_operations`: Change `configs["chat"]`, `configs["compress"]`, `configs["merge"]` to attribute access. Change `len(configs) == 3` assertion -- instead check that all three fields are not None.
   - `test_configure_overwrites_existing`: Change `t.operation_configs["chat"]` to `t.operation_configs.chat`.
   - `test_configure_type_error`: Keep as-is (kwargs path still validates).

3. **Update TestOpenWithOperationConfigs:**
   - `test_open_with_operation_configs`: Change dict indexing to attribute access. Also add a test with the new `operations=OperationConfigs(...)` parameter.
   - `test_open_without_operation_configs`: Change `== {}` to check `isinstance(t.operation_configs, OperationConfigs)` and verify all fields are None.

4. **Update TestResolveLLMConfig:**
   - `test_resolve_tract_default_used`: Change `t._default_model = "tract-default"` to `t._default_config = LLMConfig(model="tract-default")`.
   - `test_resolve_call_level_wins`: Same change for `_default_model`.
   - `test_resolve_operation_level_wins_over_tract`: Same change for `_default_model`.

5. **Add new test class TestOperationConfigsDataclass:**
```python
class TestOperationConfigsDataclass:
    """Tests for the OperationConfigs frozen dataclass."""

    def test_create_with_defaults(self):
        """All fields default to None."""
        oc = OperationConfigs()
        assert oc.chat is None
        assert oc.merge is None
        assert oc.compress is None
        assert oc.orchestrate is None

    def test_create_with_values(self):
        """Fields can be set at construction."""
        oc = OperationConfigs(
            chat=LLMConfig(model="chat-model"),
            compress=LLMConfig(model="compress-model"),
        )
        assert oc.chat.model == "chat-model"
        assert oc.compress.model == "compress-model"
        assert oc.merge is None
        assert oc.orchestrate is None

    def test_frozen(self):
        """Attempting to modify raises FrozenInstanceError."""
        oc = OperationConfigs()
        with pytest.raises(dataclasses.FrozenInstanceError):
            oc.chat = LLMConfig(model="test")

    def test_typo_caught_at_construction(self):
        """Misspelled field name raises TypeError at construction."""
        with pytest.raises(TypeError):
            OperationConfigs(chatt=LLMConfig(model="test"))

    def test_configure_operations_typed(self):
        """configure_operations accepts OperationConfigs instance."""
        t = Tract.open()
        oc = OperationConfigs(chat=LLMConfig(model="gpt-4o"))
        t.configure_operations(oc)
        assert t.operation_configs.chat.model == "gpt-4o"
        t.close()

    def test_configure_operations_mixed_raises(self):
        """Passing both OperationConfigs and kwargs raises TypeError."""
        t = Tract.open()
        oc = OperationConfigs(chat=LLMConfig(model="gpt-4o"))
        with pytest.raises(TypeError, match="Cannot mix"):
            t.configure_operations(oc, merge=LLMConfig(model="gpt-4o"))
        t.close()

    def test_open_with_operations_param(self):
        """Tract.open(operations=OperationConfigs(...)) applies config."""
        oc = OperationConfigs(chat=LLMConfig(model="gpt-4o"))
        t = Tract.open(operations=oc)
        assert t.operation_configs.chat.model == "gpt-4o"
        t.close()
```

6. **Add new test class TestFromDictAliases:**
```python
class TestFromDictAliases:
    """Tests for LLMConfig.from_dict() alias and ignore handling."""

    def test_stop_alias(self):
        """'stop' is aliased to stop_sequences."""
        config = LLMConfig.from_dict({"stop": ["a", "b"]})
        assert config.stop_sequences == ("a", "b")
        assert config.extra is None

    def test_max_completion_tokens_alias(self):
        """'max_completion_tokens' is aliased to max_tokens."""
        config = LLMConfig.from_dict({"max_completion_tokens": 500})
        assert config.max_tokens == 500
        assert config.extra is None

    def test_canonical_wins_over_alias(self):
        """If both alias and canonical present, canonical wins."""
        config = LLMConfig.from_dict({
            "stop": ["alias"],
            "stop_sequences": ["canonical"],
        })
        assert config.stop_sequences == ("canonical",)

    def test_ignored_keys_dropped(self):
        """API plumbing keys are silently dropped."""
        config = LLMConfig.from_dict({
            "model": "gpt-4o",
            "messages": [{"role": "user", "content": "hi"}],
            "tools": [{"type": "function"}],
            "stream": True,
            "response_format": {"type": "json"},
        })
        assert config.model == "gpt-4o"
        assert config.extra is None  # all non-model keys were ignored

    def test_input_not_mutated(self):
        """from_dict does not mutate the input dict."""
        d = {"stop": ["a"], "messages": [1]}
        original = dict(d)
        LLMConfig.from_dict(d)
        assert d == original

    def test_alias_and_ignore_combined(self):
        """Aliases applied and ignored keys dropped in same call."""
        config = LLMConfig.from_dict({
            "model": "gpt-4o",
            "stop": ["end"],
            "max_completion_tokens": 100,
            "messages": [],
            "tools": [],
        })
        assert config.model == "gpt-4o"
        assert config.stop_sequences == ("end",)
        assert config.max_tokens == 100
        assert config.extra is None
```

7. **Add new test class TestFromObj:**
```python
class TestFromObj:
    """Tests for LLMConfig.from_obj()."""

    def test_from_dataclass(self):
        """Extract LLMConfig from a dataclass instance."""
        source = LLMConfig(model="gpt-4o", temperature=0.7)
        result = LLMConfig.from_obj(source)
        assert result.model == "gpt-4o"
        assert result.temperature == 0.7

    def test_from_plain_object(self):
        """Extract from a plain object with __dict__."""
        class FakeConfig:
            def __init__(self):
                self.model = "gpt-4o"
                self.temperature = 0.5
                self.unknown_field = "extra"
        result = LLMConfig.from_obj(FakeConfig())
        assert result.model == "gpt-4o"
        assert result.temperature == 0.5
        assert result.extra is not None
        assert result.extra["unknown_field"] == "extra"

    def test_from_none(self):
        """from_obj(None) returns None."""
        assert LLMConfig.from_obj(None) is None
```

8. **Add new test class TestDefaultConfig:**
```python
class TestDefaultConfig:
    """Tests for consolidated _default_config."""

    def test_open_model_creates_default_config(self):
        """Tract.open(api_key=..., model=...) creates _default_config internally."""
        # We can't test with real api_key, but we can set _default_config manually
        t = Tract.open()
        t._default_config = LLMConfig(model="default-model", temperature=0.5)
        resolved = t._resolve_llm_config("chat")
        assert resolved["model"] == "default-model"
        t.close()

    def test_default_config_all_fields_available(self):
        """All LLMConfig fields from _default_config are accessible (for future Plan 02)."""
        t = Tract.open()
        t._default_config = LLMConfig(model="default", temperature=0.3)
        # Currently only model is resolved from default -- temperature requires Plan 02
        resolved = t._resolve_llm_config("chat")
        assert resolved["model"] == "default"
        t.close()

    def test_open_model_and_default_config_raises(self):
        """Providing both model= and default_config= raises ValueError."""
        with pytest.raises(ValueError, match="Cannot specify both"):
            Tract.open(
                api_key="fake-key",
                model="gpt-4o",
                default_config=LLMConfig(model="gpt-4o"),
            )
```
  </action>
  <verify>
Run: `python -m pytest tests/test_operation_config.py -x -q --tb=short`
Expected: All tests pass (existing updated + new tests).

Run: `python -m pytest tests/ -x -q --tb=short 2>&1 | tail -5`
Expected: All 1011+ tests pass (no regressions).
  </verify>
  <done>
All existing tests updated for OperationConfigs attribute access. New tests cover: OperationConfigs dataclass (defaults, values, frozen, typo detection), configure_operations dual-path, from_dict aliases (stop, max_completion_tokens, canonical-wins, ignored keys, input-not-mutated), from_obj (dataclass, plain object, None), and default_config (creation, resolution, mutual exclusion). Full test suite passes with no regressions.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_operation_config.py -v --tb=short` -- all tests pass
2. `python -m pytest tests/ -q --tb=short` -- full suite passes (1011+ tests, no regressions)
3. `python -c "from tract import OperationConfigs"` -- export works
4. `python -c "from tract import LLMConfig; c = LLMConfig.from_dict({'stop': ['x'], 'messages': []}); print(c.stop_sequences, c.extra)"` -- aliases and ignores work
</verification>

<success_criteria>
1. OperationConfigs is a frozen dataclass with chat/merge/compress/orchestrate fields
2. Typos like OperationConfigs(chatt=...) raise TypeError at construction
3. _default_model completely eliminated from tract.py, replaced by _default_config: LLMConfig | None
4. Tract.open() accepts default_config= parameter; model= and default_config= are mutually exclusive
5. configure_operations() supports both OperationConfigs and **kwargs paths
6. operation_configs property returns OperationConfigs (not dict)
7. from_dict() handles aliases and drops ignored keys
8. from_obj() extracts from dataclasses, Pydantic models, plain objects
9. All existing tests pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/12-llmconfig-cleanup-tightening/12-01-SUMMARY.md`
</output>
