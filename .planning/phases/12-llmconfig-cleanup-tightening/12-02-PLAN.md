---
phase: 12-llmconfig-cleanup-tightening
plan: 02
type: execute
wave: 2
depends_on: ["12-01"]
files_modified:
  - src/tract/tract.py
  - src/tract/operations/compression.py
  - src/tract/models/compression.py
  - src/tract/orchestrator/config.py
  - src/tract/orchestrator/loop.py
  - tests/test_operation_config.py
autonomous: true

must_haves:
  truths:
    - "chat()/generate()/merge()/compress() accept llm_config: LLMConfig | None for full call-level override"
    - "_resolve_llm_config implements 4-level chain: sugar > llm_config > operation > tract default for ALL 9 fields + extra"
    - "_build_generation_config captures ALL resolved fields (top_p, seed, frequency_penalty, etc.), not just model/temperature/max_tokens"
    - "Compression summary commits record generation_config from the LLM call"
    - "Orchestrator _call_llm() forwards max_tokens and extra_llm_kwargs from OrchestratorConfig"
    - "compress() raises when explicit LLM params (model=, llm_config=) provided without LLM client; compress(content=...) bypasses this guard"
    - "All existing tests continue to pass with new tests covering every fix"
  artifacts:
    - path: "src/tract/tract.py"
      provides: "4-level _resolve_llm_config, full _build_generation_config, llm_config= on all ops"
      contains: "llm_config: LLMConfig | None"
    - path: "src/tract/operations/compression.py"
      provides: "generation_config threading through compress_range -> _commit_compression"
      contains: "generation_config="
    - path: "src/tract/orchestrator/config.py"
      provides: "max_tokens and extra_llm_kwargs fields on OrchestratorConfig"
      contains: "extra_llm_kwargs"
    - path: "src/tract/orchestrator/loop.py"
      provides: "_call_llm forwarding full config"
      contains: "max_tokens"
  key_links:
    - from: "src/tract/tract.py:_resolve_llm_config"
      to: "src/tract/tract.py:generate"
      via: "resolved dict passed to _build_generation_config"
      pattern: "_build_generation_config.*resolved"
    - from: "src/tract/tract.py:compress"
      to: "src/tract/operations/compression.py:compress_range"
      via: "generation_config parameter"
      pattern: "generation_config="
    - from: "src/tract/tract.py:orchestrate"
      to: "src/tract/orchestrator/config.py"
      via: "max_tokens and extra_llm_kwargs on OrchestratorConfig"
      pattern: "max_tokens.*extra_llm_kwargs"
---

<objective>
Wire the 4-level config resolution chain through all LLM-powered operations, capture full generation_config, fix compression/orchestrator config forwarding, and add error guard for config-without-client.

Purpose: Completes the LLMConfig tightening by making the resolution chain (sugar > llm_config > operation > tract default) work across all 9 typed fields for every operation. Fixes the 3 downstream wiring issues (compression gen_config, orchestrator forwarding, compress error guard).

Output: Updated tract.py (resolve/build/chat/generate/merge/compress/orchestrate), operations/compression.py, orchestrator/config.py, orchestrator/loop.py, and comprehensive tests.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-llmconfig-cleanup-tightening/12-RESEARCH.md
@.planning/phases/12-llmconfig-cleanup-tightening/12-01-SUMMARY.md
@src/tract/tract.py
@src/tract/operations/compression.py
@src/tract/orchestrator/config.py
@src/tract/orchestrator/loop.py
@tests/test_operation_config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite _resolve_llm_config and _build_generation_config + add llm_config= to operations</name>
  <files>src/tract/tract.py</files>
  <action>
In `src/tract/tract.py`, rewrite the resolution chain and update all operation signatures:

**1. Rewrite `_resolve_llm_config()` (line ~633):**

Replace the entire method with:

```python
def _resolve_llm_config(
    self,
    operation: str,
    *,
    model: str | None = None,
    temperature: float | None = None,
    max_tokens: int | None = None,
    llm_config: LLMConfig | None = None,
    **kwargs: object,
) -> dict:
    """Resolve effective LLM config: sugar > llm_config > operation > tract default.

    Four-level resolution chain for each field:
    1. Sugar params (model=, temperature=, max_tokens=) -- highest priority
    2. llm_config fields (if provided and field is not None)
    3. Operation-level config (from configure_operations)
    4. Tract-level default config (_default_config)

    Returns a dict of kwargs to pass to llm_client.chat(). Only includes
    keys that have a non-None value at some level in the chain.

    Args:
        operation: Operation name ("chat", "merge", "compress", "orchestrate").
        model: Call-level model override (sugar).
        temperature: Call-level temperature override (sugar).
        max_tokens: Call-level max_tokens override (sugar).
        llm_config: Full LLMConfig override for this call.
        **kwargs: Additional call-level kwargs (highest priority).
    """
    op_config = getattr(self._operation_configs, operation, None)
    default = self._default_config

    # Sugar params dict (only the 3 convenience overrides)
    sugar = {}
    if model is not None:
        sugar["model"] = model
    if temperature is not None:
        sugar["temperature"] = temperature
    if max_tokens is not None:
        sugar["max_tokens"] = max_tokens

    resolved: dict = {}

    # Resolve each typed field through 4-level chain
    _TYPED_FIELDS = (
        "model", "temperature", "max_tokens", "top_p",
        "frequency_penalty", "presence_penalty", "top_k",
        "seed", "stop_sequences",
    )
    for field_name in _TYPED_FIELDS:
        # Level 1: Sugar param
        val = sugar.get(field_name)
        if val is not None:
            resolved[field_name] = val
            continue
        # Level 2: llm_config
        if llm_config is not None:
            val = getattr(llm_config, field_name, None)
            if val is not None:
                resolved[field_name] = val
                continue
        # Level 3: Operation config
        if op_config is not None:
            val = getattr(op_config, field_name, None)
            if val is not None:
                resolved[field_name] = val
                continue
        # Level 4: Tract default
        if default is not None:
            val = getattr(default, field_name, None)
            if val is not None:
                resolved[field_name] = val

    # Convert tuples to lists for LLM client compatibility
    if "stop_sequences" in resolved and isinstance(resolved["stop_sequences"], tuple):
        resolved["stop_sequences"] = list(resolved["stop_sequences"])

    # Merge extra kwargs: tract default < operation < llm_config < call kwargs
    # (each level's extra overrides the previous)
    if default is not None and default.extra:
        resolved.update(dict(default.extra))
    if op_config is not None and op_config.extra:
        resolved.update(dict(op_config.extra))
    if llm_config is not None and llm_config.extra:
        resolved.update(dict(llm_config.extra))
    resolved.update(kwargs)

    return resolved
```

**2. Rewrite `_build_generation_config()` (line ~698):**

Replace the entire method with:

```python
def _build_generation_config(self, response: dict, *, resolved: dict) -> dict:
    """Build generation_config from the full resolved LLM kwargs.

    Captures ALL fields that were sent to the LLM (model, temperature,
    top_p, seed, etc.) so they can be queried via query_by_config().

    The response's model field is authoritative (actual model used may
    differ from requested model due to aliases/routing).

    Args:
        response: Raw LLM response dict.
        resolved: The full resolved kwargs dict from _resolve_llm_config().
    """
    config = dict(resolved)
    # Response model is authoritative
    if "model" in response:
        config["model"] = response["model"]
    return config
```

**3. Update `generate()` (line ~726):**

- Add `llm_config: LLMConfig | None = None` parameter after max_tokens.
- Update the _resolve_llm_config call to pass llm_config:
  ```python
  llm_kwargs = self._resolve_llm_config(
      "chat", model=model, temperature=temperature,
      max_tokens=max_tokens, llm_config=llm_config,
  )
  ```
- Update the _build_generation_config call to use the new signature:
  ```python
  gen_config = self._build_generation_config(response, resolved=llm_kwargs)
  ```

**4. Update `chat()` (line ~808):**

- Add `llm_config: LLMConfig | None = None` parameter after max_tokens.
- Pass it through to generate():
  ```python
  return self.generate(
      model=model,
      temperature=temperature,
      max_tokens=max_tokens,
      llm_config=llm_config,
  )
  ```

**5. Update `merge()` (line ~1585):**

- Add `llm_config: LLMConfig | None = None` parameter after max_tokens.
- Update the _resolve_llm_config call:
  ```python
  merge_config = self._resolve_llm_config(
      "merge", model=model, temperature=temperature,
      max_tokens=max_tokens, llm_config=llm_config,
  )
  ```

**6. Update `compress()` (line ~1871):**

- Add `llm_config: LLMConfig | None = None` parameter after max_tokens.
- Add error guard for explicit LLM params without client (before the resolve call):
  ```python
  llm_client = getattr(self, "_llm_client", None)

  # Guard: explicit LLM config without LLM client
  has_explicit_llm = (
      model is not None
      or temperature is not None
      or max_tokens is not None
      or llm_config is not None
  )
  if has_explicit_llm and llm_client is None and content is None:
      from tract.llm.errors import LLMConfigError
      raise LLMConfigError(
          "LLM parameters provided (model, temperature, max_tokens, or "
          "llm_config) but no LLM client is configured. Call "
          "configure_llm() or pass api_key to Tract.open(), or provide "
          "content= for manual compression."
      )
  ```
- Update the _resolve_llm_config call:
  ```python
  llm_kwargs = self._resolve_llm_config(
      "compress", model=model, temperature=temperature,
      max_tokens=max_tokens, llm_config=llm_config,
  ) if llm_client is not None else {}
  ```
- Pass generation_config to compress_range (add as new kwarg):
  ```python
  result = compress_range(
      ...existing args...,
      llm_kwargs=llm_kwargs,
      generation_config=llm_kwargs if llm_kwargs else None,
      ...remaining args...,
  )
  ```
  NOTE: The generation_config parameter will be added to compress_range in Task 2.

**7. Update `orchestrate()` (line ~2579):**

Update the orchestrate method to forward ALL resolved config fields to OrchestratorConfig, not just model and temperature:

Replace the current config merging block (lines ~2602-2623) with:

```python
# Step 1: Resolve per-operation config BEFORE the three-way branch
orch_resolved = self._resolve_llm_config("orchestrate")

# Step 2: If operation-level config found, apply to config (mutation-safe)
if orch_resolved:
    if config is not None:
        # Caller provided config -- only fill in None/default fields
        overrides: dict = {}
        if config.model is None and "model" in orch_resolved:
            overrides["model"] = orch_resolved["model"]
        if config.temperature == 0.0 and "temperature" in orch_resolved:
            overrides["temperature"] = orch_resolved["temperature"]
        if config.max_tokens is None and "max_tokens" in orch_resolved:
            overrides["max_tokens"] = orch_resolved["max_tokens"]
        # Collect remaining resolved fields into extra_llm_kwargs
        _orch_known = {"model", "temperature", "max_tokens"}
        extra = {k: v for k, v in orch_resolved.items() if k not in _orch_known}
        if extra and config.extra_llm_kwargs is None:
            overrides["extra_llm_kwargs"] = extra
        if overrides:
            config = replace(config, **overrides)
    else:
        # No caller config -- create one from operation defaults
        from tract.orchestrator.config import OrchestratorConfig as _OrchestratorConfig
        _orch_known = {"model", "temperature", "max_tokens"}
        extra = {k: v for k, v in orch_resolved.items() if k not in _orch_known}
        config = _OrchestratorConfig(
            model=orch_resolved.get("model"),
            temperature=orch_resolved.get("temperature", 0.0),
            max_tokens=orch_resolved.get("max_tokens"),
            extra_llm_kwargs=extra if extra else None,
        )
```

  </action>
  <verify>
Run: `python -c "from tract import Tract, LLMConfig; t = Tract.open(); print(type(t._resolve_llm_config))"`
Expected: method type (confirms method exists)

Run: `python -m pytest tests/test_operation_config.py -x -q --tb=short 2>&1 | tail -5`
Expected: existing tests pass (backward compat maintained)
  </verify>
  <done>
_resolve_llm_config implements 4-level chain (sugar > llm_config > operation > tract default) for all 9 typed fields + extra. _build_generation_config captures the full resolved dict. chat/generate/merge/compress accept llm_config= parameter. compress() raises LLMConfigError on explicit LLM params without client. orchestrate() forwards max_tokens and extra to OrchestratorConfig.
  </done>
</task>

<task type="auto">
  <name>Task 2: Compression generation_config threading + orchestrator config forwarding</name>
  <files>src/tract/operations/compression.py, src/tract/orchestrator/config.py, src/tract/orchestrator/loop.py</files>
  <action>

**1. Update `src/tract/orchestrator/config.py`:**

Add two new fields to OrchestratorConfig (after the existing `temperature` field, before `on_proposal`):

```python
max_tokens: int | None = None
extra_llm_kwargs: dict | None = None
```

Update the docstring to document these fields:
- `max_tokens`: Maximum tokens for LLM response in orchestrator calls.
- `extra_llm_kwargs`: Additional LLM kwargs (top_p, seed, etc.) forwarded to client.chat().

**2. Update `src/tract/orchestrator/loop.py` -- `_call_llm()` method (line ~267):**

Update the fallback branch (when using tract's built-in LLM client, line ~299-304) to forward all config:

Replace:
```python
return client.chat(
    messages,
    model=self._config.model,
    temperature=self._config.temperature,
    tools=tools,
)
```

With:
```python
kwargs: dict = {"tools": tools}
if self._config.model:
    kwargs["model"] = self._config.model
if self._config.temperature is not None:
    kwargs["temperature"] = self._config.temperature
if self._config.max_tokens is not None:
    kwargs["max_tokens"] = self._config.max_tokens
if self._config.extra_llm_kwargs:
    kwargs.update(self._config.extra_llm_kwargs)
return client.chat(messages, **kwargs)
```

**3. Update `src/tract/operations/compression.py`:**

**3a. Add `generation_config` parameter to `compress_range()` (line ~367):**

Add after the `llm_kwargs` parameter:
```python
generation_config: dict | None = None,
```

Thread it through to _commit_compression call (line ~514):
```python
return _commit_compression(
    ...existing args...,
    generation_config=generation_config,
)
```

Also add a formal field to PendingCompression in `src/tract/models/compression.py`:
```python
_generation_config: dict | None = field(default=None, repr=False)
```
Then set it in compress_range (line ~501-511):
```python
pending._generation_config = generation_config
```

**3b. Add `generation_config` parameter to `_commit_compression()` (line ~537):**

Add after `expected_head` parameter:
```python
generation_config: dict | None = None,
```

**3c. Update the summary commit creation site** in `_commit_compression()` (line ~665):

Change:
```python
info = commit_engine.create_commit(
    content=summary_content,
    message=f"Compressed {n_commits} commits",
)
```

To:
```python
info = commit_engine.create_commit(
    content=summary_content,
    message=f"Compressed {n_commits} commits",
    generation_config=generation_config,
)
```

**3d. Update `src/tract/tract.py` compress() method:**

In the `compress_range()` call, add the generation_config parameter:
```python
result = compress_range(
    ...existing args...,
    llm_kwargs=llm_kwargs,
    generation_config=llm_kwargs if llm_kwargs else None,
    ...remaining args...,
)
```

Also update `_finalize_compression()` method (search for it in tract.py) to thread generation_config from PendingCompression:
Find the `_finalize_compression` method and add `generation_config=getattr(pending, '_generation_config', None)` to the `_commit_compression` call.

  </action>
  <verify>
Run: `python -c "from tract.orchestrator.config import OrchestratorConfig; c = OrchestratorConfig(max_tokens=100, extra_llm_kwargs={'top_p': 0.9}); print(c.max_tokens, c.extra_llm_kwargs)"`
Expected: prints "100 {'top_p': 0.9}"

Run: `python -m pytest tests/test_operation_config.py -x -q --tb=short 2>&1 | tail -5`
Expected: all tests pass
  </verify>
  <done>
OrchestratorConfig has max_tokens and extra_llm_kwargs fields. _call_llm() forwards all config fields to client.chat(). compress_range() and _commit_compression() accept and thread generation_config to summary commit creation. Compression summary commits now record the LLM config that produced them.
  </done>
</task>

<task type="auto">
  <name>Task 3: Comprehensive tests for wiring, resolution chain, and downstream fixes</name>
  <files>tests/test_operation_config.py</files>
  <action>
Add comprehensive tests to `tests/test_operation_config.py` covering all Plan 02 changes:

**1. Add TestFourLevelResolution class:**
```python
class TestFourLevelResolution:
    """Tests for the 4-level _resolve_llm_config chain."""

    def test_sugar_beats_llm_config(self):
        """Sugar param (model=) overrides llm_config.model."""
        t = Tract.open()
        t._default_config = LLMConfig(model="default")
        t.configure_operations(chat=LLMConfig(model="op"))
        llm_cfg = LLMConfig(model="llm-config")

        resolved = t._resolve_llm_config(
            "chat", model="sugar", llm_config=llm_cfg,
        )
        assert resolved["model"] == "sugar"
        t.close()

    def test_llm_config_beats_operation(self):
        """llm_config.model overrides operation config."""
        t = Tract.open()
        t.configure_operations(chat=LLMConfig(model="op"))
        llm_cfg = LLMConfig(model="llm-config")

        resolved = t._resolve_llm_config("chat", llm_config=llm_cfg)
        assert resolved["model"] == "llm-config"
        t.close()

    def test_operation_beats_default(self):
        """Operation config overrides tract default."""
        t = Tract.open()
        t._default_config = LLMConfig(model="default")
        t.configure_operations(chat=LLMConfig(model="op"))

        resolved = t._resolve_llm_config("chat")
        assert resolved["model"] == "op"
        t.close()

    def test_default_used_as_fallback(self):
        """Tract default is used when no higher-level config is set."""
        t = Tract.open()
        t._default_config = LLMConfig(model="default", temperature=0.5)

        resolved = t._resolve_llm_config("chat")
        assert resolved["model"] == "default"
        assert resolved["temperature"] == 0.5
        t.close()

    def test_all_nine_fields_resolved(self):
        """All 9 typed fields go through the resolution chain."""
        t = Tract.open()
        t._default_config = LLMConfig(
            model="m", temperature=0.1, top_p=0.2, max_tokens=100,
            stop_sequences=("s",), frequency_penalty=0.3,
            presence_penalty=0.4, top_k=10, seed=42,
        )
        resolved = t._resolve_llm_config("chat")
        assert resolved["model"] == "m"
        assert resolved["temperature"] == 0.1
        assert resolved["top_p"] == 0.2
        assert resolved["max_tokens"] == 100
        assert resolved["stop_sequences"] == ["s"]  # tuple -> list for LLM compat
        assert resolved["frequency_penalty"] == 0.3
        assert resolved["presence_penalty"] == 0.4
        assert resolved["top_k"] == 10
        assert resolved["seed"] == 42
        t.close()

    def test_mixed_levels(self):
        """Different fields come from different levels."""
        t = Tract.open()
        t._default_config = LLMConfig(model="default-model", seed=42)
        t.configure_operations(chat=LLMConfig(temperature=0.5))
        llm_cfg = LLMConfig(top_p=0.9)

        resolved = t._resolve_llm_config("chat", max_tokens=100, llm_config=llm_cfg)
        assert resolved["model"] == "default-model"  # level 4
        assert resolved["temperature"] == 0.5  # level 3
        assert resolved["top_p"] == 0.9  # level 2
        assert resolved["max_tokens"] == 100  # level 1 (sugar)
        assert resolved["seed"] == 42  # level 4
        t.close()

    def test_extra_kwargs_merge_order(self):
        """Extra kwargs merge: default < operation < llm_config < call."""
        t = Tract.open()
        t._default_config = LLMConfig(extra={"a": 1, "b": 1})
        t.configure_operations(chat=LLMConfig(extra={"b": 2, "c": 2}))
        llm_cfg = LLMConfig(extra={"c": 3, "d": 3})

        resolved = t._resolve_llm_config("chat", llm_config=llm_cfg, e=4)
        assert resolved["a"] == 1  # from default
        assert resolved["b"] == 2  # op overrides default
        assert resolved["c"] == 3  # llm_config overrides op
        assert resolved["d"] == 3  # from llm_config
        assert resolved["e"] == 4  # from call kwargs
        t.close()
```

**2. Add TestFullGenerationConfigCapture class:**
```python
class TestFullGenerationConfigCapture:
    """Tests for _build_generation_config capturing all resolved fields."""

    def test_captures_all_fields(self):
        """generation_config on commit captures full resolved config."""
        t = Tract.open()
        mock = MockLLMClient()
        t.configure_llm(mock)
        t.configure_operations(
            chat=LLMConfig(model="gpt-4o", temperature=0.7, top_p=0.9, seed=42)
        )

        t.system("You are helpful")
        t.user("Hello")
        resp = t.generate()

        # All fields should be captured in generation_config
        gc = resp.generation_config
        assert gc.model is not None  # from response (authoritative)
        assert gc.temperature == 0.7
        assert gc.top_p == 0.9
        assert gc.seed == 42
        t.close()

    def test_response_model_authoritative(self):
        """Response model overrides requested model in generation_config."""
        t = Tract.open()
        mock = MockLLMClient(model="actual-model-from-api")
        t.configure_llm(mock)
        # Use operation config so mock returns its _model field ("actual-model-from-api")
        # as the response model, not the requested model
        t.configure_operations(chat=LLMConfig(model="requested-model"))

        t.system("You are helpful")
        t.user("Hello")
        resp = t.generate()

        assert resp.generation_config.model == "actual-model-from-api"
        t.close()
```

**3. Add TestLlmConfigParameter class:**
```python
class TestLlmConfigParameter:
    """Tests for llm_config= parameter on chat/generate/merge/compress."""

    def test_generate_with_llm_config(self):
        """generate(llm_config=...) forwards config to LLM."""
        t = Tract.open()
        mock = MockLLMClient()
        t.configure_llm(mock)

        t.system("You are helpful")
        t.user("Hello")
        cfg = LLMConfig(model="cfg-model", temperature=0.3, top_p=0.8)
        resp = t.generate(llm_config=cfg)

        assert mock.last_kwargs.get("model") == "cfg-model"
        assert mock.last_kwargs.get("temperature") == 0.3
        assert mock.last_kwargs.get("top_p") == 0.8
        t.close()

    def test_chat_with_llm_config(self):
        """chat(text, llm_config=...) forwards config to LLM."""
        t = Tract.open()
        mock = MockLLMClient()
        t.configure_llm(mock)

        t.system("You are helpful")
        cfg = LLMConfig(model="cfg-model", seed=42)
        resp = t.chat("Hello", llm_config=cfg)

        assert mock.last_kwargs.get("model") == "cfg-model"
        assert mock.last_kwargs.get("seed") == 42
        t.close()

    def test_sugar_overrides_llm_config(self):
        """model= sugar param overrides llm_config.model."""
        t = Tract.open()
        mock = MockLLMClient()
        t.configure_llm(mock)

        t.system("You are helpful")
        t.user("Hello")
        cfg = LLMConfig(model="cfg-model")
        resp = t.generate(model="sugar-model", llm_config=cfg)

        assert mock.last_kwargs.get("model") == "sugar-model"
        t.close()

    def test_compress_with_llm_config(self):
        """compress(llm_config=...) forwards config to LLM."""
        t = Tract.open()
        mock = MockLLMClient(responses=["Summary"])
        t.configure_llm(mock)

        t.commit(InstructionContent(text="First"))
        t.commit(DialogueContent(role="user", text="Hello"))
        t.commit(DialogueContent(role="assistant", text="Hi"))

        cfg = LLMConfig(model="compress-cfg-model", temperature=0.1)
        t.compress(llm_config=cfg)

        assert mock.last_kwargs.get("model") == "compress-cfg-model"
        assert mock.last_kwargs.get("temperature") == 0.1
        t.close()
```

**4. Add TestCompressErrorGuard class:**
```python
class TestCompressErrorGuard:
    """Tests for compress() error when LLM params provided without client."""

    def test_compress_model_without_client_raises(self):
        """compress(model=...) without LLM client raises LLMConfigError."""
        from tract.llm.errors import LLMConfigError

        t = Tract.open()
        t.commit(InstructionContent(text="First"))
        t.commit(DialogueContent(role="user", text="Hello"))
        t.commit(DialogueContent(role="assistant", text="Hi"))

        with pytest.raises(LLMConfigError, match="LLM parameters provided"):
            t.compress(model="gpt-4o")
        t.close()

    def test_compress_llm_config_without_client_raises(self):
        """compress(llm_config=...) without LLM client raises LLMConfigError."""
        from tract.llm.errors import LLMConfigError

        t = Tract.open()
        t.commit(InstructionContent(text="First"))
        t.commit(DialogueContent(role="user", text="Hello"))
        t.commit(DialogueContent(role="assistant", text="Hi"))

        with pytest.raises(LLMConfigError, match="LLM parameters provided"):
            t.compress(llm_config=LLMConfig(model="gpt-4o"))
        t.close()

    def test_compress_content_without_client_ok(self):
        """compress(content=...) without LLM client works fine (manual mode)."""
        t = Tract.open()
        t.commit(InstructionContent(text="First"))
        t.commit(DialogueContent(role="user", text="Hello"))
        t.commit(DialogueContent(role="assistant", text="Hi"))

        result = t.compress(content="Manual summary")
        assert result is not None
        t.close()

    def test_compress_model_with_content_without_client_ok(self):
        """compress(model=..., content=...) without LLM client works (content takes precedence)."""
        t = Tract.open()
        t.commit(InstructionContent(text="First"))
        t.commit(DialogueContent(role="user", text="Hello"))
        t.commit(DialogueContent(role="assistant", text="Hi"))

        # content= provided so no LLM call needed
        result = t.compress(model="gpt-4o", content="Manual summary")
        assert result is not None
        t.close()

    def test_compress_operation_config_without_client_ok(self):
        """Operation-level config without client does not raise (no explicit call-level request)."""
        t = Tract.open()
        t.configure_operations(compress=LLMConfig(model="gpt-4o"))

        t.commit(InstructionContent(text="First"))
        t.commit(DialogueContent(role="user", text="Hello"))
        t.commit(DialogueContent(role="assistant", text="Hi"))

        # No explicit LLM params on compress() call, so no error -- but it still
        # fails because no LLM client (existing CompressionError behavior)
        from tract.exceptions import CompressionError
        with pytest.raises(CompressionError, match="No LLM client configured"):
            t.compress()
        t.close()
```

**5. Add TestCompressionGenerationConfig class:**
```python
class TestCompressionGenerationConfig:
    """Tests for compression summary commits recording generation_config."""

    def test_summary_commit_has_generation_config(self):
        """LLM-compressed summary commit records the LLM config used."""
        t = Tract.open()
        mock = MockLLMClient(responses=["Summary text"])
        t.configure_llm(mock)
        t.configure_operations(compress=LLMConfig(model="compress-model", temperature=0.1))

        t.commit(InstructionContent(text="First"))
        t.commit(DialogueContent(role="user", text="Hello"))
        t.commit(DialogueContent(role="assistant", text="Hi"))

        result = t.compress()
        # The summary commit should have generation_config
        # Walk the chain to find the summary commit
        compiled = t.compile()
        # There should be at least one message with generation_config set
        # We can check via query_by_config
        results = t.query_by_config("model", "=", "compress-model")
        assert len(results) >= 1, "Summary commit should have generation_config with compress-model"
        t.close()
```

**6. Add TestOrchestratorFullConfig class:**
```python
class TestOrchestratorFullConfig:
    """Tests for orchestrator forwarding full config."""

    def test_orchestrator_config_has_max_tokens(self):
        """OrchestratorConfig accepts max_tokens field."""
        from tract.orchestrator.config import OrchestratorConfig
        config = OrchestratorConfig(max_tokens=500)
        assert config.max_tokens == 500

    def test_orchestrator_config_has_extra_llm_kwargs(self):
        """OrchestratorConfig accepts extra_llm_kwargs field."""
        from tract.orchestrator.config import OrchestratorConfig
        config = OrchestratorConfig(extra_llm_kwargs={"top_p": 0.9, "seed": 42})
        assert config.extra_llm_kwargs["top_p"] == 0.9
        assert config.extra_llm_kwargs["seed"] == 42

    def test_orchestrate_forwards_max_tokens(self):
        """orchestrate() with operation config forwards max_tokens."""
        t = Tract.open()
        mock = MockLLMClient()
        t.configure_llm(mock)
        t.configure_operations(
            orchestrate=LLMConfig(model="orch-model", max_tokens=500)
        )

        created_configs = []
        from tract.orchestrator.loop import Orchestrator as _Orchestrator
        original_init = _Orchestrator.__init__

        def capture_init(self_orch, tract_inst, *, config=None, llm_callable=None):
            created_configs.append(config)
            original_init(self_orch, tract_inst, config=config, llm_callable=llm_callable)

        _Orchestrator.__init__ = capture_init
        try:
            t.commit(InstructionContent(text="System prompt"))
            try:
                t.orchestrate()
            except Exception:
                pass

            assert len(created_configs) == 1
            config = created_configs[0]
            assert config.model == "orch-model"
            assert config.max_tokens == 500
        finally:
            _Orchestrator.__init__ = original_init
        t.close()

    def test_orchestrate_forwards_extra_fields(self):
        """orchestrate() with operation config containing top_p/seed forwards them."""
        t = Tract.open()
        mock = MockLLMClient()
        t.configure_llm(mock)
        t.configure_operations(
            orchestrate=LLMConfig(model="orch-model", top_p=0.9, seed=42)
        )

        created_configs = []
        from tract.orchestrator.loop import Orchestrator as _Orchestrator
        original_init = _Orchestrator.__init__

        def capture_init(self_orch, tract_inst, *, config=None, llm_callable=None):
            created_configs.append(config)
            original_init(self_orch, tract_inst, config=config, llm_callable=llm_callable)

        _Orchestrator.__init__ = capture_init
        try:
            t.commit(InstructionContent(text="System prompt"))
            try:
                t.orchestrate()
            except Exception:
                pass

            config = created_configs[0]
            assert config.extra_llm_kwargs is not None
            assert config.extra_llm_kwargs.get("top_p") == 0.9
            assert config.extra_llm_kwargs.get("seed") == 42
        finally:
            _Orchestrator.__init__ = original_init
        t.close()
```

  </action>
  <verify>
Run: `python -m pytest tests/test_operation_config.py -v --tb=short 2>&1 | tail -20`
Expected: All tests pass (old + new).

Run: `python -m pytest tests/ -x -q --tb=short 2>&1 | tail -5`
Expected: Full suite passes (1011+ tests, no regressions).
  </verify>
  <done>
Comprehensive tests cover: 4-level resolution chain (sugar > llm_config > operation > default for all 9 fields + extra merge order), full generation_config capture (all fields + response model authoritative), llm_config= parameter on generate/chat/compress (with sugar override), compress error guard (model/llm_config without client, content-only OK, operation-only OK), compression generation_config on summary commits, and orchestrator full config forwarding (max_tokens + extra_llm_kwargs). All existing + new tests pass, full suite has no regressions.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_operation_config.py -v --tb=short` -- all tests pass
2. `python -m pytest tests/ -q --tb=short` -- full suite passes (1011+ tests, no regressions)
3. `python -c "from tract import Tract, LLMConfig; t = Tract.open(); t._default_config = LLMConfig(model='m', top_p=0.9); r = t._resolve_llm_config('chat'); print(r)"` -- shows both model and top_p resolved
4. Verify generation_config captures full config (not just 3 fields) via test output
5. Verify compression summary commits have generation_config via test output
</verification>

<success_criteria>
1. _resolve_llm_config implements 4-level chain for all 9 typed fields + extra, with correct priority
2. _build_generation_config captures the full resolved dict (all fields, not just model/temperature/max_tokens)
3. chat()/generate()/merge()/compress() all accept llm_config: LLMConfig | None parameter
4. Sugar params (model=, temperature=, max_tokens=) override corresponding llm_config fields
5. Compression summary commits have generation_config recorded from the LLM call
6. OrchestratorConfig has max_tokens and extra_llm_kwargs; _call_llm() forwards them
7. compress() raises LLMConfigError when explicit LLM params given without client (but manual content= is fine)
8. All existing 1011+ tests pass with no regressions
9. New tests cover every fix comprehensively
</success_criteria>

<output>
After completion, create `.planning/phases/12-llmconfig-cleanup-tightening/12-02-SUMMARY.md`
</output>
