---
phase: 07-agent-toolkit-orchestrator
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/tract/orchestrator/__init__.py
  - src/tract/orchestrator/config.py
  - src/tract/orchestrator/models.py
  - src/tract/orchestrator/callbacks.py
  - src/tract/prompts/orchestrator.py
  - src/tract/exceptions.py
  - tests/test_orchestrator_models.py
autonomous: true

must_haves:
  truths:
    - "OrchestratorConfig can specify autonomy ceiling, max steps, profile, triggers, model, temperature, and callbacks"
    - "OrchestratorProposal contains recommended action, reasoning, alternatives, and supports approve/reject/modify"
    - "Built-in callbacks auto_approve, log_and_approve, cli_prompt, and reject_all work correctly"
    - "AutonomyLevel and OrchestratorState enums exist with correct values"
    - "Orchestrator system prompt provides holistic context assessment instructions"
  artifacts:
    - path: "src/tract/orchestrator/__init__.py"
      provides: "Public exports for orchestrator module"
      exports: ["OrchestratorConfig", "AutonomyLevel", "OrchestratorState", "TriggerConfig", "ToolCall", "ProposalDecision", "OrchestratorProposal", "ProposalResponse", "StepResult", "OrchestratorResult", "auto_approve", "log_and_approve", "cli_prompt", "reject_all"]
    - path: "src/tract/orchestrator/config.py"
      provides: "Orchestrator configuration types"
      contains: "class OrchestratorConfig"
    - path: "src/tract/orchestrator/models.py"
      provides: "Orchestrator result and proposal models"
      contains: "class OrchestratorProposal"
    - path: "src/tract/orchestrator/callbacks.py"
      provides: "Built-in proposal review callbacks"
      contains: "def auto_approve"
    - path: "src/tract/prompts/orchestrator.py"
      provides: "System prompts for context assessment"
      contains: "ORCHESTRATOR_SYSTEM_PROMPT"
    - path: "tests/test_orchestrator_models.py"
      provides: "Tests for orchestrator config, models, and callbacks"
      min_lines: 100
  key_links:
    - from: "src/tract/orchestrator/callbacks.py"
      to: "src/tract/orchestrator/models.py"
      via: "callbacks accept OrchestratorProposal and return ProposalResponse"
      pattern: "OrchestratorProposal.*ProposalResponse"
    - from: "src/tract/orchestrator/config.py"
      to: "src/tract/orchestrator/callbacks.py"
      via: "OrchestratorConfig.on_proposal references callback type"
      pattern: "on_proposal"
---

<objective>
Build the orchestrator data models, configuration types, proposal flow, built-in callbacks, and assessment prompts -- all the types the orchestrator loop needs, but NOT the loop itself.

Purpose: Separating models from loop logic keeps Plan 02 focused and enables Plan 03 to compose on top of stable types. These types also serve as the public API for configuring and interacting with the orchestrator.

Output: `src/tract/orchestrator/` package with config, models, callbacks, and prompts. Tests for all model/callback behavior.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-agent-toolkit-orchestrator/07-CONTEXT.md
@.planning/phases/07-agent-toolkit-orchestrator/07-RESEARCH.md
@src/tract/models/policy.py
@src/tract/policy/evaluator.py
@src/tract/exceptions.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Orchestrator Config, Models, and Callbacks</name>
  <files>
    src/tract/orchestrator/__init__.py
    src/tract/orchestrator/config.py
    src/tract/orchestrator/models.py
    src/tract/orchestrator/callbacks.py
    src/tract/exceptions.py
  </files>
  <action>
Create the `src/tract/orchestrator/` package.

**config.py** -- Configuration types:

- `AutonomyLevel(str, Enum)`: MANUAL = "manual", COLLABORATIVE = "collaborative", AUTONOMOUS = "autonomous". Follow the existing `str, Enum` pattern used across the codebase.
- `OrchestratorState(str, Enum)`: IDLE = "idle", RUNNING = "running", PAUSING = "pausing", STOPPED = "stopped".
- `TriggerConfig(frozen=True)`: on_commit_count (int | None = None), on_token_threshold (float | None = None), on_compile (bool = False). Note: `on_schedule_seconds` is deferred -- periodic scheduling requires a background timer which is incompatible with Tract's synchronous design. May be added in a future milestone.
- `OrchestratorConfig`: autonomy_ceiling (AutonomyLevel = AutonomyLevel.COLLABORATIVE), max_steps (int = 10), profile (str = "self"), system_prompt (str | None = None), task_context (str | None = None), triggers (TriggerConfig | None = None), model (str | None = None), temperature (float = 0.0), on_proposal (Callable[[OrchestratorProposal], ProposalResponse] | None = None), on_step (Callable[[StepResult], None] | None = None). Use `@dataclass` (mutable, like TractConfig pattern).

**models.py** -- Result and proposal types:

- `ToolCall(frozen=True)`: id (str), name (str), arguments (dict). This is the CANONICAL location for ToolCall. Plan 01's toolkit package will re-export ToolCall from here.

- `ProposalDecision(str, Enum)`: APPROVED = "approved", REJECTED = "rejected", MODIFIED = "modified". Follow the codebase `str, Enum` pattern for type safety (same as AutonomyLevel, CommitOperation, Priority).

- `OrchestratorProposal`: proposal_id (str), recommended_action (ToolCall), reasoning (str), alternatives (list[ToolCall], default_factory=list), context_summary (str = ""), decision (ProposalDecision | str = "pending"), modified_action (ToolCall | None = None). Use `@dataclass` (mutable -- decision changes).
- `ProposalResponse(frozen=True)`: decision (ProposalDecision), modified_action (ToolCall | None = None), reason (str = "").
- `StepResult(frozen=True)`: step (int), tool_call (ToolCall), result_output (str = ""), result_error (str = ""), success (bool = True), proposal (OrchestratorProposal | None = None).
- `OrchestratorResult(frozen=True)`: steps (list[StepResult], default_factory=list), state (OrchestratorState = OrchestratorState.IDLE), assessment (str = ""), total_tool_calls (int = 0). Add a `@property succeeded` that returns all steps with success=True.

**callbacks.py** -- Built-in proposal review callbacks:

- `auto_approve(proposal: OrchestratorProposal) -> ProposalResponse`: Returns ProposalResponse(decision="approved"). For autonomous mode.
- `log_and_approve(proposal: OrchestratorProposal) -> ProposalResponse`: Logs proposal details via `logger.info(...)` then returns ProposalResponse(decision="approved"). For audit trail mode.
- `cli_prompt(proposal: OrchestratorProposal) -> ProposalResponse`: Interactive CLI prompt. Import Rich lazily (inside function body -- Rich is optional [cli] extra). Display proposal in a Rich Panel: action name, reasoning, arguments. Prompt user for [a]pprove / [r]eject / [m]odify. If "modify": prompt for modified arguments as JSON string, parse with json.loads(), return ProposalResponse(decision=ProposalDecision.MODIFIED, modified_action=ToolCall(id=original.id, name=original.name, arguments=parsed_args)). If JSON parse fails, warn and re-prompt. If Rich not available, fall back to plain input().
- `reject_all(proposal: OrchestratorProposal) -> ProposalResponse`: Returns ProposalResponse(decision="rejected", reason="Auto-rejected"). For testing/safety.

**exceptions.py** -- Add to existing file:
- `OrchestratorError(TraceError)`: Base exception for orchestrator errors. "Raised when the orchestrator encounters an unrecoverable error."

**__init__.py** -- Re-export all public types from config.py, models.py, callbacks.py. Must include `reject_all` alongside the other callbacks. Must include `ProposalDecision` and `ToolCall` exports.

Follow all codebase conventions: `from __future__ import annotations`, logging, TYPE_CHECKING guards.
  </action>
  <verify>
python -c "from tract.orchestrator import OrchestratorConfig, AutonomyLevel, OrchestratorState, TriggerConfig, OrchestratorProposal, ProposalResponse, StepResult, OrchestratorResult, auto_approve, log_and_approve, reject_all; print('imports OK')"
  </verify>
  <done>
All orchestrator configuration and model types importable. AutonomyLevel has MANUAL/COLLABORATIVE/AUTONOMOUS. OrchestratorConfig has autonomy_ceiling, max_steps, profile, triggers. Built-in callbacks return correct ProposalResponse values. reject_all is exported from __init__.py.
  </done>
</task>

<task type="auto">
  <name>Task 2: Assessment Prompts and Tests</name>
  <files>
    src/tract/prompts/orchestrator.py
    tests/test_orchestrator_models.py
  </files>
  <action>
**prompts/orchestrator.py** -- System prompts for the orchestrator's LLM calls:

- `ORCHESTRATOR_SYSTEM_PROMPT`: A string constant. The prompt instructs the LLM to act as a context management assistant. Key sections:
  1. Role: "You are a context management assistant for an LLM conversation. Your job is to review the current context state and take actions to maintain context health."
  2. Decision framework: "Compress when token pressure is high (>80% of budget). Pin important context (system prompts, key decisions, constraints). Branch when the conversation has diverged into a tangent. Prioritize actions by impact: compression > pinning > branching > other."
  3. Behavioral rules: "If the context is healthy, respond with a brief assessment and no tool calls. If action is needed, explain your reasoning FIRST, then use the available tools. Never compress pinned content. Prefer small targeted actions over large sweeping changes."
  4. Output format: "Always start with a brief assessment of the current context state before taking any actions."
  5. Relevance/coherence guidance: "Assess relevance by examining whether recent activity aligns with the task context (if provided). Assess coherence by looking for signs of fragmentation: many short tangential commits, frequent topic switches, or abandoned threads. These signals inform whether branching or compression is warranted."

- `build_assessment_prompt(token_count: int, max_tokens: int, commit_count: int, branch_name: str, recent_commits: list[str], task_context: str | None = None, pinned_count: int = 0, skip_count: int = 0, branch_count: int = 1) -> str`: Builds the user-side assessment prompt. Format:
  ```
  Current context state:
  - Token usage: {token_count}/{max_tokens} ({pct:.0f}%)
  - Commits: {commit_count} total
  - Current branch: {branch_name}
  - Branches: {branch_count} total
  - Annotations: {pinned_count} pinned, {skip_count} skipped
  - Recent activity:
    {recent_commits joined by newlines, max 10}

  {task_context if provided}

  Review the context and determine if any maintenance actions are needed.
  ```

  The extra fields (pinned_count, skip_count, branch_count) provide lightweight structured signals that the LLM can use to infer relevance and coherence without expensive content analysis. This addresses AUTO-07's requirement for relevance, coherence, and token pressure monitoring -- token pressure is explicit in the numbers, while relevance and coherence are LLM-inferred from the activity log combined with these structural health indicators.

**tests/test_orchestrator_models.py** -- Tests for config, models, callbacks, prompts:

1. `test_autonomy_level_values` -- AutonomyLevel.MANUAL == "manual", COLLABORATIVE == "collaborative", AUTONOMOUS == "autonomous"
2. `test_orchestrator_state_values` -- All four states have correct string values
3. `test_orchestrator_config_defaults` -- Default config: ceiling=collaborative, max_steps=10, profile="self", temperature=0.0
4. `test_trigger_config_defaults` -- All None/False by default
5. `test_trigger_config_custom` -- TriggerConfig(on_commit_count=5, on_token_threshold=0.8) stores values
6. `test_orchestrator_proposal_mutable` -- Can change decision field
7. `test_proposal_response_frozen` -- ProposalResponse is frozen (cannot modify after creation)
8. `test_step_result_frozen` -- StepResult is frozen
9. `test_orchestrator_result_succeeded` -- succeeded property filters successful steps
10. `test_auto_approve_callback` -- Returns ProposalResponse(decision=ProposalDecision.APPROVED)
11. `test_log_and_approve_callback` -- Returns ProposalResponse(decision=ProposalDecision.APPROVED), logs via caplog
12. `test_reject_all_callback` -- Returns ProposalResponse(decision=ProposalDecision.REJECTED)
12b. `test_proposal_decision_enum_values` -- ProposalDecision.APPROVED == "approved", REJECTED == "rejected", MODIFIED == "modified"
13. `test_orchestrator_system_prompt_exists` -- ORCHESTRATOR_SYSTEM_PROMPT is a non-empty string
14. `test_build_assessment_prompt` -- build_assessment_prompt() returns formatted string with token counts, branch, recent commits
15. `test_build_assessment_prompt_with_task_context` -- task_context included when provided
16. `test_orchestrator_error_exception` -- OrchestratorError inherits from TraceError
17. `test_build_assessment_prompt_with_annotations` -- pinned_count and skip_count appear in output when non-zero
18. `test_build_assessment_prompt_with_branches` -- branch_count appears in output

No external dependencies needed for these tests. Use simple assertions and mock ToolCall instances.
  </action>
  <verify>
python -m pytest tests/test_orchestrator_models.py -v
  </verify>
  <done>
All 18 tests pass. Orchestrator models, config, callbacks, and prompts work correctly. OrchestratorError added to exceptions. Assessment prompt builder formats context state properly including structural health indicators (pinned_count, skip_count, branch_count) for LLM-inferred relevance and coherence assessment.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_orchestrator_models.py -v` -- all orchestrator model tests pass
2. `python -m pytest tests/ -x --timeout=120` -- full suite passes (no regressions)
3. `python -c "from tract.orchestrator import OrchestratorConfig, AutonomyLevel, auto_approve, reject_all; print(AutonomyLevel.COLLABORATIVE); print(auto_approve.__name__); print(reject_all.__name__)"` -- smoke test
</verification>

<success_criteria>
1. OrchestratorConfig supports all configuration options (autonomy ceiling, max steps, profile, triggers, callbacks)
2. OrchestratorProposal contains recommended action, reasoning, alternatives, and mutable decision
3. Four built-in callbacks (auto_approve, log_and_approve, reject_all, cli_prompt) return correct ProposalResponse
4. Assessment prompt builder formats context state properly with structural health indicators
5. All tests pass with zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/07-agent-toolkit-orchestrator/07-02-SUMMARY.md`
</output>
