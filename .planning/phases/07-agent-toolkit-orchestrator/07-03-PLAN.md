---
phase: 07-agent-toolkit-orchestrator
plan: 03
type: execute
wave: 2
depends_on: ["07-01", "07-02"]
files_modified:
  - src/tract/orchestrator/loop.py
  - src/tract/orchestrator/assessment.py
  - src/tract/orchestrator/__init__.py
  - src/tract/tract.py
  - src/tract/__init__.py
  - tests/test_orchestrator.py
autonomous: true

must_haves:
  truths:
    - "Orchestrator executes a tool-calling loop: assess context, send tools + assessment to LLM, execute tool calls, repeat until LLM stops or max_steps"
    - "In collaborative mode, each tool call becomes a proposal sent to the on_proposal callback before execution"
    - "In autonomous mode, tool calls execute directly without review"
    - "User can call stop() to immediately halt the orchestrator and pause() for graceful wind-down, without data loss"
    - "Autonomy ceiling constrains policy autonomy: min(ceiling, policy_autonomy) determines effective autonomy"
    - "The orchestrator uses the same ToolExecutor as external agents -- no special internal APIs"
    - "User can call Tract.orchestrate() as a convenience method and configure_orchestrator() to set up"
    - "_orchestrating guard prevents policy-orchestrator recursion"
    - "TriggerConfig conditions (on_commit_count, on_token_threshold, on_compile) fire the orchestrator automatically from compile() and commit()"
  artifacts:
    - path: "src/tract/orchestrator/loop.py"
      provides: "Core orchestrator agent loop"
      contains: "class Orchestrator"
      min_lines: 200
    - path: "src/tract/orchestrator/assessment.py"
      provides: "Context health assessment builder"
      contains: "def build_context_assessment"
    - path: "tests/test_orchestrator.py"
      provides: "Integration tests for orchestrator loop"
      min_lines: 200
  key_links:
    - from: "src/tract/orchestrator/loop.py"
      to: "src/tract/toolkit/executor.py"
      via: "Orchestrator uses ToolExecutor to dispatch tool calls"
      pattern: "ToolExecutor"
    - from: "src/tract/orchestrator/loop.py"
      to: "src/tract/orchestrator/config.py"
      via: "Orchestrator reads OrchestratorConfig for behavior"
      pattern: "OrchestratorConfig"
    - from: "src/tract/orchestrator/loop.py"
      to: "src/tract/llm/client.py"
      via: "Orchestrator calls LLM via tract's configured client or user-provided callable"
      pattern: "_call_llm"
    - from: "src/tract/tract.py"
      to: "src/tract/orchestrator/loop.py"
      via: "Tract.orchestrate() creates and runs Orchestrator"
      pattern: "Orchestrator"
    - from: "src/tract/orchestrator/loop.py"
      to: "src/tract/orchestrator/models.py"
      via: "Loop creates OrchestratorProposal, StepResult, OrchestratorResult"
      pattern: "OrchestratorProposal"
---

<objective>
Build the orchestrator agent loop, context assessment, policy integration, Tract facade methods, and comprehensive integration tests.

Purpose: This completes Phase 7 by wiring the toolkit (Plan 01) and models (Plan 02) into a working orchestrator that can assess context health, propose/execute operations via LLM tool calling, respect autonomy constraints, and be controlled via stop/pause. This is the reference implementation of what any agent would build with the toolkit.

Output: Working orchestrator with `Tract.orchestrate()` convenience method. Full integration tests proving the autonomy spectrum works end-to-end.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-agent-toolkit-orchestrator/07-CONTEXT.md
@.planning/phases/07-agent-toolkit-orchestrator/07-RESEARCH.md
@.planning/phases/07-agent-toolkit-orchestrator/07-01-SUMMARY.md
@.planning/phases/07-agent-toolkit-orchestrator/07-02-SUMMARY.md
@src/tract/tract.py
@src/tract/toolkit/executor.py
@src/tract/toolkit/definitions.py
@src/tract/orchestrator/config.py
@src/tract/orchestrator/models.py
@src/tract/orchestrator/callbacks.py
@src/tract/prompts/orchestrator.py
@src/tract/llm/client.py
@src/tract/policy/evaluator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Orchestrator Loop, Assessment, and Policy Integration</name>
  <files>
    src/tract/orchestrator/loop.py
    src/tract/orchestrator/assessment.py
    src/tract/orchestrator/__init__.py
  </files>
  <action>
**assessment.py** -- Context health assessment builder:

- `build_context_assessment(tract: Tract) -> str`: Builds the user-side prompt for the LLM. Returns the formatted string.

  Data gathering:
  1. `status = tract.status()` -- provides `token_count`, `branch_name`, `head_hash`, `commit_count`, `token_budget_max`, `recent_commits`
  2. `max_tokens = status.token_budget_max or 0` -- StatusInfo.token_budget_max is the configured max token budget
  3. `recent = tract.log(limit=10)` -- recent commit history for activity log
  4. `branches = tract.list_branches()` -- for `branch_count = len(branches)`
  5. Annotation counts: iterate `tract.log(limit=500)` (or a reasonable upper bound) and count commits by priority annotation. `pinned_count = sum(1 for c in all_commits if c.priority == Priority.PINNED)`, `skip_count = sum(1 for c in all_commits if c.priority == Priority.SKIP)`. Import Priority from tract.models.annotations.

  Format into the assessment prompt using `build_assessment_prompt()` from `prompts/orchestrator.py`, passing: `token_count=status.token_count`, `max_tokens=max_tokens`, `commit_count=status.commit_count`, `branch_name=status.branch_name or "detached"`, `recent_commits=[format_commit_summary(c) for c in recent]`, `task_context=task_context`, `pinned_count=pinned_count`, `skip_count=skip_count`, `branch_count=len(branches)`.

  This function must NOT call tract.compile() -- that would be expensive and trigger policy evaluation. Use tract.status() which already has token counts.

**loop.py** -- The `Orchestrator` class:

```python
class Orchestrator:
    def __init__(
        self,
        tract: Tract,
        config: OrchestratorConfig | None = None,
        llm_callable: Callable | None = None,
    ) -> None:
```

Instance variables:
- `self._tract = tract`
- `self._config = config or OrchestratorConfig()`
- `self._executor = ToolExecutor(tract)` -- import from toolkit
- `self._llm = llm_callable`
- `self._state = OrchestratorState.IDLE`
- `self._stop_event = threading.Event()`
- `self._pause_event = threading.Event()`
- `self._orchestrating = False` -- recursion guard

Methods:

**run() -> OrchestratorResult**: Core agent loop.
1. Set `self._state = OrchestratorState.RUNNING` and `self._orchestrating = True`
2. Set `self._tract._set_orchestrating(True)` (prevents policy re-evaluation triggering the orchestrator again -- the recursion guard). Plan 03 Task 2 adds `_set_orchestrating(flag: bool)` as a method on Tract that sets `self._orchestrating = flag`. This encapsulates the flag mutation (same pattern as `_in_batch` but via setter rather than direct attribute access).
3. Build context assessment via `build_context_assessment(self._tract)`
4. Get tools via `self._tract.as_tools(profile=self._config.profile)` (list of dicts)
5. Build initial messages: system prompt (from config or default ORCHESTRATOR_SYSTEM_PROMPT) + user message (assessment)
6. Loop up to max_steps:
   a. Check `_stop_event` and `_pause_event`. If set, break.
   b. Call LLM with messages + tools via `_call_llm()`
   c. Extract tool calls from response via `_extract_tool_calls()`
   d. If no tool calls, break (LLM is done)
   e. For each tool call:
      - Determine effective autonomy: `_effective_autonomy()` returns min(ceiling, default_to_collaborative)
      - If MANUAL: skip, add to steps as skipped
      - If COLLABORATIVE and on_proposal callback set: create OrchestratorProposal, call on_proposal callback, handle response (approved -> execute, rejected -> skip, modified -> execute modified)
      - If COLLABORATIVE and no callback: skip (cannot get approval)
      - If AUTONOMOUS: execute directly via ToolExecutor
      - Record StepResult for each
      - Call on_step callback if set
   f. Format tool results via `_format_tool_results(response, tool_calls, results)` -- returns list of dicts (assistant message + tool result messages). Append all to conversation via `messages.extend(...)`
   g. Check stop/pause again
7. Set `self._orchestrating = False` and `self._tract._set_orchestrating(False)`
8. Set `self._state = OrchestratorState.IDLE`
9. Return OrchestratorResult with all steps, final state, assessment text

**_call_llm(messages, tools) -> dict**: Call LLM.
- If `self._llm` is set: call `self._llm(messages=messages, tools=tools)` and return raw dict
- Otherwise: get LLM client from `self._tract` via `getattr(self._tract, "_llm_client", None)`. If None, raise OrchestratorError("No LLM client configured. Call tract.configure_llm() or provide llm_callable to Orchestrator.")
- Call `client.chat(messages, model=self._config.model, temperature=self._config.temperature, tools=tools)`
- NOTE: `OpenAIClient.chat()` does NOT have a named `tools` parameter -- `tools` flows through `**kwargs` passthrough to the underlying httpx POST body. This is correct behavior; the kwargs are merged into the request payload sent to the OpenAI-compatible API.
- Return raw response dict

**_extract_tool_calls(response: dict) -> list[ToolCall]**: Parse OpenAI-format response.
- Navigate `response["choices"][0]["message"]["tool_calls"]`
- For each: parse id, function.name, json.loads(function.arguments)
- Return list of ToolCall. Return empty list if no tool_calls key.

**_format_tool_results(response: dict, tool_calls: list[ToolCall], results: list[ToolResult]) -> list[dict]**: Format tool execution results for the LLM conversation.
- Accepts the original LLM response dict (from `_call_llm()`) to extract the assistant message.
- Returns a list of dicts to append to the conversation:
  1. First: the assistant message extracted from `response["choices"][0]["message"]` (this preserves the `tool_calls` array intact, which OpenAI requires)
  2. Then: one `{"role": "tool", "tool_call_id": tc.id, "content": result_str}` message per tool call
- The caller appends all returned dicts to `messages` via `messages.extend(...)`.
- This correctly follows the OpenAI tool-calling conversation format: assistant message with tool_calls -> tool result messages -> next LLM call.

**_effective_autonomy(policy_autonomy: AutonomyLevel | None = None) -> AutonomyLevel**: Returns `min(ceiling, policy_autonomy)` per CONTEXT.md. The hierarchy is MANUAL < COLLABORATIVE < AUTONOMOUS. If `policy_autonomy` is None, returns the ceiling. This ensures policies cannot exceed the global ceiling: if policy says AUTONOMOUS but ceiling is COLLABORATIVE, the result is COLLABORATIVE. If policy says MANUAL but ceiling is AUTONOMOUS, the result is MANUAL. Use a dict mapping: `_AUTONOMY_ORDER = {AutonomyLevel.MANUAL: 0, AutonomyLevel.COLLABORATIVE: 1, AutonomyLevel.AUTONOMOUS: 2}` and return the level with the lower ordinal.

**stop() -> None**: Set `_stop_event`, set state to STOPPED.
**pause() -> None**: Set `_pause_event`, set state to PAUSING.
**reset() -> None**: Clear both events, set state to IDLE. For reuse.

**state property -> OrchestratorState**: Return current state.

Important safety measures:
- Wrap the entire run() in try/finally to always clear `_orchestrating` flag and restore state
- Each tool execution is atomic (Tract methods commit internally)
- Stop/pause checked BETWEEN tool calls, not during
- json.loads in _extract_tool_calls wrapped in try/except (LLM might return malformed JSON)

**__init__.py update** -- Add Orchestrator to exports.

**ToolCall reconciliation**: The canonical `ToolCall` lives in `orchestrator/models.py` (Plan 02). If Plan 01 created a duplicate ToolCall in `toolkit/models.py`, delete it and replace with a re-export: `from tract.orchestrator.models import ToolCall`. The toolkit's `__init__.py` should re-export ToolCall from orchestrator. No duplicate definitions.
  </action>
  <verify>
python -c "from tract.orchestrator import Orchestrator, OrchestratorConfig, AutonomyLevel; print('Orchestrator importable')"
  </verify>
  <done>
Orchestrator class exists with run(), stop(), pause(), reset() methods. _call_llm() dispatches to configured LLM or tract's built-in client (tools flows via **kwargs on OpenAIClient.chat()). _extract_tool_calls() parses OpenAI-format responses. _orchestrating guard prevents recursion. Assessment builder works with structural health indicators.
  </done>
</task>

<task type="auto">
  <name>Task 2: Tract Facade, __init__ Exports, and Integration Tests</name>
  <files>
    src/tract/tract.py
    src/tract/__init__.py
    tests/test_orchestrator.py
  </files>
  <action>
**Tract facade methods** -- Add to `src/tract/tract.py`:

1. `_orchestrating: bool = False` -- Add as instance variable in __init__. This is the recursion guard: when True, policy evaluation should not trigger orchestrator runs. (The policy evaluator already has `_evaluating` for its own guard; this is the cross-concern guard.)

2. `configure_orchestrator(self, config: OrchestratorConfig | None = None, llm_callable: Callable | None = None) -> None`:
   - Lazy import: `from tract.orchestrator import Orchestrator, OrchestratorConfig`
   - Store `self._orchestrator = Orchestrator(self, config=config, llm_callable=llm_callable)`
   - If no llm_callable and no `self._llm_client`, warn via logger that orchestrator will need an LLM

3. `orchestrate(self, *, config: OrchestratorConfig | None = None, llm_callable: Callable | None = None) -> OrchestratorResult`:
   - If `self._orchestrator` exists and no overrides: call `self._orchestrator.run()`
   - If config or llm_callable provided: create a new Orchestrator with those params and run it
   - If no orchestrator and no params: create one with defaults, run it
   - Return the OrchestratorResult
   - Lazy imports inside method body

4. `stop_orchestrator(self) -> None`:
   - If `self._orchestrator` exists: call `self._orchestrator.stop()`

5. `pause_orchestrator(self) -> None`:
   - If `self._orchestrator` exists: call `self._orchestrator.pause()`

6. Modify the policy evaluation trigger in `compile()` and `commit()` to check `self._orchestrating`:
   - In `compile()`, find the existing `if self._policy_evaluator is not None` block and add `and not self._orchestrating` to the condition
   - In `commit()`, same: add `and not self._orchestrating` to the policy evaluation guard
   - This prevents policies from re-triggering orchestrator during an orchestrator run

7. `_set_orchestrating(self, flag: bool) -> None`: Sets `self._orchestrating = flag`. Encapsulates the recursion guard flag for the Orchestrator to use.

8. `_check_orchestrator_triggers(self, trigger: str) -> None`: Check if orchestrator triggers should fire. Called from `compile()` and `commit()` AFTER policy evaluation, guarded by `not self._orchestrating` and `not self._in_batch`.
   - If `self._orchestrator` is None or `self._orchestrator._config.triggers` is None: return immediately
   - `triggers = self._orchestrator._config.triggers`
   - If `trigger == "compile"` and `triggers.on_compile`: call `self.orchestrate()`
   - If `trigger == "commit"` and `triggers.on_commit_count` is not None: track commit count via `self._trigger_commit_count` (int, init to 0 in __init__). Increment on each commit. When `self._trigger_commit_count >= triggers.on_commit_count`: reset counter, call `self.orchestrate()`
   - If `triggers.on_token_threshold` is not None: check `status = self.status()`, compute `pct = status.token_count / status.token_budget_max` if budget exists. If `pct >= triggers.on_token_threshold`: call `self.orchestrate()`
   - Wrap in try/except to prevent trigger errors from breaking commit/compile
   - Add the trigger check call in both `compile()` and `commit()` after policy evaluation (with the same `not self._orchestrating and not self._in_batch` guard)

**__init__.py** -- Add all Phase 7 public types to imports and __all__:
- From toolkit: `ToolDefinition`, `ToolProfile`, `ToolConfig`, `ToolResult`, `ToolExecutor`
- From orchestrator: `Orchestrator`, `OrchestratorConfig`, `AutonomyLevel`, `OrchestratorState`, `TriggerConfig`, `OrchestratorProposal`, `ProposalResponse`, `StepResult`, `OrchestratorResult`, `auto_approve`, `log_and_approve`, `cli_prompt`, `reject_all`
- `OrchestratorError` (already in exceptions.py from Plan 02)

**tests/test_orchestrator.py** -- Integration tests (~200+ lines):

All tests use a mock LLM callable (not real API calls). The mock LLM callable is a function that receives messages and tools, and returns an OpenAI-format response dict. Control what the mock returns to test different scenarios.

Mock LLM helper:
```python
def make_mock_llm(responses: list[dict]) -> Callable:
    """Create a mock LLM that returns responses in sequence."""
    call_count = [0]
    def mock_llm(messages, tools=None, **kwargs):
        idx = min(call_count[0], len(responses) - 1)
        call_count[0] += 1
        return responses[idx]
    return mock_llm
```

Response helpers:
```python
def no_tool_call_response(text="Context looks healthy."):
    return {"choices": [{"message": {"role": "assistant", "content": text}}]}

def tool_call_response(tool_name, arguments, call_id="call_1", text=""):
    return {"choices": [{"message": {
        "role": "assistant", "content": text,
        "tool_calls": [{"id": call_id, "type": "function",
                        "function": {"name": tool_name, "arguments": json.dumps(arguments)}}]
    }}]}
```

Tests:

1. `test_orchestrator_no_action_needed` -- Mock LLM returns no tool calls. Orchestrator runs, returns OrchestratorResult with 0 steps.

2. `test_orchestrator_autonomous_execute` -- Config ceiling=AUTONOMOUS. Mock LLM returns one tool call (status). Verify step executed, ToolResult.success=True, LLM called twice (initial + after tool result + final no-tools).

3. `test_orchestrator_collaborative_approve` -- Config ceiling=COLLABORATIVE, on_proposal=auto_approve. Mock LLM returns one tool call. Verify proposal created and auto-approved, step executed.

4. `test_orchestrator_collaborative_reject` -- Config ceiling=COLLABORATIVE, on_proposal=reject_all. Mock LLM returns one tool call. Verify step NOT executed, result shows rejection.

5. `test_orchestrator_manual_skip` -- Config ceiling=MANUAL. Mock LLM returns tool calls. Verify no executions (all skipped).

6. `test_orchestrator_max_steps_limit` -- Mock LLM always returns tool calls. Config max_steps=3. Verify loop stops after 3 steps.

7. `test_orchestrator_stop` -- Start orchestrator. Use on_step callback to call stop() after the first step completes. Mock LLM returns tool calls for multiple steps. Verify loop exits before step 2, state=STOPPED, and first step's result is recorded.

8. `test_orchestrator_pause` -- Start orchestrator, call pause() via on_step callback after first step. Verify loop stops gracefully.

9. `test_orchestrator_no_llm_error` -- No LLM configured, no llm_callable. Verify OrchestratorError raised.

10. `test_orchestrator_recursion_guard` -- Verify _orchestrating flag is set during run() and cleared after.

11. `test_orchestrator_error_recovery` -- Mock LLM returns tool call with bad tool name. Verify ToolResult(success=False), loop continues.

12. `test_tract_orchestrate_convenience` -- Call tract.orchestrate(llm_callable=mock_llm) directly. Verify returns OrchestratorResult.

13. `test_tract_configure_orchestrator` -- Call tract.configure_orchestrator(config=..., llm_callable=...) then tract.orchestrate().

14. `test_policy_guard_during_orchestration` -- Configure policies on tract. Run orchestrator. Verify _orchestrating flag prevents policy re-evaluation during orchestrator's tool executions. (Use a mock policy that records whether evaluate() was called.)

15. `test_orchestrator_multiple_tool_calls` -- Mock LLM returns response with 2 tool calls in one turn. Both executed. Then LLM returns no more tools. Result has 2 steps.

16. `test_as_tools_integration` -- Create tract, add some commits, call as_tools(). Verify tools list. Execute a "status" tool via ToolExecutor. Verify real status returned.

17. `test_trigger_on_commit_count` -- Configure orchestrator with TriggerConfig(on_commit_count=3) and mock LLM. Commit 3 times. Verify orchestrator ran after the 3rd commit (mock LLM was called). Commit 2 more -- orchestrator should NOT have run again yet.

18. `test_trigger_on_token_threshold` -- Configure orchestrator with TriggerConfig(on_token_threshold=0.5), token budget, and mock LLM. Add enough content to exceed 50% budget. Call compile(). Verify orchestrator ran.

19. `test_trigger_on_compile` -- Configure orchestrator with TriggerConfig(on_compile=True) and mock LLM. Call compile(). Verify orchestrator ran.

All tests use real Tract instances with `Tract.open(str(tmp_path / "test.db"))`. The first positional argument to `Tract.open()` is `path` (NOT `db_path`). Follow existing test patterns from `tests/test_tract.py`.
  </action>
  <verify>
python -m pytest tests/test_orchestrator.py -v && python -m pytest tests/ -x --timeout=180
  </verify>
  <done>
Orchestrator loop works end-to-end with mock LLM. Collaborative mode proposals work with callbacks. Autonomous mode executes directly. Stop/pause halt the loop without data loss. Recursion guard prevents policy-orchestrator infinite loops. Tract.orchestrate() convenience method works. All tests pass including full regression suite (798+ existing + new tests).
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_orchestrator.py -v` -- all orchestrator integration tests pass
2. `python -m pytest tests/test_toolkit.py tests/test_orchestrator_models.py tests/test_orchestrator.py -v` -- all Phase 7 tests pass
3. `python -m pytest tests/ -x --timeout=180` -- full suite passes (798+ existing + Phase 7 tests, no regressions)
4. `python -c "from tract import Tract, Orchestrator, OrchestratorConfig, AutonomyLevel, ToolDefinition, ToolProfile, auto_approve; print('All Phase 7 types importable')"` -- smoke test
</verification>

<success_criteria>
1. Orchestrator runs a tool-calling loop: assess -> LLM -> tool calls -> repeat
2. Collaborative mode creates proposals and awaits callback approval
3. Autonomous mode executes tool calls directly
4. Manual mode skips all tool calls
5. stop() and pause() halt the loop without data loss
6. Autonomy ceiling constrains effective autonomy
7. _orchestrating guard prevents policy-orchestrator recursion
8. Tract.orchestrate() and configure_orchestrator() work as convenience methods
9. TriggerConfig conditions automatically invoke orchestrator from compile()/commit()
10. All Phase 7 tests pass, full existing suite has zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/07-agent-toolkit-orchestrator/07-03-SUMMARY.md`
</output>
