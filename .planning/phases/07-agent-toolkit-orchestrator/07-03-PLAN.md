---
phase: 07-agent-toolkit-orchestrator
plan: 03
type: execute
wave: 2
depends_on: ["07-01", "07-02"]
files_modified:
  - src/tract/orchestrator/loop.py
  - src/tract/orchestrator/assessment.py
  - src/tract/orchestrator/__init__.py
  - src/tract/tract.py
  - src/tract/__init__.py
  - tests/test_orchestrator.py
autonomous: true

must_haves:
  truths:
    - "Orchestrator executes a tool-calling loop: assess context, send tools + assessment to LLM, execute tool calls, repeat until LLM stops or max_steps"
    - "In collaborative mode, each tool call becomes a proposal sent to the on_proposal callback before execution"
    - "In autonomous mode, tool calls execute directly without review"
    - "User can call stop() to immediately halt the orchestrator and pause() for graceful wind-down, without data loss"
    - "Autonomy ceiling constrains policy autonomy: min(ceiling, policy_autonomy) determines effective autonomy"
    - "The orchestrator uses the same ToolExecutor as external agents -- no special internal APIs"
    - "User can call Tract.orchestrate() as a convenience method and configure_orchestrator() to set up"
    - "_orchestrating guard prevents policy-orchestrator recursion"
  artifacts:
    - path: "src/tract/orchestrator/loop.py"
      provides: "Core orchestrator agent loop"
      contains: "class Orchestrator"
      min_lines: 200
    - path: "src/tract/orchestrator/assessment.py"
      provides: "Context health assessment builder"
      contains: "def build_context_assessment"
    - path: "tests/test_orchestrator.py"
      provides: "Integration tests for orchestrator loop"
      min_lines: 200
  key_links:
    - from: "src/tract/orchestrator/loop.py"
      to: "src/tract/toolkit/executor.py"
      via: "Orchestrator uses ToolExecutor to dispatch tool calls"
      pattern: "ToolExecutor"
    - from: "src/tract/orchestrator/loop.py"
      to: "src/tract/orchestrator/config.py"
      via: "Orchestrator reads OrchestratorConfig for behavior"
      pattern: "OrchestratorConfig"
    - from: "src/tract/orchestrator/loop.py"
      to: "src/tract/llm/client.py"
      via: "Orchestrator calls LLM via tract's configured client or user-provided callable"
      pattern: "_call_llm"
    - from: "src/tract/tract.py"
      to: "src/tract/orchestrator/loop.py"
      via: "Tract.orchestrate() creates and runs Orchestrator"
      pattern: "Orchestrator"
    - from: "src/tract/orchestrator/loop.py"
      to: "src/tract/orchestrator/models.py"
      via: "Loop creates OrchestratorProposal, StepResult, OrchestratorResult"
      pattern: "OrchestratorProposal"
---

<objective>
Build the orchestrator agent loop, context assessment, policy integration, Tract facade methods, and comprehensive integration tests.

Purpose: This completes Phase 7 by wiring the toolkit (Plan 01) and models (Plan 02) into a working orchestrator that can assess context health, propose/execute operations via LLM tool calling, respect autonomy constraints, and be controlled via stop/pause. This is the reference implementation of what any agent would build with the toolkit.

Output: Working orchestrator with `Tract.orchestrate()` convenience method. Full integration tests proving the autonomy spectrum works end-to-end.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-agent-toolkit-orchestrator/07-CONTEXT.md
@.planning/phases/07-agent-toolkit-orchestrator/07-RESEARCH.md
@.planning/phases/07-agent-toolkit-orchestrator/07-01-SUMMARY.md
@.planning/phases/07-agent-toolkit-orchestrator/07-02-SUMMARY.md
@src/tract/tract.py
@src/tract/toolkit/executor.py
@src/tract/toolkit/definitions.py
@src/tract/orchestrator/config.py
@src/tract/orchestrator/models.py
@src/tract/orchestrator/callbacks.py
@src/tract/prompts/orchestrator.py
@src/tract/llm/client.py
@src/tract/policy/evaluator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Orchestrator Loop, Assessment, and Policy Integration</name>
  <files>
    src/tract/orchestrator/loop.py
    src/tract/orchestrator/assessment.py
    src/tract/orchestrator/__init__.py
  </files>
  <action>
**assessment.py** -- Context health assessment builder:

- `build_context_assessment(tract: Tract) -> str`: Builds the user-side prompt for the LLM. Calls `tract.status()` to get StatusInfo (token_count, branch, HEAD). Calls `tract.log(limit=10)` to get recent commits. Formats into the assessment prompt using `build_assessment_prompt()` from `prompts/orchestrator.py`. Returns the formatted string.

  This function should also gather lightweight structural health signals for the assessment:
  - `pinned_count`: Count of commits with PINNED annotation (query via storage or iterate log)
  - `skip_count`: Count of commits with SKIP annotation
  - `branch_count`: Count of branches via `tract.list_branches()`
  These are passed to `build_assessment_prompt()` to give the LLM structured signals for inferring relevance and coherence (per AUTO-07).

  This function must NOT call tract.compile() -- that would be expensive and trigger policy evaluation. Use tract.status() which already has token counts.

**loop.py** -- The `Orchestrator` class:

```python
class Orchestrator:
    def __init__(
        self,
        tract: Tract,
        config: OrchestratorConfig | None = None,
        llm_callable: Callable | None = None,
    ) -> None:
```

Instance variables:
- `self._tract = tract`
- `self._config = config or OrchestratorConfig()`
- `self._executor = ToolExecutor(tract)` -- import from toolkit
- `self._llm = llm_callable`
- `self._state = OrchestratorState.IDLE`
- `self._stop_event = threading.Event()`
- `self._pause_event = threading.Event()`
- `self._orchestrating = False` -- recursion guard

Methods:

**run() -> OrchestratorResult**: Core agent loop.
1. Set `self._state = OrchestratorState.RUNNING` and `self._orchestrating = True`
2. Set `self._tract._orchestrating = True` (prevents policy re-evaluation triggering the orchestrator again -- the recursion guard. Check if this attribute exists on tract; set it regardless. Plan 03 will also add the guard check in Tract's policy evaluation path.)
3. Build context assessment via `build_context_assessment(self._tract)`
4. Get tools via `self._tract.as_tools(profile=self._config.profile)` (list of dicts)
5. Build initial messages: system prompt (from config or default ORCHESTRATOR_SYSTEM_PROMPT) + user message (assessment)
6. Loop up to max_steps:
   a. Check `_stop_event` and `_pause_event`. If set, break.
   b. Call LLM with messages + tools via `_call_llm()`
   c. Extract tool calls from response via `_extract_tool_calls()`
   d. If no tool calls, break (LLM is done)
   e. For each tool call:
      - Determine effective autonomy: `_effective_autonomy()` returns min(ceiling, default_to_collaborative)
      - If MANUAL: skip, add to steps as skipped
      - If COLLABORATIVE and on_proposal callback set: create OrchestratorProposal, call on_proposal callback, handle response (approved -> execute, rejected -> skip, modified -> execute modified)
      - If COLLABORATIVE and no callback: skip (cannot get approval)
      - If AUTONOMOUS: execute directly via ToolExecutor
      - Record StepResult for each
      - Call on_step callback if set
   f. Format tool results as messages, append to conversation
   g. Check stop/pause again
7. Set `self._orchestrating = False` and `self._tract._orchestrating = False`
8. Set `self._state = OrchestratorState.IDLE`
9. Return OrchestratorResult with all steps, final state, assessment text

**_call_llm(messages, tools) -> dict**: Call LLM.
- If `self._llm` is set: call `self._llm(messages=messages, tools=tools)` and return raw dict
- Otherwise: get LLM client from `self._tract` via `getattr(self._tract, "_llm_client", None)`. If None, raise OrchestratorError("No LLM client configured. Call tract.configure_llm() or provide llm_callable to Orchestrator.")
- Call `client.chat(messages, model=self._config.model, temperature=self._config.temperature, tools=tools)`
- NOTE: `OpenAIClient.chat()` does NOT have a named `tools` parameter -- `tools` flows through `**kwargs` passthrough to the underlying httpx POST body. This is correct behavior; the kwargs are merged into the request payload sent to the OpenAI-compatible API.
- Return raw response dict

**_extract_tool_calls(response: dict) -> list[ToolCall]**: Parse OpenAI-format response.
- Navigate `response["choices"][0]["message"]["tool_calls"]`
- For each: parse id, function.name, json.loads(function.arguments)
- Return list of ToolCall. Return empty list if no tool_calls key.

**_format_tool_results(tool_calls, results) -> list[dict]**: Format for sending back to LLM.
- Returns the assistant message (with tool_calls) and tool result messages in OpenAI format:
  ```
  {"role": "tool", "tool_call_id": tc.id, "content": result_str}
  ```

**_effective_autonomy() -> AutonomyLevel**: Returns the ceiling from config. Simple for now -- the ceiling IS the effective autonomy for the orchestrator. (Policy-triggered orchestration would pass the policy's autonomy, but that's handled at the trigger level.)

**stop() -> None**: Set `_stop_event`, set state to STOPPED.
**pause() -> None**: Set `_pause_event`, set state to PAUSING.
**reset() -> None**: Clear both events, set state to IDLE. For reuse.

**state property -> OrchestratorState**: Return current state.

Important safety measures:
- Wrap the entire run() in try/finally to always clear `_orchestrating` flag and restore state
- Each tool execution is atomic (Tract methods commit internally)
- Stop/pause checked BETWEEN tool calls, not during
- json.loads in _extract_tool_calls wrapped in try/except (LLM might return malformed JSON)

**__init__.py update** -- Add Orchestrator to exports.

Also reconcile ToolCall: If Plan 01's toolkit/models.py defines ToolCall and Plan 02's orchestrator/models.py also defines ToolCall, choose ONE canonical location. Since the orchestrator is the heavier consumer, keep it in orchestrator/models.py and have toolkit/models.py import from there. OR keep both and they're the same frozen dataclass. Simplest: keep both identical -- they're small frozen dataclasses with the same fields. In __init__.py for each package, export ToolCall. Users import from whichever package they use.

Actually, the cleanest fix: check what Plan 01 and Plan 02 actually created. If both have ToolCall, consolidate in this plan. Move the canonical ToolCall to whichever module makes sense (orchestrator/models.py since it's the primary consumer of tool call parsing), and have toolkit re-export it. Do this consolidation ONLY if both plans created the type. If only one did, leave it.
  </action>
  <verify>
python -c "from tract.orchestrator import Orchestrator, OrchestratorConfig, AutonomyLevel; print('Orchestrator importable')"
  </verify>
  <done>
Orchestrator class exists with run(), stop(), pause(), reset() methods. _call_llm() dispatches to configured LLM or tract's built-in client (tools flows via **kwargs on OpenAIClient.chat()). _extract_tool_calls() parses OpenAI-format responses. _orchestrating guard prevents recursion. Assessment builder works with structural health indicators.
  </done>
</task>

<task type="auto">
  <name>Task 2: Tract Facade, __init__ Exports, and Integration Tests</name>
  <files>
    src/tract/tract.py
    src/tract/__init__.py
    tests/test_orchestrator.py
  </files>
  <action>
**Tract facade methods** -- Add to `src/tract/tract.py`:

1. `_orchestrating: bool = False` -- Add as instance variable in __init__. This is the recursion guard: when True, policy evaluation should not trigger orchestrator runs. (The policy evaluator already has `_evaluating` for its own guard; this is the cross-concern guard.)

2. `configure_orchestrator(self, config: OrchestratorConfig | None = None, llm_callable: Callable | None = None) -> None`:
   - Lazy import: `from tract.orchestrator import Orchestrator, OrchestratorConfig`
   - Store `self._orchestrator = Orchestrator(self, config=config, llm_callable=llm_callable)`
   - If no llm_callable and no `self._llm_client`, warn via logger that orchestrator will need an LLM

3. `orchestrate(self, *, config: OrchestratorConfig | None = None, llm_callable: Callable | None = None) -> OrchestratorResult`:
   - If `self._orchestrator` exists and no overrides: call `self._orchestrator.run()`
   - If config or llm_callable provided: create a new Orchestrator with those params and run it
   - If no orchestrator and no params: create one with defaults, run it
   - Return the OrchestratorResult
   - Lazy imports inside method body

4. `stop_orchestrator(self) -> None`:
   - If `self._orchestrator` exists: call `self._orchestrator.stop()`

5. `pause_orchestrator(self) -> None`:
   - If `self._orchestrator` exists: call `self._orchestrator.pause()`

6. Modify the policy evaluation trigger in `compile()` and `commit()` to check `self._orchestrating`:
   - In `compile()`, find the existing `if self._policy_evaluator is not None` block and add `and not self._orchestrating` to the condition
   - In `commit()`, same: add `and not self._orchestrating` to the policy evaluation guard
   - This prevents policies from re-triggering orchestrator during an orchestrator run

**__init__.py** -- Add all Phase 7 public types to imports and __all__:
- From toolkit: `ToolDefinition`, `ToolProfile`, `ToolConfig`, `ToolResult`, `ToolExecutor`
- From orchestrator: `Orchestrator`, `OrchestratorConfig`, `AutonomyLevel`, `OrchestratorState`, `TriggerConfig`, `OrchestratorProposal`, `ProposalResponse`, `StepResult`, `OrchestratorResult`, `auto_approve`, `log_and_approve`, `cli_prompt`, `reject_all`
- `OrchestratorError` (already in exceptions.py from Plan 02)

**tests/test_orchestrator.py** -- Integration tests (~200+ lines):

All tests use a mock LLM callable (not real API calls). The mock LLM callable is a function that receives messages and tools, and returns an OpenAI-format response dict. Control what the mock returns to test different scenarios.

Mock LLM helper:
```python
def make_mock_llm(responses: list[dict]) -> Callable:
    """Create a mock LLM that returns responses in sequence."""
    call_count = [0]
    def mock_llm(messages, tools=None, **kwargs):
        idx = min(call_count[0], len(responses) - 1)
        call_count[0] += 1
        return responses[idx]
    return mock_llm
```

Response helpers:
```python
def no_tool_call_response(text="Context looks healthy."):
    return {"choices": [{"message": {"role": "assistant", "content": text}}]}

def tool_call_response(tool_name, arguments, call_id="call_1", text=""):
    return {"choices": [{"message": {
        "role": "assistant", "content": text,
        "tool_calls": [{"id": call_id, "type": "function",
                        "function": {"name": tool_name, "arguments": json.dumps(arguments)}}]
    }}]}
```

Tests:

1. `test_orchestrator_no_action_needed` -- Mock LLM returns no tool calls. Orchestrator runs, returns OrchestratorResult with 0 steps.

2. `test_orchestrator_autonomous_execute` -- Config ceiling=AUTONOMOUS. Mock LLM returns one tool call (status). Verify step executed, ToolResult.success=True, LLM called twice (initial + after tool result + final no-tools).

3. `test_orchestrator_collaborative_approve` -- Config ceiling=COLLABORATIVE, on_proposal=auto_approve. Mock LLM returns one tool call. Verify proposal created and auto-approved, step executed.

4. `test_orchestrator_collaborative_reject` -- Config ceiling=COLLABORATIVE, on_proposal=reject_all. Mock LLM returns one tool call. Verify step NOT executed, result shows rejection.

5. `test_orchestrator_manual_skip` -- Config ceiling=MANUAL. Mock LLM returns tool calls. Verify no executions (all skipped).

6. `test_orchestrator_max_steps_limit` -- Mock LLM always returns tool calls. Config max_steps=3. Verify loop stops after 3 steps.

7. `test_orchestrator_stop` -- Start orchestrator, call stop() before first LLM call (use on_step callback to trigger stop). Verify state=STOPPED.

8. `test_orchestrator_pause` -- Start orchestrator, call pause() via on_step callback after first step. Verify loop stops gracefully.

9. `test_orchestrator_no_llm_error` -- No LLM configured, no llm_callable. Verify OrchestratorError raised.

10. `test_orchestrator_recursion_guard` -- Verify _orchestrating flag is set during run() and cleared after.

11. `test_orchestrator_error_recovery` -- Mock LLM returns tool call with bad tool name. Verify ToolResult(success=False), loop continues.

12. `test_tract_orchestrate_convenience` -- Call tract.orchestrate(llm_callable=mock_llm) directly. Verify returns OrchestratorResult.

13. `test_tract_configure_orchestrator` -- Call tract.configure_orchestrator(config=..., llm_callable=...) then tract.orchestrate().

14. `test_policy_guard_during_orchestration` -- Configure policies on tract. Run orchestrator. Verify _orchestrating flag prevents policy re-evaluation during orchestrator's tool executions. (Use a mock policy that records whether evaluate() was called.)

15. `test_orchestrator_multiple_tool_calls` -- Mock LLM returns response with 2 tool calls in one turn. Both executed. Then LLM returns no more tools. Result has 2 steps.

16. `test_as_tools_integration` -- Create tract, add some commits, call as_tools(). Verify tools list. Execute a "status" tool via ToolExecutor. Verify real status returned.

All tests use real Tract instances with `Tract.open(str(tmp_path / "test.db"))`. The first positional argument to `Tract.open()` is `path` (NOT `db_path`). Follow existing test patterns from `tests/test_tract.py`.
  </action>
  <verify>
python -m pytest tests/test_orchestrator.py -v && python -m pytest tests/ -x --timeout=180
  </verify>
  <done>
Orchestrator loop works end-to-end with mock LLM. Collaborative mode proposals work with callbacks. Autonomous mode executes directly. Stop/pause halt the loop without data loss. Recursion guard prevents policy-orchestrator infinite loops. Tract.orchestrate() convenience method works. All tests pass including full regression suite (798+ existing + new tests).
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_orchestrator.py -v` -- all orchestrator integration tests pass
2. `python -m pytest tests/test_toolkit.py tests/test_orchestrator_models.py tests/test_orchestrator.py -v` -- all Phase 7 tests pass
3. `python -m pytest tests/ -x --timeout=180` -- full suite passes (798+ existing + Phase 7 tests, no regressions)
4. `python -c "from tract import Tract, Orchestrator, OrchestratorConfig, AutonomyLevel, ToolDefinition, ToolProfile, auto_approve; print('All Phase 7 types importable')"` -- smoke test
</verification>

<success_criteria>
1. Orchestrator runs a tool-calling loop: assess -> LLM -> tool calls -> repeat
2. Collaborative mode creates proposals and awaits callback approval
3. Autonomous mode executes tool calls directly
4. Manual mode skips all tool calls
5. stop() and pause() halt the loop without data loss
6. Autonomy ceiling constrains effective autonomy
7. _orchestrating guard prevents policy-orchestrator recursion
8. Tract.orchestrate() and configure_orchestrator() work as convenience methods
9. All Phase 7 tests pass, full existing suite has zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/07-agent-toolkit-orchestrator/07-03-SUMMARY.md`
</output>
