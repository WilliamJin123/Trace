---
phase: 01-foundations
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/trace/engine/__init__.py
  - src/trace/engine/hashing.py
  - src/trace/engine/tokens.py
  - src/trace/engine/commit.py
  - src/trace/engine/materialize.py
  - src/trace/models/materialized.py
  - tests/test_engine/__init__.py
  - tests/test_engine/test_hashing.py
  - tests/test_engine/test_tokens.py
  - tests/test_engine/test_commit.py
  - tests/test_engine/test_materialize.py
autonomous: true

must_haves:
  truths:
    - "Content is hashed deterministically (same content always produces same hash)"
    - "Commits are created with correct parent chain, content hash, token count, and cumulative tokens"
    - "Edit commits correctly target an existing non-edit commit via reply_to"
    - "Delete operation creates a commit that marks target as excluded"
    - "Token counting works via tiktoken with o200k_base encoding by default"
    - "Custom tokenizer can replace tiktoken via TokenCounter protocol"
    - "Materialization walks commit chain, resolves edits, applies priority filtering, maps types to roles"
    - "Materialization respects as_of parameter for time-travel"
    - "Materialized output includes accurate token count"
  artifacts:
    - path: "src/trace/engine/hashing.py"
      provides: "Deterministic canonical JSON hashing for content and commits"
      exports: ["canonical_json", "content_hash", "commit_hash"]
    - path: "src/trace/engine/tokens.py"
      provides: "TiktokenCounter implementation of TokenCounter protocol"
      exports: ["TiktokenCounter"]
    - path: "src/trace/engine/commit.py"
      provides: "CommitEngine with create_commit, validate, budget checking"
      exports: ["CommitEngine"]
    - path: "src/trace/engine/materialize.py"
      provides: "DefaultMaterializer implementing Materializer protocol"
      exports: ["DefaultMaterializer"]
  key_links:
    - from: "src/trace/engine/commit.py"
      to: "src/trace/storage/repositories.py"
      via: "CommitEngine takes repository instances, delegates all DB access"
      pattern: "CommitRepository|BlobRepository|RefRepository"
    - from: "src/trace/engine/commit.py"
      to: "src/trace/engine/hashing.py"
      via: "CommitEngine uses hashing for content-addressable storage"
      pattern: "content_hash|commit_hash"
    - from: "src/trace/engine/commit.py"
      to: "src/trace/engine/tokens.py"
      via: "CommitEngine uses TokenCounter for per-commit token counting"
      pattern: "TokenCounter|count_text"
    - from: "src/trace/engine/materialize.py"
      to: "src/trace/storage/repositories.py"
      via: "Materializer reads commits and annotations from repos"
      pattern: "CommitRepository|AnnotationRepository|batch_get_latest"
    - from: "src/trace/engine/materialize.py"
      to: "src/trace/models/content.py"
      via: "Materializer uses BUILTIN_TYPE_HINTS for role mapping"
      pattern: "BUILTIN_TYPE_HINTS|ContentTypeHints"
---

<objective>
Build the core business logic layer: deterministic hashing, token counting, the commit engine (create commits with validation, parent chain, budget enforcement), and the default materializer (walk commits, resolve edits, filter by priority, map types to roles, produce structured message list). This is where the "version control for context" mechanics live.

Purpose: The commit engine and materializer are the two most important pieces of Trace. The commit engine enforces immutability, content-addressable storage, edit validation, and token accounting. The materializer converts the commit DAG into LLM-ready output. Both must be correct.
Output: Working commit engine and materializer with comprehensive tests covering the core algorithm, edit resolution, priority filtering, and time-travel materialization.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@C:\Users\jinwi\programming_files_NEW\Trace\.planning\PROJECT.md
@C:\Users\jinwi\programming_files_NEW\Trace\.planning\ROADMAP.md
@C:\Users\jinwi\programming_files_NEW\Trace\.planning\phases\01-foundations\01-CONTEXT.md
@C:\Users\jinwi\programming_files_NEW\Trace\.planning\phases\01-foundations\01-RESEARCH.md
@C:\Users\jinwi\programming_files_NEW\Trace\.planning\phases\01-foundations\01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Deterministic hashing, token counting, and commit engine</name>
  <files>
    src/trace/engine/__init__.py
    src/trace/engine/hashing.py
    src/trace/engine/tokens.py
    src/trace/engine/commit.py
    tests/test_engine/__init__.py
    tests/test_engine/test_hashing.py
    tests/test_engine/test_tokens.py
    tests/test_engine/test_commit.py
  </files>
  <action>
    **1. Deterministic hashing (engine/hashing.py):**
    Implement exactly as shown in RESEARCH.md Pattern 3:
    - `canonical_json(data: Any) -> bytes`: json.dumps with sort_keys=True, separators=(",",":"), ensure_ascii=False, encoded to utf-8
    - `content_hash(payload: dict) -> str`: SHA-256 of canonical_json(payload), returns hex digest
    - `commit_hash(content_hash, parent_hash, content_type, operation, timestamp_iso, reply_to=None) -> str`: SHA-256 of canonical JSON of structured commit data dict. Include reply_to in dict only when not None.

    CRITICAL: Pydantic models must be converted to dicts via model_dump(mode="json") BEFORE passing to canonical_json. The hashing functions operate on plain dicts/primitives, not Pydantic models.

    **2. Token counting (engine/tokens.py):**
    - `TiktokenCounter` class implementing `TokenCounter` protocol:
      - `__init__(self, model: str = "gpt-4o", encoding_name: str | None = None)`: lazy import tiktoken, get encoding. Fall back to o200k_base if model unknown.
      - `count_text(self, text: str) -> int`: encode text, return length
      - `count_messages(self, messages: list[dict]) -> int`: count with per-message overhead (3 tokens/message, 1 token/name, 3 tokens response primer) per OpenAI cookbook
    - Cache the tiktoken Encoding instance in self._enc. Do NOT re-create per call.
    - `NullTokenCounter` class (returns 0 for everything -- useful for testing)

    **3. Commit engine (engine/commit.py):**
    `CommitEngine` class that orchestrates commit creation. Constructor takes:
    - commit_repo: CommitRepository
    - blob_repo: BlobRepository
    - ref_repo: RefRepository
    - annotation_repo: AnnotationRepository
    - token_counter: TokenCounter
    - repo_id: str
    - token_budget: TokenBudgetConfig | None = None

    Methods:

    `create_commit(content: BaseModel, operation: CommitOperation = APPEND, message: str | None = None, reply_to: str | None = None, metadata: dict | None = None) -> CommitInfo`:
    1. Validate content is a valid content type (via validate_content or isinstance check)
    2. Serialize content to dict via model_dump(mode="json")
    3. Compute content_hash via hashing.content_hash(content_dict)
    4. Count tokens: token_counter.count_text(extract_text_from_content(content))
       - Helper `extract_text_from_content(content: BaseModel) -> str` that pulls the text field regardless of content type (text, content, payload as JSON string, etc.)
    5. Store blob via blob_repo.save_if_absent (content-addressable dedup)
    6. Get current HEAD hash from ref_repo.get_head(repo_id)
    7. Compute cumulative_tokens: parent.cumulative_tokens + this.token_count (0 if no parent)
    8. If token_budget set, check cumulative_tokens against max_tokens:
       - WARN: log warning, continue
       - REJECT: raise BudgetExceededError
       - CALLBACK: call callback(cumulative_tokens, max_tokens)
    9. Generate timestamp: datetime.now(timezone.utc) with microsecond precision, format as ISO 8601
    10. Compute commit_hash via hashing.commit_hash(...)
    11. Validate edit constraints:
        - If operation == EDIT: reply_to MUST be set, reply_to MUST exist in commit_repo, reply_to target MUST NOT be an EDIT operation itself
        - If operation == DELETE: reply_to MUST be set, reply_to MUST exist
    12. Create CommitRow and save via commit_repo.save
    13. Update HEAD ref via ref_repo.update_head(repo_id, commit_hash)
    14. If content type has default_priority != NORMAL (e.g., instruction -> PINNED), create initial annotation via annotation_repo.save
    15. Return CommitInfo (the SDK-facing Pydantic model)

    `get_commit(commit_hash: str) -> CommitInfo | None`:
    - Fetch from commit_repo, convert CommitRow to CommitInfo

    `annotate(target_hash: str, priority: Priority, reason: str | None = None) -> PriorityAnnotation`:
    - Validate target_hash exists
    - Create AnnotationRow and save
    - Return PriorityAnnotation model

    **4. Hashing tests (tests/test_engine/test_hashing.py):**
    - Test canonical_json produces sorted keys, compact separators
    - Test content_hash is deterministic (same input = same output, run twice)
    - Test content_hash differs for different content
    - Test commit_hash includes parent_hash in computation (same content at different positions = different commit hash)
    - Property test with Hypothesis: forall(data) => content_hash(data) == content_hash(data)

    **5. Token counting tests (tests/test_engine/test_tokens.py):**
    - Test TiktokenCounter.count_text returns positive int for non-empty text
    - Test TiktokenCounter.count_text returns 0 for empty string
    - Test TiktokenCounter.count_messages includes per-message overhead
    - Test NullTokenCounter returns 0 for everything

    **6. Commit engine tests (tests/test_engine/test_commit.py):**
    Use in-memory SQLite with real repository implementations (integration tests via the engine):
    - Test create_commit with append operation: commit saved, HEAD updated, blob stored
    - Test create_commit with same content twice: blob deduplicated (1 blob row, 2 commit rows)
    - Test create_commit with edit operation: reply_to validated, original exists
    - Test create_commit with edit targeting another edit: raises EditTargetError
    - Test create_commit with delete operation: reply_to required
    - Test cumulative_tokens computed correctly across a chain of 3 commits
    - Test token budget WARN mode: commit succeeds, warning logged
    - Test token budget REJECT mode: raises BudgetExceededError
    - Test get_commit returns CommitInfo for existing hash, None for nonexistent
    - Test annotate creates priority annotation
    - Test instruction content automatically gets PINNED annotation
  </action>
  <verify>
    Run: `cd C:\Users\jinwi\programming_files_NEW\Trace && python -m pytest tests/test_engine/test_hashing.py tests/test_engine/test_tokens.py tests/test_engine/test_commit.py -v`
    All tests pass. Hashing is deterministic, token counting works, commit engine creates/validates/stores commits correctly.
  </verify>
  <done>
    - Canonical JSON hashing is deterministic and tested with property tests
    - TiktokenCounter works with o200k_base encoding
    - CommitEngine creates commits with correct parent chain, token counts, cumulative totals
    - Edit validation prevents targeting other edits and requires reply_to
    - Token budget enforcement works in all 3 modes (warn, reject, callback)
    - Content-addressable blob deduplication works through commit engine
    - Default priority annotations auto-created for content types that need them
  </done>
</task>

<task type="auto">
  <name>Task 2: Default materializer with edit resolution, priority filtering, and time-travel</name>
  <files>
    src/trace/models/materialized.py
    src/trace/engine/materialize.py
    tests/test_engine/test_materialize.py
  </files>
  <action>
    **1. Materialized model (models/materialized.py):**
    If not already created as part of protocols.py, create a separate models file for:
    - Re-export or extend MaterializedContext from protocols.py if needed
    - `MaterializeOptions` Pydantic model: as_of (datetime | str | None = None), include_edit_annotations (bool = False), type_to_role_map (dict[str, str] | None = None for custom overrides), aggregate_same_role (bool = True)

    **2. Default materializer (engine/materialize.py):**
    `DefaultMaterializer` class implementing the `Materializer` protocol.

    Constructor takes:
    - commit_repo: CommitRepository
    - blob_repo: BlobRepository
    - annotation_repo: AnnotationRepository
    - token_counter: TokenCounter
    - type_to_role_map: dict[str, str] | None = None (overrides BUILTIN_TYPE_HINTS defaults)

    `materialize(self, repo_id: str, head_hash: str, *, as_of: datetime | str | None = None, include_edit_annotations: bool = False) -> MaterializedContext`:

    Algorithm (following RESEARCH.md Pattern 5):

    Step 1 -- Walk commit chain:
    - Start from head_hash, follow parent_hash pointers to build ordered list root-to-HEAD
    - If as_of is provided (datetime or commit hash):
      - If datetime: filter commits where created_at <= as_of
      - If string (commit hash): walk only up to that commit

    Step 2 -- Build edit resolution map:
    - For each commit with operation=EDIT, record: edit_map[reply_to] = edit_commit
    - If multiple edits target same commit, latest one (by created_at) wins
    - Only consider edits within the as_of boundary

    Step 3 -- Build priority map:
    - Batch-fetch latest annotations for all commit hashes in the chain via annotation_repo.batch_get_latest
    - If as_of provided, only consider annotations where created_at <= as_of
    - For commits without annotations, use DEFAULT_TYPE_PRIORITIES based on content_type

    Step 4 -- Build effective commit list:
    - Iterate commits in order (root to HEAD):
      - Skip commits with operation=EDIT (they are substitutions, not standalone)
      - Skip commits with operation=DELETE (they mark targets for removal)
      - Skip commits with priority=SKIP
      - If commit is an edit target (exists in edit_map), substitute the edit's content
      - If a commit is targeted by a DELETE operation, skip it
      - Include everything else

    Step 5 -- Map content types to roles:
    - Use type_to_role_map override if provided, else BUILTIN_TYPE_HINTS[content_type].default_role
    - Special case for DialogueContent: use the role field from the content itself (user/assistant/system), NOT the type default
    - Special case for ToolIOContent: use "tool" role

    Step 6 -- Build messages:
    - For each effective commit, load blob content from blob_repo
    - Parse the blob's payload_json back to a content model
    - Extract text: for most types, use the text/content field. For ToolIOContent, format as "Tool: {tool_name}\n{json.dumps(payload)}" or similar readable format. For FreeformContent, json.dumps(payload).
    - Create Message(role=mapped_role, content=extracted_text, name=optional_name)
    - If include_edit_annotations and commit was edited, append "[edited]" marker to content

    Step 7 -- Aggregate same-role consecutive messages (optional, default on):
    - If two adjacent messages have the same role, concatenate with "\n\n"
    - Do NOT aggregate across role boundaries

    Step 8 -- Count tokens:
    - Convert messages to list[dict] format: [{"role": m.role, "content": m.content}, ...]
    - Call token_counter.count_messages(messages_dicts)
    - Return MaterializedContext(messages=messages, token_count=count, commit_count=len(effective_commits), token_source=f"tiktoken:{encoding}")

    **3. Materializer tests (tests/test_engine/test_materialize.py):**
    Use full stack (in-memory SQLite + real repos + CommitEngine + DefaultMaterializer):

    Test setup helper: create a CommitEngine and DefaultMaterializer sharing the same repos/session.

    Core tests:
    - Test materialize empty repo (no commits): returns empty messages list, token_count=0
    - Test materialize single instruction commit: returns [{role: "system", content: text}]
    - Test materialize chain of 3 commits (instruction + dialogue user + dialogue assistant): returns 3 messages with correct roles
    - Test materialize with DialogueContent uses content's role field, not type default
    - Test role mapping: instruction->system, dialogue->from content, tool_io->tool, reasoning->assistant, artifact->assistant, output->assistant

    Edit resolution tests:
    - Test edit replaces original content in materialized output
    - Test multiple edits to same target: latest edit wins
    - Test edit commit itself does NOT appear as standalone message

    Priority filtering tests:
    - Test commit with SKIP annotation is excluded from materialization
    - Test commit with PINNED annotation is included
    - Test instruction commit auto-pinned via default priority

    Delete operation tests:
    - Test deleted commit is excluded from materialization
    - Test delete commit itself does NOT appear as standalone message

    Time-travel tests:
    - Test as_of with datetime: only commits up to that time included
    - Test as_of with commit hash: only commits up to and including that hash

    Aggregation tests:
    - Test consecutive same-role messages are concatenated
    - Test different-role messages remain separate

    Token counting tests:
    - Test materialized output has positive token_count for non-empty content
    - Test token_count reflects materialized content (not raw commit content)
  </action>
  <verify>
    Run: `cd C:\Users\jinwi\programming_files_NEW\Trace && python -m pytest tests/test_engine/test_materialize.py -v`
    All materializer tests pass. Edit resolution, priority filtering, time-travel, and role mapping all work correctly.

    Then run full suite: `python -m pytest tests/ -v --tb=short`
    All tests from Plan 01 + Plan 02 pass together.
  </verify>
  <done>
    - DefaultMaterializer walks commit chain root-to-HEAD correctly
    - Edit resolution: edits replace original content, latest edit wins for same target
    - Priority filtering: SKIP commits excluded, PINNED and NORMAL included
    - Delete operation: deleted commits excluded from materialization
    - Time-travel: as_of parameter limits materialization to historical state
    - Type-to-role mapping works for all 7 content types
    - DialogueContent uses its own role field (not type default)
    - Same-role consecutive messages aggregate correctly
    - Token counting on materialized output is accurate
    - All engine tests pass (hashing + tokens + commit + materialize)
  </done>
</task>

</tasks>

<verification>
Run the full test suite including all Plan 01 and Plan 02 tests:
```bash
cd C:\Users\jinwi\programming_files_NEW\Trace && python -m pytest tests/ -v --tb=short
```

Verify the commit-then-materialize cycle works end-to-end:
```bash
python -c "
from trace.storage.engine import create_trace_engine, create_session_factory
from trace.storage.schema import Base
from trace.storage.sqlite import SqliteCommitRepository, SqliteBlobRepository, SqliteRefRepository, SqliteAnnotationRepository
from trace.engine.commit import CommitEngine
from trace.engine.materialize import DefaultMaterializer
from trace.engine.tokens import TiktokenCounter
from trace.models.content import InstructionContent, DialogueContent
from trace.models.commit import CommitOperation

engine = create_trace_engine(':memory:')
Base.metadata.create_all(engine)
Session = create_session_factory(engine)
session = Session()

commit_repo = SqliteCommitRepository(session)
blob_repo = SqliteBlobRepository(session)
ref_repo = SqliteRefRepository(session)
annot_repo = SqliteAnnotationRepository(session)
counter = TiktokenCounter()

ce = CommitEngine(commit_repo, blob_repo, ref_repo, annot_repo, counter, 'test-repo')
m = DefaultMaterializer(commit_repo, blob_repo, annot_repo, counter)

c1 = ce.create_commit(InstructionContent(text='You are a helpful assistant.'), message='system prompt')
c2 = ce.create_commit(DialogueContent(role='user', text='Hello!'), message='greeting')
c3 = ce.create_commit(DialogueContent(role='assistant', text='Hi there!'), message='response')

result = m.materialize('test-repo', c3.commit_hash)
print(f'Messages: {len(result.messages)}')
for msg in result.messages:
    print(f'  [{msg.role}]: {msg.content[:50]}')
print(f'Token count: {result.token_count}')
print(f'Commit count: {result.commit_count}')
assert len(result.messages) == 3
assert result.messages[0].role == 'system'
assert result.messages[1].role == 'user'
assert result.messages[2].role == 'assistant'
print('End-to-end commit+materialize cycle works!')
"
```
</verification>

<success_criteria>
- Canonical JSON hashing is deterministic (property-tested)
- Commit engine creates commits with correct parent chain and cumulative token counts
- Edit validation prevents invalid edit targets
- Token budget enforcement works (warn, reject, callback)
- DefaultMaterializer produces correct structured message output
- Edit resolution, priority filtering, delete handling, and time-travel all work
- Type-to-role mapping is correct for all content types
- End-to-end commit-then-materialize cycle produces valid LLM-ready output
- All tests pass (40+ tests across hashing, tokens, commit, materialize)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundations/01-02-SUMMARY.md`
</output>
