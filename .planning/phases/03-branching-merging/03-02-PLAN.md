---
phase: 03-branching-merging
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/tract/llm/__init__.py
  - src/tract/llm/client.py
  - src/tract/llm/protocols.py
  - src/tract/llm/resolver.py
  - src/tract/llm/errors.py
  - pyproject.toml
  - tests/test_llm.py
autonomous: true

must_haves:
  truths:
    - "Built-in OpenAI-compatible client sends chat completion requests and parses responses"
    - "Client retries on 429/500/502/503/504 with exponential backoff, fails immediately on 401/400"
    - "User can provide a custom callable matching LLMClient protocol instead of built-in client"
    - "Built-in OpenAIResolver takes ConflictInfo and returns Resolution using LLM"
    - "Client reads api_key and base_url from env vars (TRACT_OPENAI_API_KEY, TRACT_OPENAI_BASE_URL) when not provided"
    - "httpx and tenacity are added as required dependencies"
  artifacts:
    - path: "src/tract/llm/__init__.py"
      provides: "Package exports"
      exports: ["OpenAIClient", "OpenAIResolver", "LLMClient", "ResolverCallable"]
    - path: "src/tract/llm/client.py"
      provides: "httpx-based OpenAI-compatible client"
      contains: "class OpenAIClient"
    - path: "src/tract/llm/protocols.py"
      provides: "LLMClient and ResolverCallable protocols"
      contains: "class LLMClient"
    - path: "src/tract/llm/resolver.py"
      provides: "Built-in LLM resolver for conflict resolution"
      contains: "class OpenAIResolver"
    - path: "src/tract/llm/errors.py"
      provides: "LLM-specific error hierarchy"
      contains: "class LLMClientError"
    - path: "tests/test_llm.py"
      provides: "LLM client and resolver tests"
      min_lines: 200
  key_links:
    - from: "src/tract/llm/client.py"
      to: "httpx"
      via: "httpx.Client for sync HTTP requests"
      pattern: "httpx\\.Client"
    - from: "src/tract/llm/client.py"
      to: "tenacity"
      via: "Retrying for exponential backoff on retryable errors"
      pattern: "tenacity\\.Retrying"
    - from: "src/tract/llm/resolver.py"
      to: "src/tract/llm/client.py"
      via: "Uses OpenAIClient or LLMClient protocol for chat calls"
      pattern: "LLMClient|OpenAIClient"
    - from: "src/tract/llm/resolver.py"
      to: "src/tract/llm/protocols.py"
      via: "Conforms to ResolverCallable protocol"
      pattern: "ResolverCallable"
---

<objective>
Build the LLM client infrastructure: httpx-based OpenAI-compatible client with tenacity retry, LLMClient and ResolverCallable protocols, and the built-in OpenAIResolver.

Purpose: The LLM client is required by semantic merge (Plan 03-03), compression (Phase 4), and any future LLM-powered operations. This plan delivers the client as an independent, testable package with no dependencies on branching code.
Output: Complete `src/tract/llm/` package with client, protocols, resolver, errors, and comprehensive tests using mocked httpx responses.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-branching-merging/03-CONTEXT.md
@.planning/phases/03-branching-merging/03-RESEARCH.md

@src/tract/protocols.py
@src/tract/exceptions.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: LLM client package -- protocols, errors, client</name>
  <files>
    src/tract/llm/__init__.py
    src/tract/llm/protocols.py
    src/tract/llm/errors.py
    src/tract/llm/client.py
    pyproject.toml
  </files>
  <action>
1. **pyproject.toml** -- Add httpx and tenacity as required dependencies:
   ```toml
   dependencies = [
       "sqlalchemy>=2.0.46,<2.2",
       "pydantic>=2.10,<3.0",
       "tiktoken>=0.12.0",
       "typing-extensions>=4.12",
       "httpx>=0.27,<1.0",
       "tenacity>=8.2,<10",
   ]
   ```
   Also add httpx and tenacity to the `dev` optional dependencies list so tests can use them.

2. **llm/errors.py** -- LLM error hierarchy (all inherit from TraceError):
   ```python
   from tract.exceptions import TraceError

   class LLMClientError(TraceError):
       """Base for all LLM client errors."""

   class LLMConfigError(LLMClientError):
       """Missing or invalid LLM configuration (e.g., no API key)."""

   class LLMRateLimitError(LLMClientError):
       """Rate limited by the API (429)."""
       def __init__(self, retry_after: float | None = None): ...

   class LLMAuthError(LLMClientError):
       """Authentication failed (401/403)."""

   class LLMResponseError(LLMClientError):
       """Unexpected response format from LLM API."""
   ```

3. **llm/protocols.py** -- Protocols:
   ```python
   from typing import Protocol, runtime_checkable, Literal
   from dataclasses import dataclass, field
   from pydantic import BaseModel

   @runtime_checkable
   class LLMClient(Protocol):
       def chat(self, messages: list[dict[str, str]], *, model: str | None = None,
                temperature: float | None = None, max_tokens: int | None = None) -> dict: ...
       def close(self) -> None: ...

   @dataclass
   class Resolution:
       action: Literal["resolved", "abort", "skip"]
       content: BaseModel | None = None
       content_text: str | None = None  # Raw text alternative to content model
       reasoning: str | None = None
       generation_config: dict | None = None

   @runtime_checkable
   class ResolverCallable(Protocol):
       def __call__(self, issue: object) -> Resolution: ...
   ```
   Note: The `issue` parameter is typed as `object` here because the concrete ConflictInfo/RebaseWarning/CherryPickIssue types are defined in Plan 03-03. The protocol is designed to be duck-typed.

4. **llm/client.py** -- OpenAI-compatible httpx client:
   - `class OpenAIClient` implementing `LLMClient` protocol.
   - `__init__(api_key=None, base_url=None, default_model="gpt-4o-mini", timeout=120.0, max_retries=3)`:
     - Reads `TRACT_OPENAI_API_KEY` and `TRACT_OPENAI_BASE_URL` from env if not provided.
     - Creates `httpx.Client` with timeout and auth header.
     - Raises `LLMConfigError` if api_key is empty/None AND env var is not set.
   - `chat(messages, *, model=None, temperature=None, max_tokens=None, **kwargs) -> dict`:
     - Builds payload dict with model, messages, and optional params.
     - Uses `tenacity.Retrying` (not decorator) for configurable max_retries.
     - Retry only on retryable errors: `_is_retryable()` checks status code in {429, 500, 502, 503, 504} or connection errors.
     - On 401/403: raise `LLMAuthError` immediately (no retry).
     - On success: parse response JSON, validate `choices` key exists, return dict.
     - On 429 specifically: extract `Retry-After` header if present, raise `LLMRateLimitError` with retry_after.
     - Exponential backoff: `wait_exponential(multiplier=1, min=1, max=30) + wait_random(0, 2)`.
     - Log retries via `before_sleep_log`.
   - `close()`: Closes the underlying httpx.Client.
   - `__enter__`/`__exit__` for context manager support.
   - Helper `extract_content(response: dict) -> str`: Extracts `response["choices"][0]["message"]["content"]`. Raises `LLMResponseError` if format is unexpected.
   - Helper `extract_usage(response: dict) -> dict | None`: Extracts `response.get("usage")`.

5. **llm/__init__.py** -- Package exports:
   ```python
   from tract.llm.client import OpenAIClient
   from tract.llm.protocols import LLMClient, ResolverCallable, Resolution
   from tract.llm.errors import LLMClientError, LLMConfigError, LLMRateLimitError, LLMAuthError, LLMResponseError
   ```
   Do NOT import OpenAIResolver yet (added in Task 2).
  </action>
  <verify>
    `pip install httpx tenacity` (or `uv pip install httpx tenacity`)
    `python -c "from tract.llm import OpenAIClient, LLMClient, ResolverCallable, Resolution"` succeeds.
    `python -c "from tract.llm.errors import LLMClientError, LLMAuthError"` succeeds.
    `python -m pytest tests/ -x -q` -- all 302 existing tests pass (no imports broken).
  </verify>
  <done>LLM client package exists with OpenAIClient, protocols, and error hierarchy. httpx and tenacity are installed dependencies. No existing tests broken.</done>
</task>

<task type="auto">
  <name>Task 2: OpenAIResolver and comprehensive tests</name>
  <files>
    src/tract/llm/resolver.py
    src/tract/llm/__init__.py
    tests/test_llm.py
  </files>
  <action>
1. **llm/resolver.py** -- Built-in OpenAI-powered resolver:
   ```python
   class OpenAIResolver:
       """Built-in conflict resolver using an OpenAI-compatible LLM.

       Implements the ResolverCallable protocol. Sends conflict info
       to the LLM and returns a Resolution.
       """
       def __init__(self, client: LLMClient | OpenAIClient, *, model: str | None = None,
                    temperature: float = 0.3, max_tokens: int = 2048,
                    system_prompt: str | None = None):
           self._client = client
           self._model = model
           self._temperature = temperature
           self._max_tokens = max_tokens
           self._system_prompt = system_prompt or self._default_system_prompt()

       @staticmethod
       def _default_system_prompt() -> str:
           return (
               "You are a context merge resolver for an LLM context management system. "
               "You receive conflicting context from two branches and must produce a single, "
               "coherent resolution. Respond with ONLY the resolved content text. "
               "Do not add explanations or metadata unless asked."
           )

       def __call__(self, issue: object) -> Resolution:
           # Build messages from issue. The issue object has attributes:
           # conflict_type, commit_a, commit_b, ancestor, etc.
           # Use getattr() for duck typing since ConflictInfo is in a different plan.
           user_prompt = self._format_issue(issue)
           messages = [
               {"role": "system", "content": self._system_prompt},
               {"role": "user", "content": user_prompt},
           ]
           response = self._client.chat(
               messages, model=self._model,
               temperature=self._temperature, max_tokens=self._max_tokens,
           )
           content_text = response["choices"][0]["message"]["content"]
           usage = response.get("usage")
           gen_config = {
               "model": response.get("model", self._model),
               "temperature": self._temperature,
               "source": "infrastructure:merge",
           }
           if usage:
               gen_config["usage"] = usage
           return Resolution(
               action="resolved",
               content_text=content_text,
               reasoning=f"LLM-resolved using model {gen_config.get('model')}",
               generation_config=gen_config,
           )

       def _format_issue(self, issue: object) -> str:
           # Duck-type access to issue attributes
           conflict_type = getattr(issue, "conflict_type", "unknown")
           parts = [f"Conflict type: {conflict_type}"]
           commit_a = getattr(issue, "commit_a", None)
           commit_b = getattr(issue, "commit_b", None)
           if commit_a:
               parts.append(f"Branch A content:\n{getattr(commit_a, 'message', str(commit_a))}")
           if commit_b:
               parts.append(f"Branch B content:\n{getattr(commit_b, 'message', str(commit_b))}")
           context = getattr(issue, "compiled_context", None)
           if context and hasattr(context, "messages"):
               msgs = context.messages[:5]  # Cost-smart: only send surrounding context
               ctx_text = "\n".join(f"[{m.role}] {m.content[:200]}" for m in msgs)
               parts.append(f"Surrounding context (truncated):\n{ctx_text}")
           return "\n\n".join(parts)
   ```

2. **llm/__init__.py** -- Add OpenAIResolver to exports:
   ```python
   from tract.llm.resolver import OpenAIResolver
   ```

3. **tests/test_llm.py** -- Comprehensive test suite (~250+ lines):

   **Client tests (mock httpx):**
   - `test_chat_success`: Mock successful response, verify request format and response parsing.
   - `test_chat_with_optional_params`: temperature, max_tokens, extra kwargs forwarded.
   - `test_chat_extracts_content`: `extract_content()` returns message content string.
   - `test_chat_extracts_usage`: `extract_usage()` returns usage dict.
   - `test_retry_on_429`: Mock 429 then 200, verify retries and succeeds. Use `httpx.MockTransport` or mock the `_client.post` method.
   - `test_retry_on_500`: Mock 500 then 200, verify retries.
   - `test_no_retry_on_401`: Mock 401, verify LLMAuthError raised immediately (no retry).
   - `test_no_retry_on_400`: Mock 400, verify HTTPStatusError raised immediately.
   - `test_max_retries_exhausted`: Mock 429 N+1 times, verify raises after max_retries.
   - `test_env_var_config`: Monkeypatch TRACT_OPENAI_API_KEY and TRACT_OPENAI_BASE_URL, verify client picks them up.
   - `test_missing_api_key_raises`: No key in env or constructor, raises LLMConfigError.
   - `test_context_manager`: `with OpenAIClient(...) as c:` works, close() called on exit.

   **Protocol tests:**
   - `test_openai_client_conforms_to_protocol`: `isinstance(OpenAIClient(...), LLMClient)` is True.
   - `test_custom_client_protocol`: Define a minimal class with chat() and close(), verify isinstance check.
   - `test_resolver_callable_protocol`: `isinstance(OpenAIResolver(...), ResolverCallable)` -- note: this may not work with runtime_checkable if __call__ has different sig. Test the duck-typing contract instead: create resolver, call it with a mock issue object.

   **Resolver tests (mock client):**
   - `test_resolver_returns_resolution`: Mock LLMClient, pass a dummy issue object (SimpleNamespace with conflict_type, commit_a, commit_b), verify Resolution is returned with action="resolved".
   - `test_resolver_custom_system_prompt`: Pass custom system_prompt, verify it's included in messages sent to client.
   - `test_resolver_formats_issue`: Check that _format_issue() produces reasonable output for different issue shapes.
   - `test_resolver_records_generation_config`: Verify resolution.generation_config has model, temperature, source="infrastructure:merge".

   **Mocking strategy:** Use `unittest.mock.patch` or create a `MockLLMClient` class that records calls and returns canned responses. For httpx mocking, create a mock transport:
   ```python
   def mock_transport(request):
       return httpx.Response(200, json={"choices": [{"message": {"content": "resolved"}}], "usage": {"prompt_tokens": 10, "completion_tokens": 5, "total_tokens": 15}, "model": "gpt-4o-mini"})
   client = OpenAIClient(api_key="test-key", base_url="http://test")
   client._client = httpx.Client(transport=httpx.MockTransport(mock_transport))
   ```
  </action>
  <verify>
    `python -m pytest tests/test_llm.py -x -q` -- all new tests pass.
    `python -m pytest tests/ -x -q` -- all tests pass (302 existing + new).
  </verify>
  <done>
    - OpenAIClient sends properly formatted requests and retries correctly
    - LLMClient protocol is runtime-checkable
    - OpenAIResolver takes conflict info and returns Resolution
    - All retry, auth, and error scenarios covered by tests
    - All existing + new tests pass
  </done>
</task>

</tasks>

<verification>
1. All 302 existing tests pass (zero regressions)
2. New LLM tests pass
3. `from tract.llm import OpenAIClient, LLMClient, ResolverCallable, OpenAIResolver, Resolution` works
4. `from tract.llm.errors import LLMClientError, LLMAuthError, LLMRateLimitError` works
5. httpx and tenacity in pyproject.toml dependencies
</verification>

<success_criteria>
- OpenAIClient makes real-shaped HTTP requests (verified via mocked transport)
- Retry logic: retries 429/5xx, fails fast on 401/403/400
- LLMClient protocol works for custom implementations
- OpenAIResolver produces Resolution from conflict info
- httpx and tenacity added as required dependencies
- All existing + new tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-branching-merging/03-02-SUMMARY.md`
</output>
