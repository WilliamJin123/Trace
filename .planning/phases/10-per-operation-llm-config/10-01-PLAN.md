---
phase: 10-per-operation-llm-config
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/tract/models/config.py
  - src/tract/tract.py
  - src/tract/operations/compression.py
  - src/tract/__init__.py
  - tests/test_operation_config.py
autonomous: true

must_haves:
  truths:
    - "User can set different models per operation (chat uses gpt-4o, compress uses gpt-3.5-turbo) and each operation uses its configured model"
    - "User can set per-operation defaults that persist across multiple calls to the same operation"
    - "User can override per-operation config on individual calls (call-level > operation-level > tract-level)"
    - "Existing code without per-operation config works identically (backward compatible)"
    - "generation_config on commits accurately reflects the actual model used (including per-operation defaults)"
    - "auto_message is excluded from per-operation LLM config -- it is a pure-string truncation function with no LLM call, so per-operation LLM settings do not apply"
  artifacts:
    - path: "src/tract/models/config.py"
      provides: "LLMOperationConfig frozen dataclass"
      contains: "class LLMOperationConfig"
    - path: "src/tract/tract.py"
      provides: "_resolve_llm_config(), configure_operations(), operation_configs property"
      contains: "_resolve_llm_config"
    - path: "src/tract/operations/compression.py"
      provides: "_summarize_group() with llm_kwargs forwarding"
      contains: "llm_kwargs"
    - path: "tests/test_operation_config.py"
      provides: "Comprehensive per-operation config tests"
      min_lines: 150
  key_links:
    - from: "src/tract/tract.py (generate)"
      to: "_resolve_llm_config('chat', ...)"
      via: "config resolution before LLM call"
      pattern: "_resolve_llm_config.*chat"
    - from: "src/tract/tract.py (compress)"
      to: "_resolve_llm_config('compress', ...)"
      via: "config resolution passed as llm_kwargs"
      pattern: "_resolve_llm_config.*compress"
    - from: "src/tract/tract.py (merge)"
      to: "_resolve_llm_config('merge', ...)"
      via: "config resolution for resolver creation"
      pattern: "_resolve_llm_config.*merge"
    - from: "src/tract/tract.py (orchestrate)"
      to: "_resolve_llm_config('orchestrate')"
      via: "config resolution BEFORE three-way branch, using dataclasses.replace() for mutation safety"
      pattern: "_resolve_llm_config.*orchestrate"
---

<objective>
Add per-operation LLM configuration so each LLM-powered operation (chat, merge, compress, orchestrate) can independently use different models and parameters.

Purpose: Currently all operations share a single _default_model with no per-operation defaults. Users need different models for different tasks (cheap model for compression, powerful model for chat, precise model for merge). This is the final phase of v3.0 DX milestone.

Output: LLMOperationConfig dataclass, configure_operations() method, _resolve_llm_config() helper, all 4 active operations wired through the resolution chain, comprehensive tests.

Note: `_auto_message()` is a pure-string function (truncates content_type + text to 72 chars). It makes no LLM call and therefore is NOT included in per-operation LLM config. The 4 LLM-powered operations are: chat/generate, merge, compress, orchestrate.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-per-operation-llm-config/10-RESEARCH.md
@.planning/phases/09-conversation-layer/09-01-SUMMARY.md
@src/tract/models/config.py
@src/tract/tract.py
@src/tract/operations/compression.py
@src/tract/llm/resolver.py
@src/tract/orchestrator/config.py
@src/tract/orchestrator/loop.py
@src/tract/__init__.py
@tests/test_conversation.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: LLMOperationConfig dataclass, resolution helper, and configure_operations()</name>
  <files>
    src/tract/models/config.py
    src/tract/tract.py
    src/tract/__init__.py
  </files>
  <action>
**1. Add LLMOperationConfig to src/tract/models/config.py:**

Add a frozen dataclass (NOT Pydantic -- this is runtime-only, not persisted) at the end of the file:

```python
from dataclasses import dataclass

@dataclass(frozen=True)
class LLMOperationConfig:
    """Per-operation LLM configuration defaults.

    None fields mean 'inherit from tract-level default'.
    Use with Tract.configure_operations() to set different models
    and parameters for each LLM-powered operation.
    """
    model: str | None = None
    temperature: float | None = None
    max_tokens: int | None = None
    extra_kwargs: dict | None = None
```

Note: Use `from __future__ import annotations` at the top of config.py if not already present (it is not -- check first). The file currently uses Pydantic's BaseModel; the dataclass import is separate and fine alongside it.

**2. Add _operation_configs dict and methods to Tract in src/tract/tract.py:**

In `__init__()` (around line 163, after `self._default_model`), add:
```python
self._operation_configs: dict[str, LLMOperationConfig] = {}
```

Import LLMOperationConfig at the top of tract.py alongside the existing config import:
```python
from tract.models.config import TractConfig, LLMOperationConfig
```

Add `configure_operations()` method near the existing `configure_llm()` method (after line 1425):
```python
def configure_operations(self, **operation_configs: LLMOperationConfig) -> None:
    """Set per-operation LLM defaults.

    Each keyword argument maps an operation name to its config.
    Valid operation names: "chat", "merge", "compress", "orchestrate".

    Note: "auto_message" is NOT a valid operation name -- _auto_message()
    is a pure-string function that truncates text without any LLM call.

    Call-level overrides (e.g., model= on chat()) still take highest priority.

    Args:
        **operation_configs: Operation name -> LLMOperationConfig mappings.

    Example::

        from tract import LLMOperationConfig
        t.configure_operations(
            chat=LLMOperationConfig(model="gpt-4o", temperature=0.7),
            compress=LLMOperationConfig(model="gpt-3.5-turbo", temperature=0.0),
            merge=LLMOperationConfig(model="gpt-4o", temperature=0.3),
        )
    """
    from tract.models.config import LLMOperationConfig as _LLMOperationConfig
    for name, config in operation_configs.items():
        if not isinstance(config, _LLMOperationConfig):
            raise TypeError(
                f"Expected LLMOperationConfig for '{name}', got {type(config).__name__}"
            )
        self._operation_configs[name] = config
```

Add `_resolve_llm_config()` private helper method (near _build_generation_config, around line 620):
```python
def _resolve_llm_config(
    self,
    operation: str,
    *,
    model: str | None = None,
    temperature: float | None = None,
    max_tokens: int | None = None,
    **kwargs: object,
) -> dict:
    """Resolve effective LLM config: call-level > operation-level > tract-level.

    Returns a dict of kwargs to pass to llm_client.chat(). Only includes
    keys that have a non-None value at some level in the chain.

    Args:
        operation: Operation name ("chat", "merge", "compress", "orchestrate").
        model: Call-level model override.
        temperature: Call-level temperature override.
        max_tokens: Call-level max_tokens override.
        **kwargs: Additional call-level kwargs (highest priority).

    Returns:
        Dict of resolved kwargs for llm_client.chat().
    """
    op_config = self._operation_configs.get(operation)

    resolved: dict = {}

    # Model: call > operation > tract default
    if model is not None:
        resolved["model"] = model
    elif op_config is not None and op_config.model is not None:
        resolved["model"] = op_config.model
    elif self._default_model is not None:
        resolved["model"] = self._default_model

    # Temperature: call > operation
    if temperature is not None:
        resolved["temperature"] = temperature
    elif op_config is not None and op_config.temperature is not None:
        resolved["temperature"] = op_config.temperature

    # Max tokens: call > operation
    if max_tokens is not None:
        resolved["max_tokens"] = max_tokens
    elif op_config is not None and op_config.max_tokens is not None:
        resolved["max_tokens"] = op_config.max_tokens

    # Extra kwargs from operation config (call kwargs override)
    if op_config is not None and op_config.extra_kwargs:
        resolved.update(op_config.extra_kwargs)
    resolved.update(kwargs)

    return resolved
```

Add a read-only property for inspecting operation configs:
```python
@property
def operation_configs(self) -> dict[str, LLMOperationConfig]:
    """Current per-operation LLM configurations (read-only copy)."""
    return dict(self._operation_configs)
```

**3. Add operation_configs param to Tract.open():**

In `Tract.open()` signature (line 166), add parameter:
```python
operation_configs: dict[str, LLMOperationConfig] | None = None,
```

After the LLM auto-configuration block (after line 314 `tract._default_model = model`), add:
```python
# Apply per-operation configs if provided
if operation_configs is not None:
    tract.configure_operations(**operation_configs)
```

**4. Export LLMOperationConfig from src/tract/__init__.py:**

Add to the imports (in the Configuration section around line 32):
```python
from tract.models.config import TractConfig, TokenBudgetConfig, BudgetAction, LLMOperationConfig
```

Add `"LLMOperationConfig"` to the `__all__` list in the Config section.
  </action>
  <verify>
Run: `python -c "from tract import LLMOperationConfig; c = LLMOperationConfig(model='gpt-4o', temperature=0.7); print(c)"`
Confirm it prints the frozen dataclass without errors.

Run: `python -c "from tract import Tract, LLMOperationConfig; t = Tract.open(); t.configure_operations(chat=LLMOperationConfig(model='x')); print(t.operation_configs)"`
Confirm it prints `{'chat': LLMOperationConfig(model='x', ...)}`.
  </verify>
  <done>
LLMOperationConfig exists as frozen dataclass with model/temperature/max_tokens/extra_kwargs fields. Tract has _operation_configs dict, configure_operations() method, _resolve_llm_config() helper, and operation_configs property. Tract.open() accepts operation_configs parameter. LLMOperationConfig is exported from tract package. configure_operations() docstring lists only the 4 LLM-powered operations (chat, merge, compress, orchestrate) -- auto_message is excluded because it has no LLM call.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire all operations through resolution chain</name>
  <files>
    src/tract/tract.py
    src/tract/operations/compression.py
  </files>
  <action>
**1. Wire generate() through _resolve_llm_config("chat") in src/tract/tract.py:**

In `generate()` method (around line 698-704), replace the manual llm_kwargs building:

BEFORE:
```python
llm_kwargs: dict = {}
if model is not None:
    llm_kwargs["model"] = model
if temperature is not None:
    llm_kwargs["temperature"] = temperature
if max_tokens is not None:
    llm_kwargs["max_tokens"] = max_tokens
response = self._llm_client.chat(messages, **llm_kwargs)
```

AFTER:
```python
llm_kwargs = self._resolve_llm_config(
    "chat", model=model, temperature=temperature, max_tokens=max_tokens,
)
response = self._llm_client.chat(messages, **llm_kwargs)
```

Also update `_build_generation_config()` to accept the resolved config so generation_config accurately captures what was actually used. Change the call site in generate() (around line 713):

BEFORE:
```python
gen_config = self._build_generation_config(
    response, model=model, temperature=temperature, max_tokens=max_tokens
)
```

AFTER:
```python
gen_config = self._build_generation_config(
    response,
    model=llm_kwargs.get("model"),
    temperature=llm_kwargs.get("temperature"),
    max_tokens=llm_kwargs.get("max_tokens"),
)
```

This ensures generation_config reflects the RESOLVED model (which may come from per-operation config), not just the call-level override. The _build_generation_config method itself does NOT need changes -- it already has the right logic (response model > param model > _default_model). Now the param model may be from operation config, which is correct.

**2. Wire merge() through _resolve_llm_config("merge") in src/tract/tract.py:**

In `merge()` method (around line 1460-1469), replace the model-only resolver creation:

BEFORE:
```python
effective_resolver = resolver
if effective_resolver is None:
    effective_resolver = getattr(self, "_default_resolver", None)

if model is not None and effective_resolver is getattr(self, "_default_resolver", None):
    if hasattr(self, "_llm_client"):
        from tract.llm.resolver import OpenAIResolver
        effective_resolver = OpenAIResolver(self._llm_client, model=model)
```

AFTER:
```python
effective_resolver = resolver
if effective_resolver is None:
    effective_resolver = getattr(self, "_default_resolver", None)

# Resolve per-operation config for merge
merge_config = self._resolve_llm_config(
    "merge", model=model, temperature=temperature, max_tokens=max_tokens,
)

# If using default resolver AND config differs from default, create tailored resolver
if effective_resolver is getattr(self, "_default_resolver", None) and merge_config:
    if hasattr(self, "_llm_client"):
        from tract.llm.resolver import OpenAIResolver
        effective_resolver = OpenAIResolver(
            self._llm_client,
            model=merge_config.get("model"),
            temperature=merge_config.get("temperature", 0.3),
            max_tokens=merge_config.get("max_tokens", 2048),
        )
```

Also add `temperature` and `max_tokens` parameters to the `merge()` method signature (they don't exist currently):
```python
def merge(
    self,
    source_branch: str,
    *,
    resolver: object | None = None,
    strategy: str = "auto",
    no_ff: bool = False,
    auto_commit: bool = False,
    model: str | None = None,
    temperature: float | None = None,
    max_tokens: int | None = None,
    delete_branch: bool = False,
    message: str | None = None,
) -> MergeResult:
```

**3. Wire compress() through _resolve_llm_config("compress") in src/tract/tract.py:**

Add `model`, `temperature`, and `max_tokens` keyword arguments to the `compress()` method signature:
```python
def compress(
    self,
    *,
    commits: list[str] | None = None,
    from_commit: str | None = None,
    to_commit: str | None = None,
    target_tokens: int | None = None,
    preserve: list[str] | None = None,
    auto_commit: bool = True,
    content: str | None = None,
    instructions: str | None = None,
    system_prompt: str | None = None,
    model: str | None = None,
    temperature: float | None = None,
    max_tokens: int | None = None,
) -> CompressResult | PendingCompression:
```

In `compress()` method (around line 1752-1778), after getting llm_client but before calling compress_range, resolve config using the new call-level params:

Add before the compress_range call:
```python
# Resolve per-operation LLM config for compress
llm_kwargs = self._resolve_llm_config(
    "compress", model=model, temperature=temperature, max_tokens=max_tokens,
) if llm_client is not None else {}
```

Pass llm_kwargs to compress_range:
```python
result = compress_range(
    ...  # all existing params unchanged
    llm_client=llm_client,
    llm_kwargs=llm_kwargs,  # NEW
    content=content,
    ...
)
```

**4. Update compress_range() and _summarize_group() in src/tract/operations/compression.py:**

Add `llm_kwargs: dict | None = None` parameter to `compress_range()` signature (around line 386, after `system_prompt`):
```python
def compress_range(
    ...,
    system_prompt: str | None = None,
    llm_kwargs: dict | None = None,  # NEW: per-operation LLM config
    type_registry: dict[str, type] | None = None,
) -> CompressResult | PendingCompression:
```

Pass llm_kwargs to _summarize_group in the LLM mode block (around line 470-475):
```python
summary = _summarize_group(
    text, llm_client, token_counter,
    target_tokens=target_tokens,
    instructions=instructions,
    system_prompt=system_prompt,
    llm_kwargs=llm_kwargs,  # NEW
)
```

Add `llm_kwargs: dict | None = None` parameter to `_summarize_group()` (around line 316):
```python
def _summarize_group(
    messages_text: str,
    llm_client: LLMClient,
    token_counter: TokenCounter,
    *,
    target_tokens: int | None = None,
    instructions: str | None = None,
    system_prompt: str | None = None,
    llm_kwargs: dict | None = None,
) -> str:
```

Update the LLM call in _summarize_group (around line 353):

BEFORE:
```python
response = llm_client.chat(messages)
```

AFTER:
```python
response = llm_client.chat(messages, **(llm_kwargs or {}))
```

**5. Wire orchestrate() through _resolve_llm_config("orchestrate") in src/tract/tract.py:**

In `orchestrate()` method (around line 2416-2433), the current code has a three-way branch:
1. `if config is not None or llm_callable is not None:` -- create new orchestrator with overrides
2. `if self._orchestrator is not None:` -- reuse existing orchestrator
3. else -- create one with defaults

The fix: resolve per-operation config BEFORE the three-way branch, and apply it to the config using `dataclasses.replace()` for mutation safety (OrchestratorConfig is a mutable dataclass, but we must not mutate a caller-supplied object).

REPLACE the entire orchestrate() method body (after the docstring and the `from tract.orchestrator import Orchestrator as _Orchestrator` import) with:

```python
from tract.orchestrator import Orchestrator as _Orchestrator

# Step 1: Resolve per-operation config BEFORE the three-way branch
orch_resolved = self._resolve_llm_config("orchestrate")

# Step 2: If operation-level config found, apply to config (mutation-safe)
if orch_resolved:
    if config is not None:
        # Caller provided config -- only fill in None/default fields
        # Use dataclasses.replace() to avoid mutating the caller's object
        overrides = {}
        if config.model is None and "model" in orch_resolved:
            overrides["model"] = orch_resolved["model"]
        if config.temperature == 0.0 and "temperature" in orch_resolved:
            overrides["temperature"] = orch_resolved["temperature"]
        if overrides:
            config = replace(config, **overrides)
    else:
        # No caller config -- create one from operation defaults
        from tract.orchestrator.config import OrchestratorConfig as _OrchestratorConfig
        config = _OrchestratorConfig(
            model=orch_resolved.get("model"),
            temperature=orch_resolved.get("temperature", 0.0),
        )

# Step 3: Three-way branch (now with resolved config)
# If overrides provided, create a new orchestrator
if config is not None or llm_callable is not None:
    orch = _Orchestrator(
        self, config=config, llm_callable=llm_callable
    )
    return orch.run(trigger_autonomy=trigger_autonomy)

# If existing orchestrator, reuse it
if self._orchestrator is not None:
    orch_inst: _Orchestrator = self._orchestrator  # type: ignore[assignment]
    orch_inst.reset()
    return orch_inst.run(trigger_autonomy=trigger_autonomy)

# Create one with defaults
orch = _Orchestrator(self)
return orch.run(trigger_autonomy=trigger_autonomy)
```

Key design decisions:
- `dataclasses.replace(config, **overrides)` creates a new OrchestratorConfig instead of mutating the caller's object. `replace` is already imported at the top of tract.py (`from dataclasses import replace`).
- Resolution happens BEFORE the three-way branch so ALL code paths benefit from operation-level config (including the "reuse existing orchestrator" path -- if operation config is set, it creates a config which sends us into the first branch instead of the reuse branch, which is correct because per-operation config should take effect).
- When `config is not None` AND `orch_resolved` has values, we only override fields that are still at their defaults (model=None, temperature=0.0), preserving explicit caller choices.
  </action>
  <verify>
Run: `python -m pytest tests/ -x --timeout=120 -q` -- full existing suite passes (no regressions from wiring changes).

Manual spot-check: read the orchestrate() method and verify:
1. `_resolve_llm_config("orchestrate")` appears BEFORE the `if config is not None or llm_callable is not None:` branch
2. `replace(config, **overrides)` is used instead of `config.model = ...`
3. compress() signature includes model/temperature/max_tokens params
  </verify>
  <done>
All 4 active LLM operations wired through _resolve_llm_config(): generate() with "chat", merge() with "merge", compress() with "compress" (including call-level model/temperature/max_tokens params), orchestrate() with "orchestrate" (resolution before three-way branch, dataclasses.replace for mutation safety). No mutation of caller-supplied objects.
  </done>
</task>

<task type="auto">
  <name>Task 3: Comprehensive tests for per-operation config</name>
  <files>
    tests/test_operation_config.py
  </files>
  <action>
Create tests/test_operation_config.py with comprehensive tests.

Use the MockLLMClient pattern from tests/test_conversation.py. Tests to write:

**LLMOperationConfig dataclass tests (3 tests):**
- `test_create_with_defaults` -- all fields None
- `test_create_with_values` -- all fields set, including extra_kwargs
- `test_frozen` -- attempting to modify raises FrozenInstanceError

**configure_operations() tests (4 tests):**
- `test_configure_single_operation` -- set chat config, verify operation_configs property
- `test_configure_multiple_operations` -- set chat + compress + merge
- `test_configure_overwrites_existing` -- calling twice replaces the config
- `test_configure_type_error` -- passing non-LLMOperationConfig raises TypeError

**_resolve_llm_config() resolution chain tests (6 tests):**
- `test_resolve_call_level_wins` -- call model overrides operation and tract
- `test_resolve_operation_level_wins_over_tract` -- operation model overrides tract default
- `test_resolve_tract_default_used` -- no call/operation, falls back to _default_model
- `test_resolve_no_config_returns_empty` -- no config at any level, returns empty dict
- `test_resolve_temperature_chain` -- temperature resolution (call > operation)
- `test_resolve_extra_kwargs_merged` -- extra_kwargs from operation config forwarded

**Tract.open() with operation_configs tests (2 tests):**
- `test_open_with_operation_configs` -- pass dict, verify applied
- `test_open_without_operation_configs` -- default behavior unchanged

**chat/generate integration tests (4 tests):**
- `test_chat_uses_operation_config_model` -- configure chat model, verify MockLLMClient receives it
- `test_chat_call_override_beats_operation` -- call-level model overrides operation config
- `test_generate_uses_operation_config_temperature` -- configure temperature, verify forwarded
- `test_generation_config_reflects_operation_model` -- generation_config on commit captures per-op model

**merge integration tests (3 tests):**
- `test_merge_uses_operation_config` -- configure merge model, verify resolver gets it
- `test_merge_call_override_beats_operation` -- model= on merge() overrides operation config
- `test_merge_temperature_from_operation` -- temperature/max_tokens forwarded to resolver

**compress integration tests (4 tests):**
- `test_compress_uses_operation_config` -- configure compress model, verify _summarize_group receives it via llm_kwargs
- `test_compress_without_config_backward_compatible` -- no config = current behavior (no model kwargs sent)
- `test_compress_call_level_model_override` -- pass model= on compress(), verify it overrides operation config
- `test_compress_call_level_temperature_override` -- pass temperature= on compress(), verify forwarded

**orchestrate integration tests (3 tests):**
- `test_orchestrate_uses_operation_config_model` -- configure orchestrate model, verify OrchestratorConfig receives it
- `test_orchestrate_explicit_config_wins` -- explicit OrchestratorConfig.model overrides operation config
- `test_orchestrate_config_not_mutated` -- pass a config object, verify the ORIGINAL object is not mutated (dataclasses.replace safety)

**Backward compatibility tests (2 tests):**
- `test_no_operation_config_chat_unchanged` -- chat without any operation config works identically
- `test_no_operation_config_compress_unchanged` -- compress without operation config works identically

Test pattern: Use MockLLMClient from test_conversation.py (duplicate the class or import). Create Tract.open() instances, configure operations, call methods, assert MockLLMClient.last_kwargs contains expected model/temperature/max_tokens.

For compress tests: mock the llm_client on the tract directly (`t.configure_llm(mock_client)`) then call `t.compress()` and check `mock_client.last_kwargs`.

For merge tests: create two branches with diverging commits, configure merge operation config, call `t.merge()`, check that the resolver was created with the correct model.

For orchestrate tests: mock the orchestrator to verify config was applied. Include a test that passes a config object AND has operation-level config, then asserts the original config object was NOT mutated (validates the dataclasses.replace fix).

Total: ~31 tests.
  </action>
  <verify>
Run: `python -m pytest tests/test_operation_config.py -v` -- all tests pass.
Run: `python -m pytest tests/ -x --timeout=120` -- full suite passes (952 existing + ~31 new = ~983 tests).
  </verify>
  <done>
Comprehensive test suite covering: (1) LLMOperationConfig dataclass, (2) configure_operations() validation, (3) _resolve_llm_config() three-level resolution chain, (4) chat/generate integration with per-op model/temperature, (5) merge integration, (6) compress integration WITH call-level model/temperature/max_tokens overrides, (7) orchestrate integration WITH mutation-safety test, (8) backward compatibility.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_operation_config.py -v` -- all new tests pass
2. `python -m pytest tests/ -x --timeout=120` -- full suite passes (no regressions)
3. `python -c "from tract import LLMOperationConfig, Tract"` -- clean import
4. Manual verification: create a Tract with api_key mock, configure_operations(chat=LLMOperationConfig(model='gpt-4o'), compress=LLMOperationConfig(model='gpt-3.5-turbo')), call chat() and compress(), verify different models used
5. Verify orchestrate() has _resolve_llm_config BEFORE three-way branch and uses dataclasses.replace()
6. Verify compress() signature includes model/temperature/max_tokens params
</verification>

<success_criteria>
1. User can configure chat/generate to use one model while merge uses a different model, without reconfiguring the Tract instance between operations
2. User can set per-operation defaults (e.g., compress always uses a cheap model, chat uses a powerful model) that persist across calls
3. User can override per-operation config on individual calls (e.g., t.chat("complex question", model="gpt-4o") even when default chat model is gpt-4o-mini) -- this includes compress(model=..., temperature=..., max_tokens=...)
4. All existing tests pass without modification (backward compatible)
5. LLM-04 requirement marked Complete
</success_criteria>

<output>
After completion, create `.planning/phases/10-per-operation-llm-config/10-01-SUMMARY.md`
</output>
